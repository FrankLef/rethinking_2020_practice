```{r include=FALSE}
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(rethinking)
library(brms)
library(INLA)
library(loo)
library(dagitty, quietly = TRUE)
library(ggdag, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(ggmcmc, quietly = TRUE)
library(bayesplot, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
```


# Counting and Classification {#Counting}

We set the current theme used for plotting

```{r}
theme_set(
  ggthemes::theme_tufte(base_size = 11, base_family = "sans", ticks = TRUE) +
  theme(title = element_text(color = "midnightblue"),
        panel.grid.major  = element_blank(),
        panel.grid.minor  = element_blank(),
        plot.background = element_rect(fill = "gainsboro", color = NA))
  )
```




## 11E1 {-#prac11E1}

When coding, the predefined function `gtools::logit()` will be used.

```{r}
prob <- 0.35
# using the definition of logit
lodds = log(prob / (1 - prob))
lodds
# in coding we use a predefined function such as `gtools::logit()`
stopifnot(lodds == gtools::logit(prob))
```
## 11E2 {-#prac11E2}

When coding, the predefined function `gtools::inv.logit()` will be used. Also,
note that some authors call this function `expit()`, e.g. @brumback2022.

```{r}
lodds <- 3.2
prob = exp(lodds) / (1 + exp(lodds))
prob
# in coding we use a predefined function such as `gtools::inv.logit()`
stopifnot(prob == gtools::inv.logit(lodds))
```
## 11E3 {-#prac11E3}

See section 11.1.2, for a discussion on **relative effects** measured by 
**proportional odds** on p. 336 and Overthinking box on p. 337.


Since the logistic regression is using the link function

$$
logit(p) = \log{\frac{p}{1-p}}= \alpha + \beta x
$$

then the odds are

$$
odds(p) = \frac{p}{1-p} = \exp{(\alpha + \beta x)}
$$

and the *proportional* effect of $\beta$ is found when we increase $x$ by one unit.
That is the effect, called $z$, is found by solving for $z$

$$
\begin{align*}
\exp{(\alpha + \beta x)} \cdot z &= \exp{(\alpha + \beta (x + 1))} \\
z &= \frac{\exp{(\alpha + \beta x + 1)}}{\exp{(\alpha + \beta x)}} \\ &=
 \frac{\exp{(\beta (x + 1))}}{\exp{(\beta x)}} \\
 &= \exp(\beta)
\end{align*}
$$
Therefore the proportional change $z$ is $e^\beta$ which in this case is

```{r}
exp(1.7)
```

## 11E4 {-#prac11E4}

See section 11.2.3, p. 357-359, for detail.

The $\lambda$ parameter in Poisson can also
be seen as a rate.  The exposure is the denominator of that rate.  If different
observations have different exposure then an offset is required in the Poisson model.

So if we have the model

$$
\begin{align*}
y_i \sim Poisson(\lambda_i) \\
\log{\lambda_i} = \alpha + \beta x_i \\
\end{align*}
$$

Then the rate $\lambda_i$ is in fact $\lambda_i = \frac{\mu_i}{\tau_i}$ where
$\tau_i$ is the exposure and


$$
\begin{align*}
y_i &\sim Poisson(\lambda_i) \\
\log{\lambda_i} &= \log{\frac{\mu_i}{\tau_i}}  = \alpha + \beta x_i \\
\end{align*}
$$

which becomes

$$
\begin{align*}
\log{\frac{\mu_i}{\tau_i}} = \log{\mu_i} - \log{\tau_i}  &= \alpha + \beta x_i \\
\log{\mu_i}  &= \log{\tau_i} + \alpha + \beta x_i
\end{align*}
$$

For example if the count is over days and weeks then the exposures will be 1 and 7.


## 11M1 {-#prac11M1}

See beginning of section 11.1.3 for discussion.

The difference in the likelihood function is caused by the different organization
of data. In the binomial model there is only 2 outcomes for each observation
and therefore the joint probability is just a chain of bernoulli probability.

For example, in simple binomial probability, the probability of obtaining a $x_1=1$
and $x_2=0$ is $p(1-p)$. That is $P(x_i=1, x_2 =0) = p(1-p)= P(x_i=1, x_2 =0)$.

But if we organize the data differently and each observation is the total number
of success then $P(x_1 = 1 \mid n =2)= \binom{1}{2}p(1-p)$ because we have to take
into account there are $\binom{1}{2}$ possible orderings.

This will cause noticeable difference in the PSIS and WAIC as shown on 
p. 358-359 with models m11.4 and m11.6. However the likelihood *probability*
will be unaffected.


## 11M2 {-#prac11M2}

See 11E3 above for similar question and same method to answer.

The Poisson model is

$$
\begin{align*}
y_i &\sim \mathcal{Poisson}(\lambda_i) \\
\log{\lambda_i} &= \alpha + \beta x_i \\
&\therefore \\
\lambda_i &= \exp{(\alpha + \beta x_i)}
\end{align*}
$$

therefore the effect is found by solving the proportional change $z$ in

$$
\begin{align*}
\exp{(\alpha + \beta x_i)} \cdot z &= \exp{(\alpha + \beta (x_i + 1))} \\
z &= e^\beta
\end{align*}
$$

Therefore the effect of a coefficient of 1.7 will proportionally change the
rate $\lambda_i$ by

```{r}
exp(1.7)
```

## 11M3 {-#prac11M3}

See section 10.2.2 in chapter 10.

The $logit$ link puts the parameter $p \in [0,1]$ on the scale $(-\infty, \infty)$ so that the
Gaussian model can be used on the transformed variable.

## 11M4 {-#prac11M4}

See section 10.2.2 in chapter 10.

The $log$ link puts the parameter $\lambda \in [0,\infty)$ on the scale 
$(-\infty, \infty)$ so that the Gaussian model can be used on the transformed 
variable.

## 11M5 {-#prac11M5}

If $\lambda \in [0,1]$ then the logit link used is justified as discussed in
11M3 above.

Poisson can be used to approximate the binomial distribution when $p$ is small.
But it is also a distribution in its own right and there could be cases where
we have $\lambda \in [0,1]$ and the goal is **not** to approximate the binomial
distribution (i.e. $p$ is not small).

As an example, when $\lambda$ is expressed  as a percentage of dark-colored
cars passing every day at an intersection.

## 11M6 {-#prac11M6}

See section 10.1.2, p. 307, in chapter 10 for the binomial distribution.  It has maximum 
entropy when

1. there is only 2 unordered events,
2. it has a constant expected value

See section 10.2.1, p. 315, for Poisson distribution.

The Poisson distribution is used in practice for counts that never get close
to any theoretical maximum.

It is a special case of the binomial distribution when the probability of 
success $p$ is very small and the number of trials is very large. 
It has the same constraints as the binomial distribution in that case.

## 11M7 {-#prac11M7}

See section 11.1.1, p. 330, for the m11.4 model.


```{r}
data(chimpanzees)
dataChimp <- chimpanzees %>%
  mutate(treatment = as.integer(1 + prosoc_left + 2 * condition)) %>%
  select(pulled_left, actor, treatment)
rm(chimpanzees)
# unique(d$treatment)
skimr::skim(dataChimp)
```

### Model with $\mathcal{N}(0, 1.5)$ {-}

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha_{actor[i]} + \beta_{treatment[i]} \\
\alpha_j &\sim \mathcal{N}(0, 1.5) \\
\beta_k &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$

The fit with hamiltonian MCMC

```{r}
a_file <- here::here(getwd(), "fits", "m11M07a.rds")
m11M07a <- readRDS(a_file)
# m11M07a <- ulam(
#   data = dataChimp,
#   flist = alist(
#     pulled_left ~ dbinom(1, p),
#     logit(p) <- a[actor] + b[treatment],
#     a[actor] ~ dnorm(0, 1.5),
#     b[treatment] ~ dnorm(0, 0.5)),
#   chains = 4, log_lik = TRUE
#   )
# saveRDS(m11M07a, file = a_file)
# precis(m11M07a, depth = 2)
```

and with quadratic approximation

```{r}
a_file <- here::here(getwd(), "fits", "m11M07b.rds")
m11M07b <- readRDS(a_file)
# m11M07b <- quap(
#   dataChimp,
#   flist = alist(
#     pulled_left ~ dbinom(1, p),
#     logit(p) <- a[actor] + b[treatment],
#     a[actor] ~ dnorm(0, 1.5),
#     b[treatment] ~ dnorm(0, 0.5)))
# saveRDS(m11M07b, file = a_file)
# precis(m11M07b, depth = 2)
```

Conclusion: The 2 methods generate about the same results.  Probably because
the posterior is gaussian.  When the posterior is gaussian, the quadratic
approximation gives similar results to MCMC but more efficiently.


### Model with $\mathcal{N}(0, 10)$ {-}

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha_{actor[i]} + \beta_{treatment[i]} \\
\alpha_j &\sim \mathcal{N}(0, 10) \\
\beta_k &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$

The fit with Hamiltonian MCMC

```{r}
a_file <- here::here(getwd(), "fits", "m11M07c.rds")
m11M07c <- readRDS(a_file)
# message("22 divergent pairs after warmup, examine pairs() plot for diagnosis")
# m11M07c <- ulam(
#   data = dataChimp,
#   flist = alist(
#     pulled_left ~ dbinom(1, p),
#     logit(p) <- a[actor] + b[treatment],
#     a[actor] ~ dnorm(0, 10),
#     b[treatment] ~ dnorm(0, 0.5)),
#   chains = 4, log_lik = TRUE
#   )
# saveRDS(m11M07c, file = a_file)
# precis(m11M07c, depth = 2)
```

and with quadratic approximation

```{r}
a_file <- here::here(getwd(), "fits", "m11M07d.rds")
m11M07d <- readRDS(a_file)
# m11M07d <- quap(
#   dataChimp,
#   flist = alist(
#     pulled_left ~ dbinom(1, p),
#     logit(p) <- a[actor] + b[treatment],
#     a[actor] ~ dnorm(0, 10),
#     b[treatment] ~ dnorm(0, 0.5)))
# saveRDS(m11M07d, file = a_file)
# precis(m11M07d, depth = 2)
```

and we look at the mean for each fit object to answer the question

```{r}
gt <- list()
gt$data <- purrr::map_dfr(.x = paste0("m11M07", letters[1:4]) %>%
                            setNames(nm = .),
                         .f = ~get(.) %>% 
                           precis(depth = 2) %>%
                           data.frame() %>%
                           tibble::rownames_to_column(var = "param"),
                         .id = "fit")

gt$data %>%
  select(fit, param, mean) %>%
  pivot_wider(id_cols = param, names_from = fit, values_from = mean) %>%
  mutate(across(.cols = where(is.double), .fns = ~round(., 2)))
```

There is a significant different for actor # 2 who has mean = 11.43 with m11M07c (MCMC with `ulam`) and mean = 6.99 with `quap`.  In addition the m11M07c warns
that we have 22 divergent samples after warmup and is therefore more informative
with for the diagnosis.

The `quap` offers no diagnosis and the mean is less affected by the new prior as
it gives a more centralized value.  The quap might be giving a false sense of
security as one might tend to conclude that the mean of 6.99 is more reasonable
when in fact it does not reflect that actor #2 is actually an outlier as 
it always pull the left lever as discussed in section 11.1.1 and illustrated 
in figure 11.4.

## 11M8 {-#prac11M8}

See section 11.2.1 for details and section 10.2.4 of chapter 10 on how to
be careful when interpreting the relative importance of the coefficients.

```{r}
data(Kline)
dataKline <- Kline %>%
  mutate(log_pop_s = log(population),
         log_pop_s = as.vector(scale(log_pop_s)),
         cid = factor(contact, levels = c("low", "high")))
rm(Kline)
```

The model is

$$
total\_tools_i \sim \mathcal{Poisson}(\lambda_i) \\
\log{\lambda_i} = \alpha_{cid[i]} + \beta_{cid[i]} \log{log\_pop\_s_i} \\
\alpha_j \sim \mathcal{N}(3, 0.5) \\
\beta_k \sim \mathcal{N}(0, 0.2)
$$

the fit *including* Hawaii is

```{r}
a_file <- here::here(getwd(), "fits", "b11M08a.rds")
b11M08a <- readRDS(a_file)
# b11M08a <- brm(data = dataKline,
#              family = poisson,
#              formula = bf(total_tools ~ a + b * log_pop_s,
#                           a + b ~ 0 + cid,
#                           nl = TRUE),
#              prior = c(prior(normal(3, 0.5), nlpar = a),
#                        prior(normal(0, 0.2), nlpar = b)),
#              cores = detectCores(), seed = 11)
# b11M08a <- brms::add_criterion(b11M08a, criterion = c("waic", "loo"))
# saveRDS(b11M08a, file = a_file)
summary(b11M08a)
```

and the fit *excluding* Hawaii is

```{r}
a_file <- here::here(getwd(),"fits", "b11M08b.rds")
b11M08b <- readRDS(a_file)
# dh <- dataKline %>%
#   filter(culture != "Hawaii")
# b11M08b <- brm(data = dh,
#              family = poisson,
#              formula = bf(total_tools ~ a + b * log_pop_s,
#                           a + b ~ 0 + cid,
#                           nl = TRUE),
#              prior = c(prior(normal(3, 0.5), nlpar = a),
#                        prior(normal(0, 0.2), nlpar = b)),
#              cores = detectCores(), seed = 11)
# b11M08b <- brms::add_criterion(b11M08b, criterion = c("waic", "loo"))
# saveRDS(b11M08b, file = a_file)
summary(b11M08b)
```


```{r}
gt <- list()
gt$data <- purrr::map_dfr(.x = paste0("b11M08", letters[1:2]) %>%
                           setNames(nm = .),
                         .f = ~get(.) %>%
                           fixef() %>%
                           data.frame() %>%
                           tibble::rownames_to_column(var = "param"),
                         .id = "fit")
gt$data %>%
  select(fit, param, Estimate) %>%
  pivot_wider(id_cols = param, names_from = fit, values_from = Estimate) %>%
  mutate(across(.cols = where(is.double), .fns = ~round(., 2))) %>%
  mutate(diff = b11M08a - b11M08b)
```


Conclusion: The impact is on the factor *low* which has a lower cluster factor
of 3.18 without Hawaii vs 3.32 with Hawaii.  The effect without Hawaii is also
lower with 0.19 vs 0.38 with Hawaii.

It is important to note that these differences are relative, as described in
section 10.2.4 of chapter 10. That is *keep in mind that a big beta-coefficient 
may not correspond to a big effect on the outcome*.


## 11H1 {-#prac11H1}


```{r}
data(chimpanzees)
dataChimp <- chimpanzees %>%
  mutate(actor = factor(actor),
         treatment = factor(1 + prosoc_left + 2 * condition)) %>%
  select(pulled_left, actor, treatment)
rm(chimpanzees)
```

### Null model, general intercept {-}

This corresponds to model `m11.1` in the textbook but with the calibrated
prior.

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha \\
\alpha &\sim \mathcal{N}(0, 1.5)
\end{align*}
$$



```{r}
a_file <- here::here("fits", "b11H01a.rds")
b11H01a <- readRDS(file = a_file)
# b11H01a <- brm(data = dataChimp,
#               family = binomial,
#               formula = pulled_left | trials(1) ~ 1,
#               prior = c(
#                 prior(normal(0, 1.5), class = Intercept)
#                 ),
#               seed = 11, cores = detectCores()
#               )
# b11H01a <- brms::add_criterion(b11H01a, criterion = c("waic", "loo"))
# saveRDS(b11H01a, file = a_file)
summary(b11H01a)
```

### Model with treatment intercepts {-}

This corresponds to model `m11.3` in the textbook.

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha + \beta_{treatment[i]} \\
\alpha &\sim \mathcal{N}(0, 1.5) \\
\beta_k &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$


```{r}
a_file <- here::here("fits", "b11H01b.rds")
b11H01b <- readRDS(file = a_file)
# b11H01b <- brm(data = dataChimp,
#               family = binomial,
#               formula = bf(pulled_left | trials(1) ~ a + b,
#                            a ~ 1,
#                            b ~ 0 + treatment,
#                            nl = TRUE),
#               prior = c(
#                 prior(normal(0, 1.5), nlpar = a),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment1),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment2),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment3),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment4)
#                 ),
#              cores = detectCores(), seed = 11)
# b11H01b <- brms::add_criterion(b11H01b, criterion = c("waic", "loo"))
saveRDS(b11H01b, file = a_file)
# summary(b11H01b)
```


### Model with treatment and actor intercepts {-}

This corresponds to model `m11.4` in the textbook.

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha_{actor[i]} + \beta_{treatment[i]} \\
\alpha_j &\sim \mathcal{N}(0, 1.5) \\
\beta_k &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$


```{r}
a_file <- here::here("fits", "b11H01c.rds")
b11H01c <- readRDS(file = a_file)
# b11H01c <- brm(data = dataChimp,
#               family = binomial,
#               formula = bf(pulled_left | trials(1) ~ a + b,
#                            a ~ 0 + actor,
#                            b ~ 0 + treatment,
#                            nl = TRUE),
#               prior = c(
#                 prior(normal(0, 1.5), nlpar = a),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment1),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment2),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment3),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment4)
#                 ),
#              cores = detectCores(), seed = 11)
# b11H01c <- brms::add_criterion(b11H01c, criterion = c("waic", "loo"))
saveRDS(b11H01c, file = a_file)
summary(b11H01c)
```



### Model comparison {-}

The model with general intercept ("null model") and treatment-only intercepts
have about the same performance.  The model with actor and treatment intercept
is the most informative as the treatment is directly related to what the actor
choose.


```{r}
# no need to print all the columns, the result is obvious
loo_compare(b11H01a, b11H01b, b11H01c, criterion = "waic")
```

## 11H2 {-#prac11H2}

```{r}
data(eagles, package = "MASS")
# bitwise number describing the conditions
bw <- c(
  "0" = "not large pir., not adult pir., not large vic.",
  "1" = "large pir., not adult pir., not large vic.",
  "2" = "not large pir., adult pir., not large vic.",
  "3" = "large pir., adult pir., not large vic.",
  "4" = "not large pir., not adult pir., large vic., ",
  "5" = "large pir., not adult pir., large vic.",
  "6" = "not large pir., adult pir., large vic.",
  "7" = "large pir., adult pir., large vic.")
dataEagles <- eagles %>%
  mutate(id = seq_len(n()),
         cond = (P == "L") * 1 + (A == "A") * 2 + (V == "L") * 4,
         idx = cond + 1,  # indx=ex to use with INLA
         desc = bw[as.character(cond)],
         cond = factor(cond, ordered = TRUE, levels = 0:7),
         desc = factor(desc)) %>%
  relocate(id)
rm(eagles)
glimpse(dataEagles)
skimr::skim(dataEagles)
```



### (a) {-}

The model

$$
\begin{align*}
y_i &\sim \mathcal{Binomial}(n_i, p_i) \\
logit(p_i) &= \alpha + \beta_P P_i + \beta_V V_i, + \beta_A A_i \\
\alpha &\sim \mathcal{N}(0, 1.5) \\
\beta_P, \beta_V, \beta_A &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$

#### `quap` fit {-}


```{r}
a_file <- here::here("fits", "m11H02a.rds")
m11H02a <- readRDS(file = a_file)
# m11H02a <- quap(
#   dataEagles,
#   flist = alist(
#     y ~ dbinom(n, p),
#     logit(p) <- a + bP[cond],
#     a ~ dnorm(0, 1.5),
#     bP[cond] ~ dnorm(0, 0.5))
# )
# saveRDS(m11H02a, file = a_file)
precis(m11H02a, depth = 2)
```

```{r}
gt <- list()
gt$m11H02a <- rethinking::link(m11H02a) %>%
  data.frame() %>%
  setNames(nm = paste("cond", seq_len(length(.)), sep = "_")) %>%
  pivot_longer(cols = everything(), names_to = "cond") %>%
  group_by(cond) %>%
  ggdist::mean_qi()
gt$m11H02a

gt$m11H02a$value * dataEagles$n[dataEagles$cond]

```



#### `brm` fit {-}

see b11.2 in @kurtz2020b on how to do this.

```{r}
a_file <- here::here("fits", "b11H02a.rds")
b11H02a <- readRDS(file = a_file)
# b11H02a <- brm(data = dataEagles,
#                    family = binomial,
#                    formula = bf(y | trials(n) ~ a + b,
#                                 a ~ 1,
#                                 b ~ 0 + cond,
#                                 nl = TRUE),
#                    prior = c(
#                      prior(normal(0, 1.5), nlpar = a),
#                      prior(normal(0, 0.5), nlpar = b)),
#                    cores = detectCores(), seed = 12)
# b11H02a <- brms::add_criterion(b11H02a, criterion = c("waic", "loo"))
# saveRDS(b11H02a, file = a_file)
summary(b11H02a)
```

```{r}
gt$b11H02a_epred <- epred_draws(b11H02a, newdata = dataEagles) %>%
  as.data.frame() %>%
  select(cond, .epred) %>%
  group_by(cond) %>%
  ggdist::mean_qi()
gt$b11H02a_epred
gt$b11H02a_linpred <- linpred_draws(b11H02a, newdata = dataEagles) %>%
  as.data.frame() %>%
  select(cond, .linpred) %>%
  group_by(cond) %>%
  ggdist::mean_qi()
gt$b11H02a_linpred

gt$b11H02a_predict <- predicted_draws(b11H02a, newdata = dataEagles) %>%
  as.data.frame() %>%
  select(cond, .prediction) %>%
  group_by(cond) %>%
  ggdist::mean_qi()
gt$b11H02a_predict

dataEagles$n[dataEagles$cond]
gtools::inv.logit(gt$b11H02a_linpred$.linpred) * dataEagles$n[dataEagles$cond]
```



```{r}
fixef(b11H02a)[,"Estimate"]
gtools::inv.logit(fixef(b11H02a)[,"Estimate"])
inv_logit_scaled(fixef(b11H02a)[,"Estimate"])
```


#### `inla` fit {-}

```{r}
str(dataEagles)
```



```{r}
a_file <- here::here("fits", "i11H02a.rds")
i11H02a <- readRDS(file = a_file)
# i11H02a <- inla(data = dataEagles,
#                 formula =  y ~ 1 + f(idx, model = "iid",  
#                                      hyper = list(prec = list(param = c(0.001, 0.001)))),
#                 family = "binomial",
#                 Ntrials = n,
#                 control.family = list(link = "logit"),
#                 control.predictor = list(link = 1, compute = TRUE),
#                 control.compute = list(dic = TRUE, cpo = TRUE, waic = TRUE))
# saveRDS(i11H02a, file = a_file)
summary(i11H02a)
```


```{r}
i11H02a$summary.fixed
```

which gives the fitted values

```{r}
i11H02a$summary.fitted.values
```


#### Conclusion {-}

and we can now compare the fitted rate and mean for each of the 3 methods.


```{r}
# Make sure the data.frame is sorted in increasing order of "cond"
# to match the fitted data
df <- dataEagles %>%
  select(y, n, cond) %>%
  arrange(cond) %>%  # important: make sure data is arrange by cond
  mutate(p_y = y / n,
         p_quap = gt$m11H02a$value,
         p_brm = gtools::inv.logit(gt$b11H02a_linpred$.linpred),
         p_inla = i11H02a$summary.fitted.values$mean) %>%
  mutate(mean_quap = p_quap * n,
         mean_brm = p_brm * n,
         mean_inla = p_inla *n) %>%
  mutate(across(.cols = starts_with("p"), .fns = ~round(., digits = 2)),
         across(.cols = starts_with("mean"), .fns = ~round(., digits = 1)))
df
```

Conclusion: `INLA` is very fast but not really good . . . probably because
the distribution is skewed. `INLA` is designed fo centralized (gaussian) 
distributions.


### (b) {-}