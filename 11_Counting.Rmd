```{r include=FALSE}
library(rethinking)
library(brms)
library(INLA)
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(modelr)
library(tidybayes, quietly = TRUE)
library(loo)
library(simstudy)
library(dagitty, quietly = TRUE)
library(ggdag, quietly = TRUE)
library(posterior)
library(ggdist, quietly = TRUE)
library(ggmcmc, quietly = TRUE)
library(bayesplot, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
library(gt)
```


# Counting and Classification {#Counting}

We set the current theme used for plotting

```{r}
theme_set(
  ggthemes::theme_tufte(base_size = 11, base_family = "sans", ticks = TRUE) +
  theme(title = element_text(color = "midnightblue"),
        panel.grid.major  = element_blank(),
        panel.grid.minor  = element_blank(),
        plot.background = element_rect(fill = "gainsboro", color = NA),
        strip.background = element_rect(fill = "wheat", color = NA))
  )
```




## 11E1 {-#prac11E1}

When coding, the predefined function `gtools::logit()` will be used.

```{r}
prob <- 0.35
# using the definition of logit
lodds = log(prob / (1 - prob))
lodds
# in coding we use a predefined function such as `gtools::logit()`
stopifnot(lodds == gtools::logit(prob))
```
## 11E2 {-#prac11E2}

When coding, the predefined function `gtools::inv.logit()` will be used. Also,
note that some authors call this function `expit()`, e.g. @brumback2022.

```{r}
lodds <- 3.2
prob = exp(lodds) / (1 + exp(lodds))
prob
# in coding we use a predefined function such as `gtools::inv.logit()`
stopifnot(prob == gtools::inv.logit(lodds))
```
## 11E3 {-#prac11E3}

See section 11.1.2, for a discussion on **relative effects** measured by 
**proportional odds** on p. 336 and Overthinking box on p. 337.


Since the logistic regression is using the link function

$$
logit(p) = \log{\frac{p}{1-p}}= \alpha + \beta x
$$

then the odds are

$$
odds(p) = \frac{p}{1-p} = \exp{(\alpha + \beta x)}
$$

and the *proportional* effect of $\beta$ is found when we increase $x$ by one unit.
That is the effect, called $z$, is found by solving for $z$

$$
\begin{align*}
\exp{(\alpha + \beta x)} \cdot z &= \exp{(\alpha + \beta (x + 1))} \\
z &= \frac{\exp{(\alpha + \beta x + 1)}}{\exp{(\alpha + \beta x)}} \\ &=
 \frac{\exp{(\beta (x + 1))}}{\exp{(\beta x)}} \\
 &= \exp(\beta)
\end{align*}
$$
Therefore the proportional change $z$ is $e^\beta$ which in this case is

```{r}
exp(1.7)
```

## 11E4 {-#prac11E4}

See section 11.2.3, p. 357-359, for detail.

The $\lambda$ parameter in Poisson can also
be seen as a rate.  The exposure is the denominator of that rate.  If different
observations have different exposure then an offset is required in the Poisson model.

So if we have the model

$$
\begin{align*}
y_i \sim Poisson(\lambda_i) \\
\log{\lambda_i} = \alpha + \beta x_i \\
\end{align*}
$$

Then the rate $\lambda_i$ is in fact $\lambda_i = \frac{\mu_i}{\tau_i}$ where
$\tau_i$ is the exposure and


$$
\begin{align*}
y_i &\sim Poisson(\lambda_i) \\
\log{\lambda_i} &= \log{\frac{\mu_i}{\tau_i}}  = \alpha + \beta x_i \\
\end{align*}
$$

which becomes

$$
\begin{align*}
\log{\frac{\mu_i}{\tau_i}} = \log{\mu_i} - \log{\tau_i}  &= \alpha + \beta x_i \\
\log{\mu_i}  &= \log{\tau_i} + \alpha + \beta x_i
\end{align*}
$$

For example if the count is over days and weeks then the exposures will be 1 and 7.


## 11M1 {-#prac11M1}

See beginning of section 11.1.3 for discussion.

The difference in the likelihood function is caused by the different organization
of data. In the binomial model there is only 2 outcomes for each observation
and therefore the joint probability is just a chain of bernoulli probability.

For example, in simple binomial probability, the probability of obtaining a $x_1=1$
and $x_2=0$ is $p(1-p)$. That is $P(x_i=1, x_2 =0) = p(1-p)= P(x_i=1, x_2 =0)$.

But if we organize the data differently and each observation is the total number
of success then $P(x_1 = 1 \mid n =2)= \binom{1}{2}p(1-p)$ because we have to take
into account there are $\binom{1}{2}$ possible orderings.

This will cause noticeable difference in the PSIS and WAIC as shown on 
p. 358-359 with models m11.4 and m11.6. However the likelihood *probability*
will be unaffected.


## 11M2 {-#prac11M2}

See 11E3 above for similar question and same method to answer.

The Poisson model is

$$
\begin{align*}
y_i &\sim \mathcal{Poisson}(\lambda_i) \\
\log{\lambda_i} &= \alpha + \beta x_i \\
&\therefore \\
\lambda_i &= \exp{(\alpha + \beta x_i)}
\end{align*}
$$

therefore the effect is found by solving the proportional change $z$ in

$$
\begin{align*}
\exp{(\alpha + \beta x_i)} \cdot z &= \exp{(\alpha + \beta (x_i + 1))} \\
z &= e^\beta
\end{align*}
$$

Therefore the effect of a coefficient of 1.7 will proportionally change the
rate $\lambda_i$ by

```{r}
exp(1.7)
```

## 11M3 {-#prac11M3}

See section 10.2.2 in chapter 10.

The $logit$ link puts the parameter $p \in [0,1]$ on the scale $(-\infty, \infty)$ so that the
Gaussian model can be used on the transformed variable.

## 11M4 {-#prac11M4}

See section 10.2.2 in chapter 10.

The $log$ link puts the parameter $\lambda \in [0,\infty)$ on the scale 
$(-\infty, \infty)$ so that the Gaussian model can be used on the transformed 
variable.

## 11M5 {-#prac11M5}

If $\lambda \in [0,1]$ then the logit link used is justified as discussed in
11M3 above.

Poisson can be used to approximate the binomial distribution when $p$ is small.
But it is also a distribution in its own right and there could be cases where
we have $\lambda \in [0,1]$ and the goal is **not** to approximate the binomial
distribution (i.e. $p$ is not small).

As an example, when $\lambda$ is expressed  as a percentage of dark-colored
cars passing every day at an intersection.

## 11M6 {-#prac11M6}

See section 10.1.2, p. 307, in chapter 10 for the binomial distribution.  It has maximum 
entropy when

1. there is only 2 unordered events,
2. it has a constant expected value

See section 10.2.1, p. 315, for Poisson distribution.

The Poisson distribution is used in practice for counts that never get close
to any theoretical maximum.

It is a special case of the binomial distribution when the probability of 
success $p$ is very small and the number of trials is very large. 
It has the same constraints as the binomial distribution in that case.

## 11M7 {-#prac11M7}

See section 11.1.1, p. 330, for the m11.4 model.


```{r}
data(chimpanzees)
dataChimp <- chimpanzees %>%
  mutate(treatment = as.integer(1 + prosoc_left + 2 * condition)) %>%
  select(pulled_left, actor, treatment)
rm(chimpanzees)
# unique(d$treatment)
skimr::skim(dataChimp)
```

### Model with $\mathcal{N}(0, 1.5)$ {-}

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha_{actor[i]} + \beta_{treatment[i]} \\
\alpha_j &\sim \mathcal{N}(0, 1.5) \\
\beta_k &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$

The fit with hamiltonian MCMC

```{r}
a_file <- here::here(getwd(), "fits", "m11M07a.rds")
m11M07a <- readRDS(a_file)
# m11M07a <- ulam(
#   data = dataChimp,
#   flist = alist(
#     pulled_left ~ dbinom(1, p),
#     logit(p) <- a[actor] + b[treatment],
#     a[actor] ~ dnorm(0, 1.5),
#     b[treatment] ~ dnorm(0, 0.5)),
#   chains = 4, log_lik = TRUE
#   )
# saveRDS(m11M07a, file = a_file)
# precis(m11M07a, depth = 2)
```

and with quadratic approximation

```{r}
a_file <- here::here(getwd(), "fits", "m11M07b.rds")
m11M07b <- readRDS(a_file)
# m11M07b <- quap(
#   dataChimp,
#   flist = alist(
#     pulled_left ~ dbinom(1, p),
#     logit(p) <- a[actor] + b[treatment],
#     a[actor] ~ dnorm(0, 1.5),
#     b[treatment] ~ dnorm(0, 0.5)))
# saveRDS(m11M07b, file = a_file)
# precis(m11M07b, depth = 2)
```

Conclusion: The 2 methods generate about the same results.  Probably because
the posterior is gaussian.  When the posterior is gaussian, the quadratic
approximation gives similar results to MCMC but more efficiently.


### Model with $\mathcal{N}(0, 10)$ {-}

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha_{actor[i]} + \beta_{treatment[i]} \\
\alpha_j &\sim \mathcal{N}(0, 10) \\
\beta_k &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$

The fit with Hamiltonian MCMC

```{r}
a_file <- here::here(getwd(), "fits", "m11M07c.rds")
m11M07c <- readRDS(a_file)
# message("22 divergent pairs after warmup, examine pairs() plot for diagnosis")
# m11M07c <- ulam(
#   data = dataChimp,
#   flist = alist(
#     pulled_left ~ dbinom(1, p),
#     logit(p) <- a[actor] + b[treatment],
#     a[actor] ~ dnorm(0, 10),
#     b[treatment] ~ dnorm(0, 0.5)),
#   chains = 4, log_lik = TRUE
#   )
# saveRDS(m11M07c, file = a_file)
# precis(m11M07c, depth = 2)
```

and with quadratic approximation

```{r}
a_file <- here::here(getwd(), "fits", "m11M07d.rds")
m11M07d <- readRDS(a_file)
# m11M07d <- quap(
#   dataChimp,
#   flist = alist(
#     pulled_left ~ dbinom(1, p),
#     logit(p) <- a[actor] + b[treatment],
#     a[actor] ~ dnorm(0, 10),
#     b[treatment] ~ dnorm(0, 0.5)))
# saveRDS(m11M07d, file = a_file)
# precis(m11M07d, depth = 2)
```

and we look at the mean for each fit object to answer the question

```{r}
gt <- list()
gt$data <- purrr::map_dfr(.x = paste0("m11M07", letters[1:4]) %>%
                            setNames(nm = .),
                         .f = ~get(.) %>% 
                           precis(depth = 2) %>%
                           data.frame() %>%
                           tibble::rownames_to_column(var = "param"),
                         .id = "fit")

gt$data %>%
  select(fit, param, mean) %>%
  pivot_wider(id_cols = param, names_from = fit, values_from = mean) %>%
  mutate(across(.cols = where(is.double), .fns = ~round(., 2)))
```

There is a significant different for actor # 2 who has mean = 11.43 with m11M07c (MCMC with `ulam`) and mean = 6.99 with `quap`.  In addition the m11M07c warns
that we have 22 divergent samples after warmup and is therefore more informative
with for the diagnosis.

The `quap` offers no diagnosis and the mean is less affected by the new prior as
it gives a more centralized value.  The quap might be giving a false sense of
security as one might tend to conclude that the mean of 6.99 is more reasonable
when in fact it does not reflect that actor #2 is actually an outlier as 
it always pull the left lever as discussed in section 11.1.1 and illustrated 
in figure 11.4.

## 11M8 {-#prac11M8}

See section 11.2.1 for details and section 10.2.4 of chapter 10 on how to
be careful when interpreting the relative importance of the coefficients.

```{r}
data(Kline)
dataKline <- Kline %>%
  mutate(log_pop_s = log(population),
         log_pop_s = as.vector(scale(log_pop_s)),
         cid = factor(contact, levels = c("low", "high")))
rm(Kline)
```

The model is

$$
total\_tools_i \sim \mathcal{Poisson}(\lambda_i) \\
\log{\lambda_i} = \alpha_{cid[i]} + \beta_{cid[i]} \log{log\_pop\_s_i} \\
\alpha_j \sim \mathcal{N}(3, 0.5) \\
\beta_k \sim \mathcal{N}(0, 0.2)
$$

the fit *including* Hawaii is

```{r}
a_file <- here::here(getwd(), "fits", "b11M08a.rds")
b11M08a <- readRDS(a_file)
# b11M08a <- brm(data = dataKline,
#              family = poisson,
#              formula = bf(total_tools ~ a + b * log_pop_s,
#                           a + b ~ 0 + cid,
#                           nl = TRUE),
#              prior = c(prior(normal(3, 0.5), nlpar = a),
#                        prior(normal(0, 0.2), nlpar = b)),
#              cores = detectCores(), seed = 11)
# b11M08a <- brms::add_criterion(b11M08a, criterion = c("waic", "loo"))
# saveRDS(b11M08a, file = a_file)
summary(b11M08a)
```

and the fit *excluding* Hawaii is

```{r}
a_file <- here::here(getwd(),"fits", "b11M08b.rds")
b11M08b <- readRDS(a_file)
# dh <- dataKline %>%
#   filter(culture != "Hawaii")
# b11M08b <- brm(data = dh,
#              family = poisson,
#              formula = bf(total_tools ~ a + b * log_pop_s,
#                           a + b ~ 0 + cid,
#                           nl = TRUE),
#              prior = c(prior(normal(3, 0.5), nlpar = a),
#                        prior(normal(0, 0.2), nlpar = b)),
#              cores = detectCores(), seed = 11)
# b11M08b <- brms::add_criterion(b11M08b, criterion = c("waic", "loo"))
# saveRDS(b11M08b, file = a_file)
summary(b11M08b)
```


```{r}
gt <- list()
gt$data <- purrr::map_dfr(.x = paste0("b11M08", letters[1:2]) %>%
                           setNames(nm = .),
                         .f = ~get(.) %>%
                           fixef() %>%
                           data.frame() %>%
                           tibble::rownames_to_column(var = "param"),
                         .id = "fit")
gt$data %>%
  select(fit, param, Estimate) %>%
  pivot_wider(id_cols = param, names_from = fit, values_from = Estimate) %>%
  mutate(across(.cols = where(is.double), .fns = ~round(., 2))) %>%
  mutate(diff = b11M08a - b11M08b)
```


Conclusion: The impact is on the factor *low* which has a lower cluster factor
of 3.18 without Hawaii vs 3.32 with Hawaii.  The effect without Hawaii is also
lower with 0.19 vs 0.38 with Hawaii.

It is important to note that these differences are relative, as described in
section 10.2.4 of chapter 10. That is *keep in mind that a big beta-coefficient 
may not correspond to a big effect on the outcome*.


## 11H1 {-#prac11H1}

See section 11.1.1 on how to do this practice.

```{r}
data(chimpanzees)
dataChimp <- chimpanzees %>%
  mutate(actor = factor(actor),
         treatment = factor(1 + prosoc_left + 2 * condition)) %>%
  select(pulled_left, actor, treatment)
rm(chimpanzees)
```

### Null model, general intercept {-}

This corresponds to model `m11.1` in the textbook but with the calibrated
prior.

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha \\
\alpha &\sim \mathcal{N}(0, 1.5)
\end{align*}
$$



```{r}
a_file <- here::here("fits", "b11H01a.rds")
b11H01a <- readRDS(file = a_file)
# b11H01a <- brm(data = dataChimp,
#               family = binomial,
#               formula = pulled_left | trials(1) ~ 1,
#               prior = c(
#                 prior(normal(0, 1.5), class = Intercept)
#                 ),
#               seed = 11, cores = detectCores()
#               )
# b11H01a <- brms::add_criterion(b11H01a, criterion = c("waic", "loo"))
# saveRDS(b11H01a, file = a_file)
summary(b11H01a)
```

### Model with treatment intercepts {-}

This corresponds to model `m11.3` in the textbook.

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha + \beta_{treatment[i]} \\
\alpha &\sim \mathcal{N}(0, 1.5) \\
\beta_k &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$


```{r}
a_file <- here::here("fits", "b11H01b.rds")
b11H01b <- readRDS(file = a_file)
# b11H01b <- brm(data = dataChimp,
#               family = binomial,
#               formula = bf(pulled_left | trials(1) ~ a + b,
#                            a ~ 1,
#                            b ~ 0 + treatment,
#                            nl = TRUE),
#               prior = c(
#                 prior(normal(0, 1.5), nlpar = a),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment1),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment2),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment3),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment4)
#                 ),
#              cores = detectCores(), seed = 11)
# b11H01b <- brms::add_criterion(b11H01b, criterion = c("waic", "loo"))
# saveRDS(b11H01b, file = a_file)
summary(b11H01b)
```


### Model with treatment and actor intercepts {-}

This corresponds to model `m11.4` in the textbook.

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha_{actor[i]} + \beta_{treatment[i]} \\
\alpha_j &\sim \mathcal{N}(0, 1.5) \\
\beta_k &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$


```{r}
a_file <- here::here("fits", "b11H01c.rds")
b11H01c <- readRDS(file = a_file)
# b11H01c <- brm(data = dataChimp,
#               family = binomial,
#               formula = bf(pulled_left | trials(1) ~ a + b,
#                            a ~ 0 + actor,
#                            b ~ 0 + treatment,
#                            nl = TRUE),
#               prior = c(
#                 prior(normal(0, 1.5), nlpar = a),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment1),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment2),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment3),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment4)
#                 ),
#              cores = detectCores(), seed = 11)
# b11H01c <- brms::add_criterion(b11H01c, criterion = c("waic", "loo"))
# saveRDS(b11H01c, file = a_file)
summary(b11H01c)
```



### Model comparison {-}

The model with general intercept ("null model") and treatment-only intercepts
have about the same performance.  The model with actor and treatment intercept
is the best fit. An interpretation of this could be that the actor is more
relevant to understand the observations than the treatment.


```{r}
# no need to print all the columns, the result is obvious
loo_compare(b11H01a, b11H01b, b11H01c, criterion = "waic")
```

## 11H2 {-#prac11H2}

See sections 11.1.3 and 11.1.4 for details on how to do this practice.

```{r}
data(eagles, package = "MASS")
# bitwise number describing the conditions
bw <- c(
  "0" = "not large pir., not adult pir., not large vic.",
  "1" = "large pir., not adult pir., not large vic.",
  "2" = "not large pir., adult pir., not large vic.",
  "3" = "large pir., adult pir., not large vic.",
  "4" = "not large pir., not adult pir., large vic., ",
  "5" = "large pir., not adult pir., large vic.",
  "6" = "not large pir., adult pir., large vic.",
  "7" = "large pir., adult pir., large vic.")
dataEagles <- eagles %>%
  mutate(id = seq_len(n()),
         cond = (P == "L") * 1 + (A == "A") * 2 + (V == "L") * 4,
         idx = cond + 1,  # indx=ex to use with INLA
         desc = bw[as.character(cond)],
         cond = factor(cond, ordered = TRUE, levels = 0:7),
         desc = factor(desc)) %>%
  relocate(id)
rm(eagles)
skimr::skim(dataEagles)
```



### (a) {-}

The model

$$
\begin{align*}
y_i &\sim \mathcal{Binomial}(n_i, p_i) \\
logit(p_i) &= \alpha + \beta_P P_i + \beta_V V_i, + \beta_A A_i \\
\alpha &\sim \mathcal{N}(0, 1.5) \\
\beta_P, \beta_V, \beta_A &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$

#### `quap` fit {-}


```{r}
a_file <- here::here("fits", "m11H02a.rds")
m11H02a <- readRDS(file = a_file)
# m11H02a <- quap(
#   dataEagles,
#   flist = alist(
#     y ~ dbinom(n, p),
#     logit(p) <- a + bP[cond],
#     a ~ dnorm(0, 1.5),
#     bP[cond] ~ dnorm(0, 0.5))
# )
# saveRDS(m11H02a, file = a_file)
precis(m11H02a, depth = 2)
```

```{r}
gt <- list()
gt$m11H02a_link <- rethinking::link(m11H02a) %>%
  data.frame() %>%
  setNames(nm = paste("cond", seq_len(length(.)), sep = "_")) %>%
  pivot_longer(cols = everything(), names_to = "cond") %>%
  group_by(cond) %>%
  ggdist::mean_qi(.width = 0.89)
# glimpse(gt$m11H02a)

gt$m11H02a_sim <- rethinking::sim(m11H02a) %>%
  data.frame() %>%
  setNames(nm = paste("cond", seq_len(length(.)), sep = "_")) %>%
  pivot_longer(cols = everything(), names_to = "cond") %>%
  group_by(cond) %>%
  ggdist::mean_qi(.width = 0.89)
# glimpse(gt$m11H02a_sim)
```



#### `brm` fit {-}

see b11.2 in @kurtz2020b on how to do this.

```{r}
a_file <- here::here("fits", "b11H02a.rds")
b11H02a <- readRDS(file = a_file)
# b11H02a <- brm(data = dataEagles,
#                    family = binomial,
#                    formula = bf(y | trials(n) ~ a + b,
#                                 a ~ 1,
#                                 b ~ 0 + cond,
#                                 nl = TRUE),
#                    prior = c(
#                      prior(normal(0, 1.5), nlpar = a),
#                      prior(normal(0, 0.5), nlpar = b)),
#                    cores = detectCores(), seed = 12)
# b11H02a <- brms::add_criterion(b11H02a, criterion = c("waic", "loo"))
# saveRDS(b11H02a, file = a_file)
summary(b11H02a)
```

```{r}

gt$b11H02a_epred <- epred_draws(b11H02a, newdata = dataEagles) %>%
  as.data.frame() %>%
  select(cond, .epred) %>%
  group_by(cond) %>%
  ggdist::mean_qi(.width = 0.89) %>%
  inner_join(dataEagles[, c("cond", "n")], by = "cond") %>%
  mutate(prob = .epred / n,
         prob_lower = .lower / n,
         prob_upper = .upper / n)
# gt$b11H02a_epred


gt$b11H02a_linpred <- linpred_draws(b11H02a, newdata = dataEagles) %>%
  as.data.frame() %>%
  select(cond, .linpred) %>%
  group_by(cond) %>%
  ggdist::mean_qi(.width = 0.89)
# gt$b11H02a_linpred

gt$b11H02a_predict <- predicted_draws(b11H02a, newdata = dataEagles) %>%
  as.data.frame() %>%
  select(cond, .prediction) %>%
  group_by(cond) %>%
  ggdist::mean_qi(.width = 0.89) %>%
  inner_join(dataEagles[, c("cond", "n")], by = "cond") %>%
  mutate(prob = .prediction / n,
         prob_lower = .lower / n,
         prob_upper = .upper / n)
# gt$b11H02a_predict
```



```{r}
fixef(b11H02a)[,"Estimate"]
gtools::inv.logit(fixef(b11H02a)[,"Estimate"])
inv_logit_scaled(fixef(b11H02a)[,"Estimate"])
```


#### `inla` fit {-}


```{r}
a_file <- here::here("fits", "i11H02a.rds")
i11H02a <- readRDS(file = a_file)
# i11H02a <- inla(data = dataEagles,
#                 formula =  y ~ 1 + f(idx, model = "iid",  
#                                      hyper = list(prec = list(param = c(0.001, 0.001)))),
#                 family = "binomial",
#                 Ntrials = n,
#                 control.family = list(link = "logit"),
#                 control.predictor = list(link = 1, compute = TRUE),
#                 control.compute = list(dic = TRUE, cpo = TRUE, waic = TRUE))
# saveRDS(i11H02a, file = a_file)
summary(i11H02a)
```


```{r}
i11H02a$summary.fixed
```

which gives the fitted values

```{r}
i11H02a$summary.fitted.values
```


#### Conclusion {-}

and we can now compare the 3 fits using WAIC.

```{r}
data.frame(
  fit = c("quap", "brm", "inla"),
  waic = c(as.double(rethinking::WAIC(m11H02a)["WAIC"]), 
           brms::WAIC(b11H02a)$estimates["waic", "Estimate"],
           i11H02a$waic$waic),
  se = c(as.double(rethinking::WAIC(m11H02a)["std_err"]),
         brms::WAIC(b11H02a)$estimates["waic", "SE"],
         sd(i11H02a$waic$local.waic))
  ) %>%
  mutate(across(where(is.double), .fns = ~round(., digits = 2))) %>%
  arrange(waic)
```

`brm` has the lowest WAIC. `quap` has virtually the same WAIC, well
within the santdard error. `inla` seems to be doing particularly worse than the 
others. Given the speed of `quap` with the same results, it would be the favorite
choice in a practical setting (my opinion).

The reason why `inla` is doing worse seems to be illustrated in the following table
which shows that its estimates **are outside of the range determined by `quap` and
`brm`**. It also shows that `inla` has estimates closer to the observed values
which might indicate overfitting . . . hence the worse WAIC.


```{r}
# Make sure the data.frame is sorted in increasing order of "cond"
# to match the fitted data
fits <- list()
fits$df <- dataEagles %>%
  select(y, n, cond) %>%
  arrange(cond) %>%  # important: make sure data is arrange by cond
  mutate(p_obs = y / n,
         p_quap = gt$m11H02a_link$value,
         p_brm = gtools::inv.logit(gt$b11H02a_linpred$.linpred),
         p_inla = i11H02a$summary.fitted.values$mean) %>%
  mutate(mu_quap = p_quap * n,
         mu_brm = p_brm * n,
         mu_inla = p_inla * n) %>%
  mutate(across(.cols = starts_with("p"), .fns = ~round(., digits = 2)),
         across(.cols = starts_with("mu"), .fns = ~round(., digits = 0)))
fits$lng <- fits$df %>%
  select(cond, p_obs, p_quap, p_brm, p_inla) %>%
  pivot_longer(cols = starts_with("p_"), names_to = "fit", values_to = "p")
ggplot(fits$lng, aes(x = cond, y = p, color = fit, shape = fit)) +
  geom_point(size = 2) +
  ggrepel::geom_text_repel(mapping = aes(label = p), size = 3) +
  scale_color_paletteer_d("awtools::spalette", direction = -1) +
  scale_shape_ordinal() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  labs(title = "p values by fit", y = NULL)
```



### (b) {-}


The predicted probabilities

```{r}
p <- list()
p$prob <- ggplot(dataEagles, aes(x = cond, y = y / n)) +
  geom_linerange(gt$b11H02a_predict, 
                 mapping = aes(x = cond, y = prob,
                               ymin = prob_lower, ymax = prob_upper),
                 size = 5, color = "gold3") +
  geom_pointrange(gt$b11H02a_epred,
                  mapping = aes(x = cond, y = prob, 
                                ymin = prob_lower, ymax = prob_upper),
                  size = 1, shape = 1, color = "magenta") +
  geom_point(size = 3, color = "forestgreen") +
  ggrepel::geom_text_repel(aes(label = round(y / n, 2))) +
  ggrepel::geom_text_repel(gt$b11H02a_epred, 
                           mapping = aes(x = cond, y = prob, 
                                         label = round(prob, 2))) +
  theme(legend.position = "none") +
  labs(title = "Posterior predictions of probabilities by condition",
       subtitle = "Fit b11H02a with 89% interval",
       x = "condition", y = "probability")
p$prob
```

and he predicted success counts

```{r}
p$count <- ggplot(dataEagles, aes(x = cond, y = y)) +
  geom_linerange(gt$b11H02a_predict, 
                 mapping = aes(x = cond, y = .prediction,
                               ymin = .lower, ymax = .upper),
                 size = 5, color = "lightseagreen") +
  geom_pointrange(gt$b11H02a_epred,
                  mapping = aes(x = cond, y = .epred, 
                                ymin = .lower, ymax = .upper),
                  size = 1, shape = 1, color = "darkviolet") +
  geom_point(size = 3, color = "brown") +
  ggrepel::geom_text_repel(aes(label = y)) +
  ggrepel::geom_text_repel(gt$b11H02a_epred, 
                           mapping = aes(x = cond, y = .epred, 
                                         label = round(.epred, 0))) +
  theme(legend.position = "none") +
  labs(title = "Posterior predictions of success count by condition",
       subtitle = "Fit b11H02a with 89% interval",
       x = "condition", y = "count")
p$count
```


The difference between the predicted probabilities and the predicted counts
is mainly because of the *effect size* a large difference in probability
might not translate in a large difference in count as shown in the computed
differences in a) above.

### (c) {-}


```{r}
a_file <- here::here("fits", "b11H02b.rds")
b11H02b <- readRDS(file = a_file)
# b11H02b <- brm(data = dataEagles,
#                    family = binomial,
#                    formula = bf(y | trials(n) ~ a + b + A*P,
#                                 a ~ 1,
#                                 b ~ 0 + cond,
#                                 nl = TRUE),
#                    prior = c(
#                      prior(normal(0, 1.5), nlpar = a),
#                      prior(normal(0, 0.5), nlpar = b)),
#                    cores = detectCores(), seed = 12)
# b11H02b <- brms::add_criterion(b11H02b, criterion = c("waic", "loo"))
# saveRDS(b11H02b, file = a_file)
summary(b11H02b)
```

```{r}
loo_compare(b11H02a, b11H02b, criterion = "waic")
```


```{r}
model_weights(b11H02a, b11H02b, weights = "waic")
```

Conclusion: Adding the variables $A = age of pirate$ and $P = pirate's size$
worsens the WAIC.  This is caused by the fact that the combinations of variables
$A$ and $P$ are already taken into account in the variable $cond$.  The added
parameter $A*P$ adds no information that is not already in $cond$ and increases 
the penalty term of the WAIC. See section 7.4.2 for more details on waic.


## 11H3 {-#prac11H3}

See section 11.2.1 for details on how to do this practice.

* We keep the variable $PCTCOVER$ as is since it is arguably already scaled
as a percentage. 
* We log transform $FORESTAGE$ to reduce the large variations.
* We log transform $SALAMAN$ after adding 1 to avoid $log(0)$ with `log1p()`
* We remove 1 line, site 30, that has **zero everywhere** and has the only
$FORESTAGE = 0$. It is clearly an invalid row of data.


```{r}
data(salamanders)
dataSalam <- salamanders %>%
  filter(FORESTAGE > 0) %>%
  mutate(site = as.factor(SITE),
         age = log(FORESTAGE),
         cover = log(PCTCOVER))
rm(salamanders)
skimr::skim(dataSalam)
```

```{r}
p <- list()
p$points <- ggplot(dataSalam, aes(x = PCTCOVER, y = SALAMAN, color = FORESTAGE)) +
  geom_point() +
  scale_color_paletteer_c("pals::coolwarm") +
  theme(legend.position = c(0.2,0.7)) +
  labs(title = "Salamander vs pct cover")
p$count <- ggplot(dataSalam, aes(x = SALAMAN)) +
  geom_histogram(aes(fill = ..count..)) +
  scale_fill_paletteer_c("pals::warmcool") +
  theme(legend.position = "none") +
  labs(title = "Count of salamnder")
wrap_plots(p)
```



### Priors: Intercept {-}

Since the model is

$$
\begin{align*}
salamanders_i &\sim \mathcal{Poisson(\lambda_i)} \\
\log{\lambda_i} &= a \\
a &\sim \mathcal{N(mean=m, sd=s)}
\end{align*}
$$

then we know that $\lambda \sim \mathcal{LogNormal(meanlog = m, sdlogs = s)}$
and we need to simulate for different values of $m$ and $s$ using the
`rlnorm()` function.

However since we are using `simstudy` as our prime simulation
tools, lets do it with `simstudy`.


```{r}
sim <- list(nsamples = 100)
sim <- within(sim, {
  defs <- defData(varname = "a", dist = "normal", formula = "..m", variance = "..v")
  defs <- defData(defs, varname = "lambda", dist = "nonrandom",
                formula = "a", link = "log")
  grid <- expand.grid(mean = c(1, 2, 3), sd = c(0.5, 1, 1.5)) %>%
    mutate(params = sprintf("m=%.2f, s=%.2f", mean, sd))
  lst <- lapply(X = seq_len(nrow(grid)), FUN = function(i) {
    m <- grid$mean[i]
    s <- grid$sd[i]
    v <- s^2
    set.seed(as.integer(as.Date("2021-12-11")))
    genData(n = nsamples, dtDefs = defs) %>%
      as.data.frame() %>%
      mutate(mean = m,
             sd = s,
             params = grid$params[i])
    })
  
    data <- do.call(rbind, lst)
    })

ggplot(sim$data, aes(x = lambda, fill = params, color = params)) +
  geom_density(aes(y = ..scaled..), size = 1.5) +
  scale_fill_paletteer_d("ggthemes::Classic_Cyclic") +
  scale_color_paletteer_d("ggthemes::Classic_Cyclic") +
  coord_cartesian(xlim = c(0, 10)) +
  theme(legend.position = "none",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  labs(title = "Density of intercept prior", x = NULL, y = NULL) +
  facet_grid(sprintf("sd of a=%.1f", sd) ~ sprintf("mean of a=%.1f", mean))
```

For the prior of the intercept we select $a \sim \mathcal{N(mean=2, sd=0.5)}$.
We justify this choice by the following

* The nb of salamanders should be greater than 0 and easily higher tha 5
* The nb of salamanders is unlikely to be be higher than 20 $exp(1) \approx 20$


### Priors: Slope {-}


```{r}
sim <- list(nsamples = 100, npreds = 50)
sim <- within(sim, {
  # define the model
  defs <- defData(varname = "a", dist = "normal", formula = 2,
                  variance = 0.5^2)
  defs <- defData(defs, varname = "b", dist = "normal", 
                  formula = "..m", variance = "..v")
  
  # generate data using the grid f specs
  grid <- expand.grid(mean = 0, 
                      sd = seq(from = 0.1, to = 0.6, by = 0.1)) %>%
    mutate(params = sprintf("m=%.1f, s=%.1f", mean, sd))
  
  lst_log <- lapply(seq_len(nrow(grid)), FUN = function(i) {
    set.seed(as.integer(as.Date("2021-12-11")))
    m <- grid$mean[i]
    s <- grid$sd[i]
    v <- s^2
    genData(n = nsamples, dtDefs = defs) %>%
      as.data.frame() %>%
      mutate(mean = m,
             sd = s,
             params = grid$params[i]) %>%
      tidyr::expand(nesting(id, a, b, mean, sd, params),
           log_predictor = seq(from = log(1), to = log(100), length.out = npreds)) %>%
      mutate(lambda = exp(a + b * log_predictor))
    })
  names(lst_log) <- grid$params
  data_log <- do.call(rbind, lst_log)
})


p$log <- ggplot(sim$data_log, aes(x = log_predictor, y = lambda, group = id, color = params)) +
  geom_line() +
  scale_fill_paletteer_d("LaCroixColoR::PassionFruit") +
  scale_x_continuous(labels = scales::label_percent(scale = 1)) +
  coord_cartesian(ylim = c(0, 200)) +
  theme(legend.position = "none") +
  labs(title = "Prior predictive distribution of the mean nb of salamanders",
       x = "Log pct of cover", y = "mean nb of salamanders") +
  facet_wrap(. ~ params, scales = "free_y")
# p$log

p$hline <- 100
p$nat <- ggplot(sim$data_log, aes(x = exp(log_predictor), y = lambda, group = id, color = params)) +
  geom_line() +
  geom_hline(yintercept = p$hline, color = "darkblue", linetype = "dashed", size = 1) +
  scale_fill_paletteer_d("LaCroixColoR::PassionFruit") +
  scale_x_continuous(labels = scales::label_percent(scale = 1)) +
  coord_cartesian(ylim = c(0, 200)) +
  theme(legend.position = "none") +
  labs(title = "Prior predictive distribution of the mean (lambda)",
       x = "Pct of cover", y = "mean nb of salamanders") +
  facet_wrap(. ~ params, scales = "free_y")
p$nat
```

For the prior of the slope we select $b \sim \mathcal{N(mean=0, sd=0.5)}$
which seems to offer a reasonably good coverage.


### (a) {-}

#### `quap` {-}

```{r}
a_file <- here::here("fits", "m11H03a.rds")
m11H03a <- readRDS(file = a_file)
# m11H03a <- quap(
#   data = dataSalam,
#   flist = alist(
#     SALAMAN ~ dpois(lambda),
#     log(lambda) <- a + b * cover,
#     a ~ dnorm(2, 0.5),
#     b ~ dnorm(0, 0.5))
#   )
# saveRDS(m11H03a, file = a_file)
precis(m11H03a)
```


#### `brm` {-}


```{r}
a_file <- here::here("fits", "b11H03a.rds")
b11H03a <- readRDS(a_file)
# b11H03a <- brm(data = dataSalam,
#              family = poisson,
#              formula = SALAMAN ~ 1 + cover,
#              prior = c(prior(normal(2, 0.5), class = Intercept),
#                        prior(normal(0, 0.5), class = b)),
#              cores = detectCores())
# b11H03a <- brms::add_criterion(b11H03a, criterion = c("waic", "loo"))
# saveRDS(b11H03a, file = a_file)
```


```{r}
summary(b11H03a)
```


#### `inla` {-}


```{r}
a_file <- here::here("fits", "i11H03a.rds")
i11H03a <- readRDS(file = a_file)
# i11H03a <- inla(data = dataSalam,
#                 formula =  SALAMAN ~ 1 + cover,
#                 family = "poisson",
#                 control.family = list(link = "log"),
#                 control.fixed = list(mean.intercept = 2, prec.intercept = (1 / 0.5^2),
#                                      mean = 0, prec = 1 / 0.5^2),
#                 control.predictor = list(link = 1, compute = TRUE),
#                 control.compute = list(dic = TRUE, cpo = TRUE, waic = TRUE))
# saveRDS(i11H03a, file = a_file)
```


and the summary is very similar to what `quap` and `brm` gave

```{r}
summary(i11H03a)
```

#### `quap`, `brm`, `inla` {-}

`quap` and `brm` give the same result and `inla` seems to be much less variable. Given
`inla` speed and precision it would be my personal choice here.

```{r}
data.frame(
  fit = c("quap", "brm", "inla"),
  waic = c(as.double(rethinking::WAIC(m11H03a)["WAIC"]), 
           brms::WAIC(b11H03a)$estimates["waic", "Estimate"],
           i11H03a$waic$waic),
  se = c(as.double(rethinking::WAIC(m11H03a)["std_err"]),
         brms::WAIC(b11H03a)$estimates["waic", "SE"],
         sd(i11H03a$waic$local.waic))) %>%
  mutate(across(where(is.double), .fns = ~round(., digits = 2))) %>%
  arrange(waic)
```




#### Outlier {-}


```{r}
loo(b11H03a)
```

see section 7.5.2, p. 232 for more details on this


```{r}
p <- list()
p$bound <- 0.4
p$df <- data.frame(paretoK = b11H03a$criteria$loo$diagnostics$pareto_k,
                   penaltyWAIC = b11H03a$criteria$waic$pointwise[, "p_waic"],
                   SITE = pull(dataSalam, SITE),
                   SALAMAN = pull(dataSalam, SALAMAN)) %>%
  mutate(out = paretoK >= 0.4)
p$out <- ggplot(p$df, aes(x = paretoK, y = penaltyWAIC, color = out)) +
  geom_point() +
  geom_vline(xintercept = 0.4, linetype = "dashed", color = "violetred") +
  ggrepel::geom_text_repel(aes(label = SITE), size = 3) +
  scale_x_continuous(breaks = scales::breaks_width(width = 0.1)) +
  scale_color_manual(values = c("TRUE" = "violetred", "FALSE" = "royalblue")) +
  theme(legend.position = "none") +
  labs(title = "Outlier diagnostic", subtitle = "Fit b11H03a")
p$out
```

Conclusion: Site # 17 seems to be highly influental (high PSIS) and unlikely
to occur (high WAIC penalty term). It has 2 salamanders, normally it should be 
zero.

```{r}
loo(b11H03a) %>% 
  pareto_k_ids(threshold = 0.4)
```

```{r}
fitted <- list()
fitted <- within(fitted, {
  data <- epred_draws(b11H03a, newdata = dataSalam) %>%
    as.data.frame()
  summ <- data %>%
    select(PCTCOVER, SALAMAN) %>%
    group_by(PCTCOVER) %>%
    ggdist::mean_qi(.width = 0.89)
  })
# str(fitted$summ)

p <- list()
p$all <-  ggplot(dataSalam, aes(x = PCTCOVER, y = SALAMAN)) +
# p$all <-  ggplot(dataSalam) +
  geom_lineribbon(fitted$summ, 
                  mapping = aes(x = PCTCOVER, y = SALAMAN, ymin = .lower, ymax = .upper),
                  inherit.aes = FALSE,
                  color = "firebrick" , fill = "lightcoral") +
  geom_point(color = "darkblue") +
  labs(title = "Expected count with 89% interval",
       subtitle = "b11H03a")
p$sel <- p$all +
  coord_cartesian(xlim = c(75, 100)) +
  labs(title = "Expected count with 89% interval with zoom on 75 <= PCTCOVER <= 100",
       subtitle = "b11H03a")

wrap_plots(p, ncol = 1)
```


Conclusion: The expected count barely reaches the actual values for forest
coverage above 75%.  Probably caused by the fact that all data is gathered above
75% and almost always zero otherwise.


### (b) {-}

Using FORESTAGE as a predictor we have the model

$$
\begin{align*}
salamanders_i &\sim \mathcal{Poisson(\lambda_i)} \\
\log{\lambda_i} &= \alpha + \beta \log(FORESTAGE_i) \\
\alpha &\sim \mathcal{N(mean=4, sd=0.5)} \\
\beta &\sim \mathcal{N(mean=0, sd=0.5)}
\end{align*}
$$


#### priors {-}


##### Intercept prior {-}

```{r}
sim <- list(nsamples = 100)
sim <- within(sim, {
  defs <- defData(varname = "a", dist = "normal", formula = "..m", variance = "..v")
  defs <- defData(defs, varname = "lambda", dist = "nonrandom",
                formula = "a", link = "log")
  grid <- expand.grid(mean = log(c(25, 50, 75)), sd = log(c(1.5, 2, 3))) %>%
    mutate(params = sprintf("m=%.2f, s=%.2f", mean, sd))
  lst <- lapply(X = seq_len(nrow(grid)), FUN = function(i) {
    m <- grid$mean[i]
    s <- grid$sd[i]
    v <- s^2
    set.seed(as.integer(as.Date("2021-12-11")))
    genData(n = nsamples, dtDefs = defs) %>%
      as.data.frame() %>%
      mutate(mean = m,
             sd = s,
             params = grid$params[i])
    })
  
    data <- do.call(rbind, lst)
    })

ggplot(sim$data, aes(x = lambda, fill = params, color = params)) +
  geom_density(aes(y = ..scaled..), size = 1.5) +
  scale_fill_paletteer_d("awtools::bpalette") +
  scale_color_paletteer_d("awtools::bpalette") +
  coord_cartesian(xlim = c(0, 100)) +
  theme(legend.position = "none",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  labs(title = "Density of intercept prior", x = NULL, y = NULL) +
  facet_grid(sprintf("sd of a=%.1f", sd) ~ sprintf("mean of a=%.1f", mean))
```

For the prior of the intercept we select $a \sim \mathcal{N(mean=4, sd=0.5)}$.



##### Slope prior {-}

```{r}
sim <- list(nsamples = 100, 
            npreds = 50)
sim <- within(sim, {
  # define the params
  defs <- defData(varname = "a", dist = "normal", formula = 4,
                  variance = 0.5^2)
  defs <- defData(defs, varname = "b", dist = "normal", 
                  formula = "..m", variance = "..v")
  
  # generate data using the grid f specs
  grid <- expand.grid(mean = 0, 
                      sd = seq(from = 0.1, to = 0.6, by = 0.1)) %>%
    mutate(params = sprintf("m=%.1f, s=%.1f", mean, sd))
  
  lst_log <- lapply(seq_len(nrow(grid)), FUN = function(i) {
    set.seed(as.integer(as.Date("2021-12-11")))
    m <- grid$mean[i]
    s <- grid$sd[i]
    v <- s^2
    genData(n = nsamples, dtDefs = defs) %>%
      as.data.frame() %>%
      mutate(mean = m,
             sd = s,
             params = grid$params[i]) %>%
      tidyr::expand(nesting(id, a, b, mean, sd, params),
           log_predictor = seq(from = log(2), to = log(700), length.out = npreds)) %>%
      mutate(lambda = exp(a + b * log_predictor))
    })
  names(lst_log) <- grid$params
  data_log <- do.call(rbind, lst_log)
})


p$log <- ggplot(sim$data_log, aes(x = log_predictor, y = lambda, group = id, color = params)) +
  geom_line() +
  scale_fill_paletteer_d("LaCroixColoR::PassionFruit") +
  scale_x_continuous(labels = scales::label_number()) +
  coord_cartesian(ylim = c(0, 200)) +
  theme(legend.position = "none") +
  labs(title = "Prior predictive distribution of the mean nb of salamanders",
       x = "Log of FORESTAGE", y = "mean nb of salamanders") +
  facet_wrap(. ~ params, scales = "free_y")
# p$log

p$hline <- 100
p$nat <- ggplot(sim$data_log, aes(x = exp(log_predictor), y = lambda, group = id, color = params)) +
  geom_line() +
  geom_hline(yintercept = p$hline, color = "darkblue", linetype = "dashed", size = 1) +
  scale_fill_paletteer_d("LaCroixColoR::PassionFruit") +
  scale_x_continuous(labels = scales::label_number()) +
  coord_cartesian(ylim = c(0, 200)) +
  theme(legend.position = "none") +
  labs(title = "Prior predictive distribution of the mean (lambda)",
       x = "FORESTAGE", y = "mean nb of salamanders") +
  facet_wrap(. ~ params, scales = "free_y")
p$nat
```


For the prior of the slope we select $b \sim \mathcal{N(mean=0, sd=0.5)}$
which seems to offer a reasonably good coverage.


#### `brm` {-}

```{r}
a_file <- here::here("fits", "b11H03b.rds")
b11H03b <- readRDS(a_file)
# b11H03b <- brm(data = dataSalam,
#              family = poisson,
#              formula = SALAMAN ~ 1 + age,
#              prior = c(prior(normal(4, 0.5), class = Intercept),
#                        prior(normal(0, 0.5), class = b)),
#              cores = detectCores())
# b11H03b <- brms::add_criterion(b11H03b, criterion = c("waic", "loo"))
# saveRDS(b11H03b, file = a_file)
```


```{r}
print(loo_compare(b11H03a, b11H03b), simplify = FALSE)
```

#### Inference {-}

Adding $FORESTAGE$ does not help since $FORESTAGE$ and $PCTCOVER$ 
seem to be colinear as described in section 6.1 of the textbook.  That
is, the older the forest is, the more coverage percentage we have.

Also, beyond the fact that salamanders appear when coverage is above 75%,
there seems to be little relationship between $SALAMAN$ and $PCTCOVER$ or
$FORESTAGE$ as shown in the following plots

```{r}
GGally::ggscatmat(dataSalam %>% select(-SITE, - age, - cover))
```

and comparing to log yields about the same results

```{r}
GGally::ggscatmat(dataSalam %>% select(-SITE, -PCTCOVER, -FORESTAGE))
```



```{r}
p <- list()
p$cover_age <- ggplot(dataSalam, aes(x = PCTCOVER, y = FORESTAGE)) +
  geom_point() +
  coord_cartesian(xlim = c(75, 100))

p$cover_salam <- ggplot(dataSalam, aes(x = PCTCOVER, y = SALAMAN)) +
  geom_point() +
  coord_cartesian(xlim = c(75, 100))

p$age_salam <- ggplot(dataSalam, aes(x = FORESTAGE, y = SALAMAN)) +
  geom_point()

p$logage_logsalam <- ggplot(dataSalam, aes(x = log(FORESTAGE), y = log(SALAMAN))) +
  geom_point()

wrap_plots(p, ncol = 2)
```


One way to improve the model could be to use a different rate for each $SITE$.

```{r}
a_file <- here::here("fits", "b11H03c.rds")
b11H03c <- readRDS(a_file)
# b11H03c <- brm(data = dataSalam,
#              family = poisson,
#              formula = bf(SALAMAN ~ a + b * cover,
#                           a + b ~ 0 + site,
#                           nl = TRUE),
#              prior = c(prior(normal(0, 2), nlpar = a),
#                        prior(normal(0, 0.5), nlpar = b)),
#              cores = detectCores())
# b11H03c <- brms::add_criterion(b11H03c, criterion = c("waic", "loo"))
# saveRDS(b11H03c, file = a_file)
```


```{r}
# summary(b11H03c)
```


```{r}
print(loo_compare(b11H03a, b11H03b, b11H03c), simplify = FALSE)
```

Which confirms that the using the $SITE$ improve the model noticeably.



```{r}
fitted <- list()
fitted <- within(fitted, {
  data <- epred_draws(b11H03c, newdata = dataSalam) %>%
    as.data.frame()
  summ <- data %>%
    select(PCTCOVER, SALAMAN) %>%
    group_by(PCTCOVER) %>%
    ggdist::mean_qi(.width = 0.89)
  })
# str(fitted$summ)

p <- list()
p$all <-  ggplot(dataSalam, aes(x = PCTCOVER, y = SALAMAN)) +
# p$all <-  ggplot(dataSalam) +
  geom_lineribbon(fitted$summ, 
                  mapping = aes(x = PCTCOVER, y = SALAMAN, ymin = .lower, ymax = .upper),
                  inherit.aes = FALSE,
                  color = "tan3" , fill = "moccasin") +
  geom_point(color = "darkblue") +
  labs(title = "Expected count with 89% interval",
       subtitle = "b11H03c")
p$sel <- p$all +
  coord_cartesian(xlim = c(75, 100)) +
  labs(title = "Expected count with 89% interval with zoom on 75 <= PCTCOVER <= 100",
       subtitle = "b11H03c")

wrap_plots(p, ncol = 1)
```



## 11H4 {-#prac11H4}

See section 11.1.4 for reference.

Also the paper from van der Lee and Ellemers can be found at 
[NWO](https://www.pnas.org/content/112/40/12349).

```{r}
data(NWOGrants)
disc_labels <- c("Chem", "Life", "Human", "Inter", "Med", "Physical", 
                 "Physics", "Soc",  "Tech")
dataGrants <- NWOGrants %>%
  mutate(female = as.integer(gender == "f"),
         disc = factor(discipline, labels = disc_labels),
         prop = round(awards / applications, 4))
rm(NWOGrants)
skimr::skim(dataGrants)
# levels(dataGrants$discipline)
# dataGrants
```
We consider  a mediation path through $discipline$ as requested in the question

```{r}
scm <- list()
scm <- within(scm, {
  dag <- dagify(
    discipline ~ gender,
    award ~ discipline,
  outcome = "award",
  exposure = "gender")
  
  plot <- dag %>% 
    tidy_dagitty(seed = as.integer(as.Date("2021-12-12")), layout = "sugiyama") %>%
    ggdag_status(color = status, text = TRUE, 
                       node_size = 14, text_size = 4, 
                       text_col = "black") +
    scale_color_manual(values = list(latent="mediumvioletred", 
                              exposure="lightcoral", 
                              outcome="cornflowerblue"), 
                       na.value = "honeydew3") +
    scale_fill_manual(values = list(latent="mediumvioletred", 
                              exposure="lightcoral", 
                              outcome="cornflowerblue"), 
                      na.value = "honeydew3") +
    ggdag::theme_dag_blank(panel.background = element_rect(fill="snow", color="snow")) +
    theme(legend.position = "none") +
    labs(title = "11H4 DAG")
})
scm$plot
```


and the model is

$$
\begin{align*}
awards_i &\sim \mathcal{Binomial}(n_i, p_i) \\
logit(p_i) &= \alpha_{gid[i]} + \delta_{disc[i]} \\
\alpha_j &\sim \mathcal{N}(mean, sd) \\
\delta_k &\sim \mathcal{N}(mean, sd)
\end{align*}
$$


### Priors {-}

This prior represents the probability of choosing a gender.  *It is also used to
represent the probability of choosing a discipline*.  We assume it should be
centered around 50% and be between 0 and 1. It gives about the same results as
in section 11.1.4. We choose $\sim \mathcal{N(0, 1.5)}$.

```{r}
sim <- list(nsamples = 100, npreds = 25)
sim <- within(sim, {
  defs <- defData(varname = "a", dist = "normal", formula = "..m", 
                  variance = "..v")
  defs <- defData(defs, varname = "p", dist = "nonrandom", formula = "a",
                  link = "logit")
  
  grid <- expand.grid(mean = c(0, 1, 2), sd = c(0.5, 1, 1.5)) %>%
    mutate(params = sprintf("m=%.1f, s=%.1f", mean, sd))
  lst <- lapply(X = seq_len(nrow(grid)), FUN = function(i) {
    m <- grid$mean[i]
    s <- grid$sd[i]
    v <- s^2
    set.seed(as.integer(as.Date("2021-12-12")))
    genData(n = nsamples, dtDefs = defs) %>%
      as.data.frame() %>%
      mutate(mean = m,
             sd = s,
             params = grid$params[i])
    })
  
    data <- do.call(rbind, lst)
})

ggplot(sim$data, aes(x = p, fill = params, color = params)) +
  geom_density(aes(y = ..scaled..), size = 1.5) +
  scale_fill_paletteer_d("ggthemes::Classic_10") +
  scale_color_paletteer_d("ggthemes::Classic_10") +
  theme(legend.position = "none",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  labs(title = "Prior probability", x = NULL, y = NULL) +
  facet_grid(sprintf("sd = %.1f", sd) ~ sprintf("mean = %.1f", mean))
```

### Fits {-}

As explained by McElreath in section 11.1.4, p. 345, using indexed predictors in
this case could be overparametrized.  I have tried using both methods, i.e.
indexed predictors or using dummy variables and they give the same results.

The dummy variables method is however more cumbersome to use, so my final choice
was with indexed predictors.

#### `quap` {-}


The model used to get the total effect of gender, both direct and indirect.

```{r}
a_file <- here::here(getwd(), "fits", "m11H04_g.rds")
m11H04_g <- readRDS(a_file)
# m11H04_g <- quap(
#  dataGrants,
#  flist = alist(
#    awards ~ dbinom(applications, p),
#    logit(p) <- a[gender],
#    a[gender] ~ dnorm(0, 1.5)))
# saveRDS(m11H04_g, file = a_file)
precis(m11H04_g, depth = 2)
```


The model used to get the total effect of discipline, both direct and indirect.

```{r}
a_file <- here::here(getwd(), "fits", "m11H04_d.rds")
m11H04_d <- readRDS(a_file)
# m11H04_d <- quap(
#   dataGrants,
#   flist = alist(
#     awards ~ dbinom(applications, p),
#     logit(p) <- a[disc],
#     a[disc] ~ dnorm(0, 1.5)))
# saveRDS(m11H04_d, file = a_file)
precis(m11H04_d, depth = 2)
```



The model used to get the direct effect of $gender$ taking into account
the $discipline$ effect.

```{r}
a_file <- here::here(getwd(), "fits", "m11H04_gd.rds")
m11H04_gd <- readRDS(a_file)
# m11H04_gd <- quap(
#   dataGrants,
#   flist = alist(
#     awards ~ dbinom(applications, p),
#     logit(p) <- bG[gender] + bD[disc],
#     bG[gender] ~ dnorm(0, 1.5),
#     bD[disc] ~ dnorm(0, 1.5)))
# saveRDS(m11H04_gd, file = a_file)
precis(m11H04_gd, depth = 2)
```




#### `brm` {-}




```{r}
a_file <- here::here("fits", "b11H04_g.rds")
b11H04_g <- readRDS(a_file)
# b11H04_g <- brm(data = dataGrants,
#       family = binomial,
#       formula = awards | trials(applications) ~ 0 + gender,
#       prior = prior(normal(0, 1.5), class = b),
#       cores = detectCores(), seed = 11)
# b11H04_g <- brms::add_criterion(b11H04_g, criterion = c("waic", "loo"))
# saveRDS(b11H04_g, file = a_file)
summary(b11H04_g)
```



```{r}
a_file <- here::here("fits", "b11H04_d.rds")
b11H04_d <- readRDS(a_file)
# b11H04_d <- brm(data = dataGrants,
#       family = binomial,
#       formula = awards | trials(applications) ~ 0 + disc,
#       prior= c(prior(normal(0, 1.5), class = b)),
#       cores = detectCores(), seed = 11)
# b11H04_d <- brms::add_criterion(b11H04_d, criterion = c("waic", "loo"))
# saveRDS(b11H04_d, file = a_file)
summary(b11H04_d)
```



```{r}
a_file <- here::here("fits", "b11H04_gd.rds")
b11H04_gd <- readRDS(a_file)
# b11H04_gd <- brm(data = dataGrants,
#       family = binomial,
#       formula = awards | trials(applications) ~ 0 + gender + disc,
#       prior= c(prior(normal(0, 1.5), class = b)),
#       cores = detectCores(), seed = 11)
# b11H04_gd <- brms::add_criterion(b11H04_gd, criterion = c("waic", "loo"))
# saveRDS(b11H04_gd, file = a_file)
summary(b11H04_gd)
```

#### `inla` {-}

```{r}
a_file <- here::here("fits", "i11H04_g.rds")
i11H04_g <- readRDS(file = a_file)
# i11H04_g <- inla(data = dataGrants,
#                 formula =  awards ~ -1 + gender,
#                 family = "binomial",
#                 Ntrials = applications,
#                 control.family = list(link = "logit"),
#                 control.predictor = list(link = 1, compute = TRUE),
#                 control.compute = list(dic = TRUE, cpo = TRUE, waic = TRUE))
# saveRDS(i11H04_g, file = a_file)
summary(i11H04_g)
```


```{r}
a_file <- here::here("fits", "i11H04_d.rds")
i11H04_d <- readRDS(file = a_file)
# i11H04_d <- inla(data = dataGrants,
#                 formula =  awards ~ -1 + disc,
#                 family = "binomial",
#                 Ntrials = applications,
#                 control.family = list(link = "logit"),
#                 control.predictor = list(link = 1, compute = TRUE),
#                 control.compute = list(dic = TRUE, cpo = TRUE, waic = TRUE))
# saveRDS(i11H04_d, file = a_file)
summary(i11H04_d)
```





```{r}
a_file <- here::here("fits", "i11H04_gd.rds")
i11H04_gd <- readRDS(file = a_file)
# i11H04_gd <- inla(data = dataGrants,
#                 formula =  awards ~ -1 + gender + disc,
#                 family = "binomial",
#                 Ntrials = applications,
#                 control.family = list(link = "logit"),
#                 control.predictor = list(link = 1, compute = TRUE),
#                 control.compute = list(dic = TRUE, cpo = TRUE, waic = TRUE))
# saveRDS(i11H04_gd, file = a_file)
summary(i11H04_gd)
i11H04_gd$summary.fixed
```

### `quap`, `brm`, `inla` {-}

```{r}
data.frame(
  fit = c("quap", "brm", "inla"),
  waic = c(as.double(rethinking::WAIC(m11H04_g)["WAIC"]), 
           brms::WAIC(b11H04_g)$estimates["waic", "Estimate"],
           i11H04_g$waic$waic),
  se = c(as.double(rethinking::WAIC(m11H04_g)["std_err"]),
         brms::WAIC(b11H04_g)$estimates["waic", "SE"],
         sd(i11H04_g$waic$local.waic))) %>%
  mutate(across(where(is.double), .fns = ~round(., digits = 2))) %>%
  arrange(waic)
```



```{r}
data.frame(
  fit = c("quap", "brm", "inla"),
  waic = c(as.double(rethinking::WAIC(m11H04_d)["WAIC"]), 
           brms::WAIC(b11H04_d)$estimates["waic", "Estimate"],
           i11H04_d$waic$waic),
  se = c(as.double(rethinking::WAIC(m11H04_d)["std_err"]),
         brms::WAIC(b11H04_d)$estimates["waic", "SE"],
         sd(i11H04_d$waic$local.waic))) %>%
  mutate(across(where(is.double), .fns = ~round(., digits = 2))) %>%
  arrange(waic)
```
Conclusion: `inla` seems to be a very good model with less `se`.


```{r}
summ <- list()
summ$quap <- precis(m11H04_gd, depth = 2) %>%
  data.frame() %>%
  tibble::rownames_to_column(var = "var") %>%
  mutate(var = c(levels(dataGrants$gender), levels(dataGrants$disc))) %>%
  select(var, mean, sd) %>%
  mutate(fit = "quap")
summ$brm <- fixef(b11H04_gd) %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "var") %>%
  mutate(var = c(levels(dataGrants$gender), levels(dataGrants$disc)[-1])) %>%
  select(var, mean = Estimate, sd = Est.Error) %>%
  mutate(fit = "brm")
summ$inla <- i11H04_gd$summary.fixed %>%
  tibble::rownames_to_column(var = "var") %>%
  mutate(var = c(levels(dataGrants$gender), levels(dataGrants$disc)[-1])) %>%
  select(var, mean, sd) %>%
  mutate(fit = "inla")
# str(summ$quap)
# str(summ$brm)
# str(summ$inla)
summ$all <- do.call(rbind, list(quap = summ$quap, brm = summ$brm, inla = summ$inla)) %>%
  pivot_longer(cols = c("mean", "sd"), names_to = "moment", values_to = "value") %>%
  pivot_wider(id_cols = "var", names_from = c("moment" ,"fit"), values_from = "value")
# str(summ$all)
```



```{r}
summ$gt <- summ$all %>% 
  gt::gt(rowname_col = "var") %>%
  tab_header(title = "Comparing values from quap, brm and inla",
             subtitle = "Practice 11H4") %>%
  tab_spanner_delim(delim = "_", columns = everything(), split = "first") %>%
  cols_align(align = "center", columns = everything()) %>%
  fmt_number(columns = everything(), decimals = 2) %>%
  data_color(columns = everything(), colors = "mintcream") %>%
  tab_style(style = cell_borders(
    sides = c("left"),
    color = "grey90",
    weight = px(1.5),
    style = "solid"),
    locations = cells_body(
      columns = "sd_quap"
    )) %>%
  # tab_style(style = cell_borders(
  #   sides = c("left"),
  #   color = "grey90",
  #   weight = px(1.5),
  #   style = "solid"),
  #   locations = cells_column_spanners(
  #     spanners = c("sd")
  #   )) %>%
  tab_style(style = cell_borders(
    sides = c("left"),
    color = "grey90",
    weight = px(1.5),
    style = "solid"),
    locations = cells_column_labels(
      columns = c("sd_quap")
    )) %>%
  tab_options(heading.background.color = "darkslategray",
    heading.title.font.weight = "bold",
    heading.subtitle.font.weight = "bold",
    column_labels.background.color = "seashell",
    column_labels.font.weight = "bold",
    stub.background.color = "seashell")
summ$gt
```

### Inferences {-}

See section 11.1.4, in particular p. 342-345, for discussion and guidelines
by McElreath.

We will use the result from the `brm` fit to answer the questions.

The total effect, that is, including any confound is covered by the model
`b11H04_g`

```{r}
summary(b11H04_g)
```

Using the total effect of gender, the predicted rates are pretty much always outside of the 
observed range. 

```{r}
predict <- list()
predict <- within(predict, {
  data <- predicted_draws(b11H04_g, newdata = dataGrants) %>%
    as.data.frame() %>%
    select(disc, gender, .prediction) %>%
    # summarize_draws(mean, ~quantile2(.,  probs = c(0.05, 0.95)))
    # select(discipline, gender, applications, awards) %>%
    group_by(disc, gender) %>%
    ggdist::mean_qi() %>%
    mutate(applications = dataGrants$applications,
           prop = .prediction / applications,
           prop.lower = .lower / applications,
           prop.upper = .upper / applications)
})
ggplot(predict$data %>% filter(disc != "Human"), 
       aes(x = gender, y = prop, ymin = prop.lower, ymax = prop.upper,
                         color = gender)) +
  geom_pointinterval(shape = 1, size = 1, fatten_point = 3, position = "dodge") +
  geom_point(dataGrants %>% filter(disc != "Human"), 
             mapping = aes(x = gender, y = prop, color = gender),
             inherit.aes = FALSE, position = "dodge", shape = 19) +
  scale_color_manual(values = c("f" = "purple", "m" = "navyblue")) +
  theme(legend.position = "bottom") +
  labs(title = "Admission rate considering only gender (excluding Humanities)",
       subtitle = "Model b11H04a, observed values are full circle, predictions are round circle", 
       x = NULL) +
  facet_wrap(. ~ disc)
```


Using $discipline$ as well as $gender$, `b11H04_gd` does not improve the model. It
actually makes it worse.  Actually, given the se it seems all 3 models have similar
performances.


```{r}
loo_compare(b11H04_g, b11H04_d, b11H04_gd)
```

The summary for `b11H04_gd` is

```{r}
summary(b11H04_gd)
```

and the plot of `b11H04_gd` is virtually identical to `b11H04_g`

```{r}
predict <- list()
predict <- within(predict, {
  data <- predicted_draws(b11H04_gd, newdata = dataGrants) %>%
    as.data.frame() %>%
    select(disc, gender, .prediction) %>%
    # summarize_draws(mean, ~quantile2(.,  probs = c(0.05, 0.95)))
    # select(discipline, gender, applications, awards) %>%
    group_by(disc, gender) %>%
    ggdist::mean_qi() %>%
    mutate(applications = dataGrants$applications,
           prop = .prediction / applications,
           prop.lower = .lower / applications,
           prop.upper = .upper / applications)
})
# glimpse(predict$data)
head(predict$data)
# summary(b11H04b)
# dataGrants
```

and the plots


```{r}
ggplot(predict$data %>% filter(disc != "Human"), 
       aes(x = gender, y = prop, ymin = prop.lower, ymax = prop.upper,
                         color = gender)) +
  geom_pointinterval(shape = 1, size = 1, fatten_point = 3, position = "dodge") +
  geom_point(dataGrants %>% filter(disc != "Human"), 
             mapping = aes(x = gender, y = prop, color = gender),
             inherit.aes = FALSE, position = "dodge", shape = 19) +
  scale_color_manual(values = c("f" = "purple", "m" = "navyblue")) +
  theme(legend.position = "none") +
  labs(title = "Admission rate considering gender and discipline (excluding humanities)",
       subtitle = "Model b11H04b", x = NULL) +
  facet_wrap(. ~ disc)
```


#### Conclusions {-}

Using $discipline$ and $gender$ has little impact on the overall results. As
a matter of fact, both models with gender only `b11H04_g` and gender as well as
discipline `b11H04_gd` show $gender$ is the most
influential coefficient with $gender = female$ having negative
effect on being awarded a grant.


```{r}
summary(b11H04_gd)
```


We observe that all model have about the same waic. This could point to
the existence of a counfound with a significant relation with both
gender and discipline.


```{r}
loo_compare(b11H04_g, b11H04_d, b11H04_gd, criterion = "waic")
```


## 11H5 {-#prac11H5}



```{r}
scm <- list()
scm <- within(scm, {
  dag <- dagify(
    gender ~ stage,
    discipline ~ gender + stage,
    award ~ discipline + gender,
  outcome = "award",
  exposure = "gender")
  
  plot <- dag %>% 
    tidy_dagitty(seed = as.integer(as.Date("2021-12-12")), layout = "sugiyama") %>%
    ggdag_status(color = status, text = TRUE, 
                       node_size = 14, text_size = 4, 
                       text_col = "black") +
    scale_color_manual(values = list(latent="mediumvioletred", 
                              exposure="lightcoral", 
                              outcome="cornflowerblue"), 
                       na.value = "honeydew3") +
    scale_fill_manual(values = list(latent="mediumvioletred", 
                              exposure="lightcoral", 
                              outcome="cornflowerblue"), 
                      na.value = "honeydew3") +
    ggdag::theme_dag_blank(panel.background = element_rect(fill="snow", color="snow")) +
    theme(legend.position = "none") +
    labs(title = "11H5 DAG")
})
scm$plot
```

**What happens when we condition on discipline?**

By conditioning on $discipline$, $award$ is still dependent of $gender$ as the 
backdoor through $stage$. The backdoor is shown in the conditional
independencies of the DAG

```{r}
impliedConditionalIndependencies(scm$dag)
```

ans as a result only conditioning on $stage$ could make $award$ independent of
$gender$

```{r}
adjustmentSets(scm$dag, exposure = "gender", outcome = "award")
```


**Is it possible for gender a direct effect by regressing on both gender and
discipline?**

No because both $gender$ and $discipline$ are not independent as theyr are
influenced by $stage$.

This is shown by the model in the previous model with gender only

```{r}
b11H04_g
```

which remains very similarly determined by gender even once we include
$discipline$ as the previous practive model showed

```{r}
b11H04_gd
```



## 11H6 {-#prac11H6}

The data a lot of missing data. To avoid issues the rows
with missing social_learning, brain or research_effort are removed.


```{r}
data(Primates301)
dataPrimates <- Primates301 %>%
  drop_na(social_learning, brain, research_effort) %>%
  mutate(brain_log = log(brain),
         research_log = log(research_effort))
rm(Primates301)
skimr::skim(dataPrimates)
```


## a) {-}


```{r}
a_file <- here::here("fits", "b11H06a.rds")
b11H06a <- readRDS(a_file)
# b11H06a <- brm(data = dataPrimates,
#              family = poisson,
#              formula = social_learning ~ 1 + brain_log,
#              prior = c(prior(normal(0, 0.5), class = Intercept),
#                        prior(normal(0, 0.5), class = b)),
#              cores = detectCores())
# b11H06a <- brms::add_criterion(b11H06a, criterion = c("waic", "loo"))
# saveRDS(b11H06a, file = a_file)
```


```{r}
summary(b11H06a)
```


```{r}
fitted <- list()
fitted <- within(predict, {
  newdata <- data.frame(brain_log = seq_range(dataPrimates$brain_log, n = 50))
  data <- epred_draws(object = b11H06a, newdata = newdata)
  p <- ggplot(dataPrimates, aes(x = brain_log, y = social_learning)) +
    stat_lineribbon(data, mapping = aes(x = brain_log, y = .epred),
                    point_interval = median_qi, .width = c(0.055, 0.945),
                    fill = "lightblue", color = "blue", size = 1) +
    geom_point(color = "darkorchid4") +
    scale_x_continuous(breaks = scales::breaks_extended(7),
                       labels = function(x) {
                         scales::label_number_si(accuracy = 1)(exp(x))
                         }) +
    labs(title = "Social Learning vs Brain Size",
         subtitle = "11H6 a)", x = "brain size")
    
})
fitted$p
```

Interpretation: brain size has little efect until it reach 100, then its effect
exponential.


## b) {-}



```{r}
a_file <- here::here("fits", "b11H06b.rds")
b11H06b <- readRDS(a_file)
# b11H06b <- brm(data = dataPrimates,
#              family = poisson,
#              formula = social_learning ~ 1 + research_log,
#              prior = c(prior(normal(0, 0.5), class = Intercept),
#                        prior(normal(0, 0.5), class = b)),
#              cores = detectCores())
# b11H06b <- brms::add_criterion(b11H06b, criterion = c("waic", "loo"))
# saveRDS(b11H06b, file = a_file)
```


```{r}
summary(b11H06b)
```


```{r}
fitted <- list()
fitted <- within(predict, {
  newdata <- data.frame(research_log = seq_range(dataPrimates$research_log, n = 50))
  data <- epred_draws(object = b11H06b, newdata = newdata)
  p <- ggplot(dataPrimates, aes(x = research_log, y = social_learning)) +
    stat_lineribbon(data, mapping = aes(x = research_log, y = .epred),
                    point_interval = median_qi, .width = c(0.055, 0.945),
                    fill = "bisque", color = "bisque4", size = 1) +
    geom_point(color = "darkorchid4") +
    scale_x_continuous(breaks = scales::breaks_extended(7),
                       labels = function(x) {
                         scales::label_number_si(accuracy = 1)(exp(x))
                         }) +
    labs(title = "Social Learning vs Research Effort",
         subtitle = "11H6 b)", x = "research effort")
    
})
fitted$p
```

Conclusion: There is little difference between this model and the previous one
statistically speaking.  However the model `11H06b`, with research effort as a
predictor seems to be a **collider bias** as described in the introduction
of chapter 6, p. 161-162.Thta is, it can be interpreted as the fact that species with more 
social learning are selected by more researcher which causes a collider effect.
See the DAG in part c) below.

c) {-}


```{r}
scm <- list()
scm <- within(scm, {
  dag <- dagify(
    social_learning ~ research_effort,
    social_learning ~ brain_size,
  outcome = "social_learning",
  exposure = c("research_effort", "brain_size"))
  
  plot <- dag %>% 
    tidy_dagitty(layout = "sugiyama") %>%
    ggdag_status(color = status, text = TRUE, 
                       node_size = 14, text_size = 4, 
                       text_col = "black") +
    scale_color_manual(values = list(latent="mediumvioletred", 
                              exposure="lightcoral", 
                              outcome="cornflowerblue"), 
                       na.value = "honeydew3") +
    scale_fill_manual(values = list(latent="mediumvioletred", 
                              exposure="lightcoral", 
                              outcome="cornflowerblue"), 
                      na.value = "honeydew3") +
    ggdag::theme_dag_blank(panel.background = element_rect(fill="snow", color="snow")) +
    theme(legend.position = "none") +
    labs(title = "11H6 c) DAG")
})
scm$plot
```

to illustrate the collider, let take into account both $brain size$ and
$reserarch effort$



```{r}
a_file <- here::here("fits", "b11H06c.rds")
b11H06c <- readRDS(a_file)
# b11H06c <- brm(data = dataPrimates,
#              family = poisson,
#              formula = social_learning ~ 1 + brain_log + research_log,
#              prior = c(prior(normal(0, 0.5), class = Intercept),
#                        prior(normal(0, 0.5), class = b)),
#              cores = detectCores())
# b11H06c <- brms::add_criterion(b11H06c, criterion = c("waic", "loo"))
# saveRDS(b11H06c, file = a_file)
```


right away we can tell there is a counfounding effect as the WAIC varies greatly
with `b11H06b` (predictor is research effort) and `b11H06c` )with both research
effort and brain size) having the same quality of fit.

```{r}
loo_compare(b11H06a, b11H06b, b11H06c, criterion = "waic")
```

```{r}
round(model_weights(b11H06a, b11H06b, b11H06c), 3)
```



and if we look at the posterior distribution of the coefficients which are identical
for all 3 models. Therefore no model is really more informative.  All 3 variables
are correlated because of the collider bias.

```{r}
samples <- list(probs = c(0.055, 0.945))
samples$fits <- list(
    "brain_size" = "b11H06a",
    "research_effort" = "b11H06b",
    "all" = "b11H06c")

samples$data <- purrr::map_dfr(.x = samples$fits, .f = function(x) {
  as_draws(get(x)) %>%
    summarise_draws(mean, median, ~quantile2(., probs = samples$probs)) %>%
    filter(!(variable %in% c("lp__"))) %>%
    mutate(across(.cols = where(is.numeric), .fns = round, 2))
  },
  .id = "fit"
  )
# glimpse(samples$data)
```



```{r}
p <- list()
p$coefs <- ggplot(samples$data, aes(x = mean, xmin = q5.5, xmax = q94.5, 
                                    y = variable, color = fit)) +
  geom_pointinterval(fatten_point = 3, size = 2,
                     position = position_dodge(width = 0.5)) +
  geom_vline(xintercept = 0, color = "darkblue", linetype = "dashed") +
  scale_x_continuous(breaks = scales::breaks_extended(n = 7)) +
  scale_color_paletteer_d("ggthemes::Classic_10") +
  theme(legend.position = "bottom",
        legend.title = element_blank()) +
  labs(title = "Posterior distribution of coefficients with 89% interval",
       subtitle = "116H6 c)",
        x = NULL, y = NULL)
p$coefs
```


