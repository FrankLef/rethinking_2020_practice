```{r include=FALSE}
library(rethinking)
library(brms)
library(INLA)
library(eflStats)
library(eflINLA)
library(eflRethinking)
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(tidybayes.rethinking, quietly = TRUE)
library(posterior, quietly = TRUE)
library(modelr, quietly = TRUE)
library(simstudy, quietly = TRUE)
library(gt)
library(gtsummary)
library(scales)
library(dagitty, quietly = TRUE)
library(ggdag, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
```


# Multivariate Linear Models {#multivariate}

## Notes {-}

### Aknowledments {-}

Many thanks to [Jake Thompson](https://sr2-solutions.wjakethompson.com/more-linear-models.html)
who provided nice, clean solutions using `brms` which inspired to solve
these practices.

### Packages {-}

The `simstudy` is used to generate data under different scenarios to study DAG.


## 5E1 {-#prac5E1}

1. **Not** multiple linear, just univariate regression.
2. Multiple linear with 2 variables $x_i$ and $z_i$
3. **Not** multiple linear, just univariate with a centralized variable
4. Multiple linear with 2 variables $x_i$ and $z_i$

## 5E2 {-#prac5E2}

In section 5.1.2 the notion of control is related to conditional independencies
where the question becomes *is there any additional values in knowing a variable,
once I already know the other variables?* (p.131).

Also see Rethinking box in section 5.1.2 on the usage of the word *control*.

The control can be executed by using *predictor residual plots* or *counterfactual plots*
as described in section 5.1.5.

The model is

$$
\begin{align*}
diversity_i &\sim Normal(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 \cdot latitude_i + \beta_2 \cdot plantdiversity_i \\
\alpha &\sim Normal(0, 1) \\
\beta_1, \beta_2 &\sim Normal(0, 1) \\
\sigma &\sim Uniform(0, 1)
\end{align*}
$$

## 5E3 {-#prac5E3}

See section 5.2 on *Masked relationship*.

A masked relationship is when there are 2 predictors that are correlated with each other,
However one is positively correlated with the outcome, the other one is negatively 
correlated with the outcome. See beginning of section 5.2 for description.

Model is 

$$
\begin{align*}
phd_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 \cdot funding_i + \beta_2 \cdot size_i \\
\alpha &\sim \mathcal{N}(0, 1)\\
\beta_1, \beta_2 &\sim \mathcal{N}(0, 1)\\
\sigma &\sim \mathcal{Uniform}(0, 1)
\end{align*}
$$

The slopes should be positive.

## 5E4 {-#prac5E4}

Let $k$ be the number of categories, only need to have $k-1$ variables since
the intercept represents the $k^{th}$ category, hence model 1, 3, 4 are equivalent.
See explanation in first paragraph of section 5.3.2.

Model 5 is also inferentially equivalent as it uses
a re-parametrization which was explained in the first edition, edition of 2016. 
(overthinking box of section 5.4.1 of the 2016 edition)


## 5M1 {-#prac5M1}

A spurious correlation where an outcome is correlated with 2 predictors and
and when one predictor is manipulated the correlation between
the outcome and another predictor vanishes.

For example the performance of a manufacturing plant increases when you buy
equipment (i.e. invest in capital$C$) or invest in training (i.e. invest in
labor $L$).  But if the training $L$ is on how to use the equipment $C$
then the effect on productivity $P$ can be a spurious relationship.

Lets do a simulation to illustrate


```{r}
set.seed(as.integer(as.Date("2021-10-17")))
sim <- list()
sim <- within(sim, {
  defs <- defData(varname = "C", dist = "normal", formula = 0, variance = 1)
  defs <- defData(defs, varname = "L", dist = "normal", formula = "C", variance = 1)
  defs <- defData(defs, varname = "P", dist = "normal", formula = "C", variance = 1)
  data <- genData(n = 100L, dtDefs = defs)
})
# output must e standardized
sim$data <- sim$data %>%
  mutate(across(.fns = scale))
# str(sim$data)
```

we use `INLA` to fit the model to the data since it is faster.

```{r}
# univariate regression of P on C
a_file <- here::here("fits", "i05M01_capital.rds")
i05M01_capital <- readRDS(file = a_file)
# i05M01_capital <- inla(
#   data = sim$data,
#   formula = P ~ C,
#   )
# saveRDS(i05M01_capital, file = a_file)


# univariate regression of P on L
a_file <- here::here("fits", "i05M01_labor.rds")
i05M01_labor <- readRDS(file = a_file)
# i05M01_labor <- inla(
#   data = sim$data,
#   formula = P ~ L
#   )
# saveRDS(i05M01_labor, file = a_file)

# univariate regression of P on C and L
a_file <- here::here("fits", "i05M01_all.rds")
i05M01_all <- readRDS(file = a_file)
# i05M01_all <- inla(
#   data = sim$data,
#   formula = P ~ C + L
#   )
# saveRDS(i05M01_all, file = a_file)
```

and looking at the marginal distribution (very close to posterior distributions
in a simple case like this one).

We can observe that the labor $L$ influence is greatly reduced when using the
full model (i.e. with all variables). Its coefficient goes to 0.2 from 0.6 when
using all variables.

```{r}
p <- list()
p$capital <- eflINLA::tidy_marg_draws_inla(i05M01_capital) %>%
  as_draws_df() %>%
  eflStats::gather_intervals_rng()
p$labor <- eflINLA::tidy_marg_draws_inla(i05M01_labor) %>%
  as_draws_df() %>%
  eflStats::gather_intervals_rng()
p$all <- eflINLA::tidy_marg_draws_inla(i05M01_all) %>%
  as_draws_df() %>%
  eflStats::gather_intervals_rng()
p$df <- purrr::map_dfr(.x = list("capital" = p$capital, 
                                 "labor" = p$labor, 
                                 "all" = p$all),
                       .f = ~ data.frame(.x), .id = "model")

ggplot(p$df %>% filter(!(.variable %in% c("b_Intercept", "sigma"))), 
       aes(x = .value, xmin = .lower, xmax = .upper, y = model, color = model)) +
  geom_pointinterval(fatten_point = 3, size = 2) +
  geom_vline(aes(xintercept = .value, color = model), linetype = "dashed") +
  scale_x_continuous(labels = scales::label_number(accuracy = 0.1)) +
  scale_color_paletteer_d("fishualize::Scarus_quoyi") +
  ggdist::theme_ggdist() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = "bottom",
        strip.text = element_text(face = "bold", size = rel(1)),
        strip.background = element_rect(fill = "darkslategray1")) + 
  labs(title = "Mode and 95% interval of marginal distributions",
       subtitle = "5M1", x = NULL) +
  facet_wrap(. ~ .variable)
```


## 5M2 {-#prac5M2}

The productivity $P$ of an enterprise will increase depending on how much material
$M$ it can process but will be affected inversely depending on how much waste $W$ 
is  generated. 

There could be a scenario where the pressure to process more
material $M$ increases the waste $W$ which is counterproductive (literally).



```{r}
set.seed(as.integer(as.Date("2021-10-17")))
sim <- list()
sim <- within(sim, {
  def <- defData(varname = "M", dist = "normal", formula = 0, variance = 1)
  def <- defData(def, varname = "W", dist = "normal", formula = "M", variance = 1)
  def <- defData(def, varname = "P", dist = "normal", formula = "M - W", variance = 1)
  data <- genData(n = 500L, dtDefs = def)
})
# output must e standardized
sim$data <- sim$data %>%
  mutate(across(.fns = scale))
# str(sim$data)
```

we use `INLA` to fit the model to the data since it is faster.

```{r}
# univariate regression of P on M
a_file <- here::here("fits", "i05M02_material.rds")
i05M02_material <- readRDS(file = a_file)
# i05M02_material <- inla(
#   data = sim$data,
#   formula = P ~ M)
# saveRDS(i05M02_material, file = a_file)

# univariate regression of P on W
a_file <- here::here("fits", "i05M02_waste.rds")
i05M02_waste <- readRDS(file = a_file)
# i05M02_waste <- inla(
#   data = sim$data,
#   formula = P ~ W)
# saveRDS(i05M02_waste, file = a_file)

# univariate regression of P on M and W
a_file <- here::here("fits", "i05M02_all.rds")
i05M02_all <- readRDS(file = a_file)
# i05M02_all <- inla(
#   data = sim$data,
#   formula = P ~ M + W)
# saveRDS(i05M02_all, file = a_file)
```

and we plot the results which are similar to what we have in R code 5.40, p.150
of @elreath2020. That is, the effect of either $M$ or $W$ alone is much less than
when they considered together.  That is, for example, the coefficient of $M$ 
alone is 0 but, when considering $W$ also it's coefficient is above 0.5.


```{r}
p <- list()
p$material <- eflINLA::tidy_marg_draws_inla(i05M02_material) %>%
  as_draws_df() %>%
  eflStats::gather_intervals_rng()
p$waste <- eflINLA::tidy_marg_draws_inla(i05M02_waste) %>%
  as_draws_df() %>%
  eflStats::gather_intervals_rng()
p$all <- eflINLA::tidy_marg_draws_inla(i05M02_all) %>%
  as_draws_df() %>%
  eflStats::gather_intervals_rng()
p$df <- purrr::map_dfr(.x = list("material" = p$material, 
                                 "waste" = p$waste, 
                                 "all" = p$all),
                       .f = ~ data.frame(.x), .id = "model")

ggplot(p$df %>% filter(!(.variable %in% c("b_Intercept", "sigma"))), 
       aes(x = .value, xmin = .lower, xmax = .upper, y = model, color = model)) +
  geom_pointinterval(fatten_point = 3, size = 2) +
  geom_vline(aes(xintercept = .value, color = model), linetype = "dashed") +
  scale_x_continuous(labels = scales::label_number(accuracy = 0.1)) +
  scale_color_paletteer_d("fishualize::Scarus_quoyi") +
  ggdist::theme_ggdist() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = "bottom",
        strip.text = element_text(face = "bold", size = rel(1)),
        strip.background = element_rect(fill = "darkslategray1")) + 
  labs(title = "Mode and 95% interval of marginal distributions",
       subtitle = "5M2", x = NULL) +
  facet_wrap(. ~ .variable)
```


## 5M3 {-#prac5M3}

More divorces could lead to more remarriage and therefore contribute to an increase
in marriage rate.

This could be evaluated with multivariable regression if a flag is created to
identify which marriage is a remarriage.  We could then compare the models with
and without remarriage.

## 5M4 {-#prac5M4}

### Data {-}

The number of members of the LDS by state for 2021 was scrapped from 
[population](https://worldpopulationreview.com/state-rankings/mormon-population-by-state).
on Sep 17, 2021.

Note: The mormon file has data for Nevada but not District of Columbia. In
`WaffleDivorce` there is data for District of Columbia but not Nevada.

```{r}
lds <- read.csv(here::here("Mormon2021.csv"))
stopifnot(any(is.finite(lds$mormonPop2021)), any(is.finite(lds$Population2021)))
```

```{r}
data("WaffleDivorce")
d <- WaffleDivorce
d <- inner_join(x=d, y=lds, by = "Location") %>%
  mutate(L = mormonPop2021 / Population2021) %>%
  select(Location, L, A = MedianAgeMarriage, M = Marriage, D = Divorce) %>%
  mutate(across(.cols = where(is.numeric), 
                .fns = function(x) {as.vector(scale(x))}))
stopifnot(nrow(d) == 49)

# The mormon file has data for Nevada but not District of Columbia. In
# `WaffleDivorce` there is data for District of Columbia but not Nevada.
lds$Location[!(lds$Location %in% WaffleDivorce$Location)]
WaffleDivorce$Location[!(WaffleDivorce$Location %in% lds$Location)]
rm(WaffleDivorce)
```


### Model {-}


$$
\begin{align*}
D_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_A A_i +  \beta_L L_i + \beta_M M_i\\
\alpha &\sim \mathcal{N}(0, 0.2) \\
\beta_A, \beta_L, \beta_M &\sim \mathcal{N}(0, 0.5) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$

### `brm` {-}


```{r}
a_file <- here::here("fits", "b05M04.rds")
b05M04 <- readRDS(file = a_file)
# b05M04 <- brm(
#   data = d,
#   formula = D ~ 1 + A + L + M,
#   family = gaussian,
#   prior = c(
#     prior(normal(0, 0.02), class = Intercept),
#     prior(normal(0, 0.5), class = b),
#     prior(exponential(1), class = sigma)
#   ),
#   iter = 2000, warmup = 1000, chains = 4, core = detectCores(), seed = 5)
# saveRDS(b05M04, file = a_file)
summary(b05M04)
```

We observe that the addition of the mormon percentage $L$ add significant
information and does not affect age $A$ and marriage $M$ which are similar
to what we obtained without $L$.
increase significantly

```{r}
# brm posterior summary formatted with gt
brms::posterior_summary(b05M04) %>%
  as.data.frame() %>%
  filter(row.names(.) != "lp__") %>%
  eflRethinking::gt_posterior(
    labs = list(title = "Posterior Summary BRMS", subtitle = "Practice 5M4"),
    qtl = list(Q2.5 = "2.5%", Q97.5 = "97.5%"),
    .tab_options = list(heading.background.color = "lightgreen"))
```



### `inla` {-}

See section 2.3.1, p. 18 from @gomez2020 for multivariable regression
with `inla`.

```{r}
a_file <- here::here("fits", "i05M04.rds")
i05M04 <- readRDS(file = a_file)
# i05M04 <- inla(
#   data = d,
#   formula = D ~ A + L + M)
# saveRDS(i05M04, file = a_file)
```


and for the posterior summary we use `eflINLA::posterior_summary_inla()`. See
how this function works by pressing `F2`. Also see @ref(app-inla)

```{r}
eflINLA::posterior_summary_inla(i05M04)
```

### brm vs inla {-}

Using the results from the posterior summaries from brm and inla, we can observe
they give very similar.

```{r}
summ <- list()
summ$brm <- brms::posterior_summary(b05M04) %>%
  as.data.frame() %>%
  select(mean = Estimate, sd = Est.Error) %>%
  tibble::rownames_to_column(var = "var") %>%
  filter(var != "lp__")
summ$inla <- eflINLA::posterior_summary_inla(i05M04) %>%
  select(mean, sd) %>%
  tibble::rownames_to_column(var = "var")
eflRethinking::gt_posterior_compare(
  summ, var_df = "brm",
  labs = list(title = "Posterior Summary Comparison", subtitle = "Practice 5M4"))
```



### Conclusion {-}

For every increase of 1 sd of the mormon population, the divorce
rate is reduced by about 0.32 which is a significant impact.

The posterior plots are as follows

```{r}
gather_draws(model=b05M04, `b_.*`, regex = TRUE) %>%
  ggplot(aes(x = .value, y = .variable, color = .variable)) +
  stat_halfeye(.width = c(0.25, 0.5, 0.75), fill = "navajowhite") +
  geom_vline(xintercept=0, color = "brown", linetype = "dashed") +
  scale_color_paletteer_d("ggthemes::Classic_10") +
  ggdist::theme_tidybayes() +
  theme(legend.position="none") +
  labs(title = "Practice 5M4",
       subtitle = "Posterior distributions from brms fit", x=NULL, y=NULL)
```



## 5M5 {-#prac5M5}

The dag used to illustrate the model is as follows.  The outcome is $obesity$
and $price$ is a latent variable that impact all the elements since it not only
affect  the driving but also the restaurant which sees its cost increase because
of the more expensive fuel.


```{r}
p <- list()
p$dag <- ggdag::dagify(obesity ~ restaurant + driving + price,
                       restaurant ~ driving + price,
                       driving ~ price,
                       outcome = "obesity",
                       latent = "price") %>%
  ggdag::ggdag(seed=5, layout="circle", node=FALSE) +
  geom_dag_text(color = "springgreen4") +
  ggdag::theme_dag() +
  theme(panel.background = element_rect(fill = "mintcream", color = "mintcream"))
p$dag
```

One of the multivariable regressions can be formulated as follows

$$
\begin{align*}
obesity_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_D \cdot driving_i + \beta_P \cdot price_i + \beta_R \cdot restaurant_i \\
\alpha &\sim \mathcal{N}(0, 1)\\
\beta_D, \beta_P, \beta_R &\sim \mathcal{N}(0, 1)\\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$
The  data to use could be as follows

```{r}
data.frame(
  variable = c("obesity", "driving", "price", "restaurant"),
  measure = c("body mass index", "km driven", 
              "cpi for fuel and restaurant", "attendance")
)
```



## 5H1 {-#prac5H1}

The conditional independency is *M is independent of D conditional on A* as it 
is a *chain*.

The data does support it as there was no relation between  M and D once we 
removed the influence of A.  See Figure 5.7 in section 5.1.5.3.

### dagitty {-}

We can also use daggity to reaffirm our conclusion above

```{r out.width="50%", fig.cap="5H1 DAG"}
dag <- ggdag::dagify(D ~ A, A ~ M)
ggdag(dag, layout = "auto", seed = 5) +
  ggdag::theme_dag_blank(panel.background = 
                           element_rect(fill="aliceblue", color="aliceblue")) +
  geom_dag_point(color = "darksalmon") +
  geom_dag_text(color = "steelblue4")
```

and see what has conditional independence

```{r}
impliedConditionalIndependencies(dag)
```

and the number of equivalent dag are

```{r}
msg <- sprintf("there are %d equivalent DAG", length(equivalentDAGs(dag)))
message(msg)
```



## 5H2 {-#prac5H2}

See section 5.1.5.3 on . 140 of @elreath2020. Also see the same section in
@kurtz2020b.

### Data and model {-}

See practice 5H1 for the DAG.


```{r}
data("WaffleDivorce")
d <- WaffleDivorce %>% 
  select(Location, MedianAgeMarriage, Marriage, Divorce) %>%
  mutate(A=MedianAgeMarriage, M=Marriage, D=Divorce) %>%
  mutate(across(.cols = c(A, M, D), 
                .fns = function(x) {as.vector(scale(x))}))
rm(WaffleDivorce)
stopifnot(nrow(d) == 50)
skimr::skim(d)
```

The dag involves 2 models

The *D model* where $D$ is caused by $A$ as well as $M$


$$
\begin{align*}
D_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_A A_i + \beta_M M_i \\
\alpha &\sim \mathcal{N}(0, 0.2) \\
\beta_A, \beta_M &\sim \mathcal{N}(0, 0.5) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$
and the *A model* where $A$ is caused by $M$

$$
\begin{align*}
A_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_M M_i \\
\alpha &\sim \mathcal{N}(0, 0.2) \\
\beta_M &\sim \mathcal{N}(0, 0.5) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$


### `brm` {-}

We have 2 models defined as

```{r}
d_model <- bf(D ~ 1 + A + M)
a_model <- bf(A ~ 1 + M)
```

and make sure you set `set_rescor(FALSE)` to prevent `brms` from taking
a residual correlation between the 2 models.

```{r}
a_file <- here::here("fits", "b05H02.rds")
b05H02 <- readRDS(file = a_file)
# b05H02 <- brm(data = d,
#               family = gaussian,
#               formula = d_model + a_model + set_rescor(FALSE),
#               prior = c(prior(normal(0, 0.2), class = Intercept, resp = D),
#                         prior(normal(0, 0.5), class = b, resp = D),
#                         prior(exponential(1), class = sigma, resp = D),
# 
#                         prior(normal(0, 0.2), class = Intercept, resp = A),
#                         prior(normal(0, 0.5), class = b, resp = A),
#                         prior(exponential(1), class = sigma, resp = A)),
#               cores = detectCores(), seed = 5)
# saveRDS(b05H02, file = a_file)
```

```{r}
summary(b05H02)
```

and we will manipulate the state marriage rate by halving it so that now
we use them for $M$ once we set $A=0$ to remove the influence of $A$.
We then simulate to see what the effect this has on $D$.

```{r}
sim <- list()
sim$newdata <- data.frame(
  M = seq_range(x=d$M / 2, n=30),
  A = 0)
sim$D <- predict(b05H02, resp = "D", newdata = sim$newdata) %>%
  as.data.frame() %>%
  bind_cols(sim$newdata)
# str(sim$D)
sim$A <- predict(b05H02, resp = "A", newdata = sim$newdata) %>%
  as.data.frame() %>%
  bind_cols(sim$newdata)
# str(sim$A)
```

and plot the results

```{r}
p <- list()
p$D <- ggplot(sim$D, aes(x = M, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_smooth(stat = "identity", fill="palegreen", color="palegreen4") +
  ggdist::theme_ggdist() +
  scale_x_continuous(labels = function(x) {
    y <- x * sd(d$Marriage) + mean(d$Marriage)
    scales::label_number(accuracy = 1)(y)
  }) +
  scale_y_continuous(labels = function(x) {
    y <- x * sd(d$Divorce) + mean(d$Divorce)
    scales::label_number(accuracy = 1)(y)
  }) +
  labs(title ="Counterfactual effect of M / 2 on D",
       x="Manipulated Marriage rate", y="Predicted D with 95% CI")

p$A <- ggplot(sim$A, aes(x = M, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_smooth(stat = "identity", fill="steelblue1", color="steelblue4") +
  ggdist::theme_ggdist() +
  scale_x_continuous(labels = function(x) {
    y <- x * sd(d$Marriage) + mean(d$Marriage)
    scales::label_number(accuracy = 1)(y)
  }) +
  scale_y_continuous(labels = function(x) {
    y <- x * sd(d$MedianAgeMarriage) + mean(d$MedianAgeMarriage)
    scales::label_number(accuracy = 1)(y)
  }) +
  labs(title ="Counterfactual effect of M / 2 on A",
       x="Manipulated Marriage rate", y="Predicted A with 95% CI")
wrap_plots(p)
```

which seems to indicate that $M$ has very little influence on $D$ once we control
$A$. That is $D$ is independent of $M$ conditional on $A$.


### `inla` {-}

We don't seem to have the ability with `inla` to run many models at once as
done just above with `brm`.  Inla  is however so fast we can run 2 models
separately and still be faster than brm.

For the *D model* we have

```{r}
i05H02d_args <- list(
  data = d,
  formula = D ~ A + M,
  family = "gaussian",
  control.fixed = list(mean.intercept = 0, prec.intercept = 1 / (0.2^2),
                       mean = 0, prec = 1 / (0.5^2)),
  control.family = list(
    hyper = list(prec = list(prior = "loggamma", param = c(1, 0.00005)))),
  control.compute = list(config = TRUE, dic = TRUE, waic = TRUE),
  quantiles = c(0.025, 0.975)
)

a_file <- here::here("fits", "i05H02d.rds")
i05H02d <- readRDS(file = a_file)
# i05H02d <- do.call(inla, i05H02d_args)
# saveRDS(i05H02d, file = a_file)
```


and for the *A model* we have

```{r}
# model A has a modified version of D model's arguments
i05H02a_args <- i05H02d_args
i05H02a_args$formula <- A ~ M
a_file <- here::here("fits", "i05H02a.rds")
i05H02a <- readRDS(file = a_file)
# i05H02a <- do.call(inla, i05H02a_args)
# saveRDS(i05H02a, file = a_file)
```


### compare brm vs inla {-}

For the model $D$

```{r}
brms::posterior_summary(b05H02) %>%
  as.data.frame() %>%
  filter(grepl(pattern = "^b_D|^sigma_D",x = row.names(.))) %>%
  bind_cols(eflINLA::posterior_summary_inla(i05H02d)) %>%
  select(Estimate, mean, Est.Error, sd) %>%
  round(digits = 4)
```

and for model $A$

```{r}
brms::posterior_summary(b05H02) %>%
  as.data.frame() %>%
  filter(grepl(pattern = "^b_A|^sigma_A",x = row.names(.))) %>%
  bind_cols(eflINLA::posterior_summary_inla(i05H02a)) %>%
  select(Estimate, mean, Est.Error, sd) %>%
  round(digits = 4)
```


Conclusion: brm and inla give very similar result. inla is certainly faster.

## 5H3 {-#prac5H3}

See section 5.1.5.3 on . 140 of @elreath2020. Also see the same section in
@kurtz2020b.


### Data, model, DAG {-}


```{r fig.cap="5H3 Data"}
data(milk)
d <- milk %>%
  as.data.frame() %>%
  tidyr::drop_na() %>%
  mutate(K = as.vector(scale(kcal.per.g)),
         N = as.vector(scale(neocortex.perc)),
         M = as.vector(scale(log(mass))))
rm(milk)
# skimr::skim(d, kcal.per.g, neocortex.perc, mass, K, N, M)
# it should give us a dataframe with 17 rows
stopifnot(nrow(d) == 17)
d %>%
  gtsummary::tbl_summary(
    by = clade,
    include = c(kcal.per.g, neocortex.perc, mass, K, N, M)) %>%
  # add_n() %>%
  add_overall() %>%
  bold_labels()
```

The model is the same as model `m5.7` in the textbook.



$$
\begin{align*}
K_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_M log(M_i) + \beta_N  N_i \\
\alpha &\sim \mathcal{N}(0, 0.2) \\
\beta_M, \beta_N &\sim \mathcal{N}(0, 0.5) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$


The DAG is

```{r echo=TRUE, out.width="50%", fig.cap="5H3 DAG"}
dag <- ggdag::dagify(N ~ M, K ~ M + N)
ggdag(dag, layout = "tree", seed = 5) +
  ggdag::theme_dag_blank(panel.background = 
                           element_rect(fill="oldlace", color="oldlace")) +
  geom_dag_point(color = "darkseagreen1") +
  geom_dag_text(color = "brown1")
```


```{r}
x <- impliedConditionalIndependencies(dag)
if(length(x)) x else message("no conditional independence")
```

```{r}
msg <- sprintf("there are %d equivalent DAG", length(equivalentDAGs(dag)))
message(msg)
```

See practice 5H2 just above for similar method with `brms`. We have 2 models

```{r}
k_model <- bf(K ~ 1 + M + N)
n_model <- bf(N ~ 1 + M)
```

which gives the following fit

```{r}
a_file <- here::here("fits", "b05H03.rds")
b05H03 <- readRDS(file = a_file)
# b05H03 <- brm(data = d,
#               family = gaussian,
#               formula = k_model + n_model + set_rescor(FALSE),
#               prior = c(prior(normal(0, 0.2), class = Intercept, resp = K),
#                         prior(normal(0, 0.5), class = b, resp = K),
#                         prior(exponential(1), class = sigma, resp = K),
# 
#                         prior(normal(0, 0.2), class = Intercept, resp = N),
#                         prior(normal(0, 0.5), class = b, resp = N),
#                         prior(exponential(1), class = sigma, resp = N)),
#               cores = detectCores(), seed = 5)
# saveRDS(b05H03, file = a_file)
```


```{r}
sim <- list()
sim$newdata <- data.frame(
  M = seq_range(x=2 * d$M, n=30),
  N = 0)
sim$K <- predict(b05H03, resp = "K", newdata = sim$newdata) %>%
  as.data.frame() %>%
  bind_cols(sim$newdata)
# str(sim$M)
sim$N <- predict(b05H03, resp = "N", newdata = sim$newdata) %>%
  as.data.frame() %>%
  bind_cols(sim$newdata)
# str(sim$N)
```

and plot the results

```{r}
p <- list()
p$K <- ggplot(sim$K, aes(x = M, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_smooth(stat = "identity", fill="tan", color="tan4") +
  ggdist::theme_ggdist() +
  scale_x_continuous(labels = function(x) {
    y <- x * sd(d$M) + mean(d$M)
    scales::label_number(accuracy = 0.1)(y)
  }) +
  scale_y_continuous(labels = function(x) {
    y <- x * sd(d$K) + mean(d$K)
    scales::label_number(accuracy = 0.1)(y)
  }) +
  labs(title ="Counterfactual effect of 2 x M on K",
       x="Manipulated M", y="Predicted K with 95% CI")

p$N <- ggplot(sim$N, aes(x = M, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_smooth(stat = "identity", fill="darkolivegreen1", color="darkolivegreen4") +
  ggdist::theme_ggdist() +
  scale_x_continuous(labels = function(x) {
    y <- x * sd(d$M) + mean(d$M)
    scales::label_number(accuracy = 0.1)(y)
  }) +
  scale_y_continuous(labels = function(x) {
    y <- x * sd(d$N) + mean(d$N)
    scales::label_number(accuracy = 0.1)(y)
  }) +
  labs(title ="Counterfactual effect of 2 x M on N",
       x="Manipulated M", y="Predicted N with 95% CI")
wrap_plots(p)
```

## 5H4 {-#prac5H4}

### Data {-}


```{r}
data("WaffleDivorce")
d <- WaffleDivorce %>%
  select(Location, MedianAgeMarriage, Marriage, Divorce, South) %>%
  mutate(A=MedianAgeMarriage, M=Marriage, D=Divorce) %>%
  mutate(across(.cols = c(A, M, D),
                .fns = function(x) {as.vector(scale(x))}))
rm(WaffleDivorce)
stopifnot(nrow(d) == 50)
d %>%
  select(-Location) %>%
  mutate(South = if_else(South==0, "Not South", "South")) %>%
  gtsummary::tbl_summary(by=South) %>%
  add_overall() %>%
  bold_labels()
```
### Correlations

We can look at the pairs of variables to see which ones are related

```{r}
p <- list()
p$All <- GGally::ggcorr(d[, c("A", "M", "D")], 
                        label = TRUE) +
  theme() +
  labs(title = "All data")
p$South <- GGally::ggcorr(d[d$South!=0, c("A", "M", "D")], 
                        label = TRUE) +
  theme() +
  labs(title = "SOUTH")
p$NotSouth <- GGally::ggcorr(d[d$South==0, c("A", "M", "D")], 
                        label = TRUE) +
  theme() +
  labs(title = "NOT SOUTH")

wrap_plots(p) +
  plot_annotation(title = "Correlations with and without the South effect") &
  theme(title = element_text(color = "midnightblue"),
        legend.position = "bottom")
```




### DAG {-}

Based on the correlation plots just above.  When $South = 1$, there seems to 
be influence on the correlation between $M$ and $D$ and, to a lesser extent,
between $A$ and $D$ (but still noticeable). No significant change is noted
between $A$ and $M$.

Otherwise, when $South = 0$, there is no effect on the correlations which are 
the same as for the data in general.

Therefore we suggest the following DAG's as possible scenarios

```{r out.width="100%", fig.cap="5H4 DAG"}
dag <- list()
p <- list()
dag$A <- ggdag::dagify(D ~ A + M, A ~ S, M ~ A, M ~ S)
p$A <- ggdag(dag$A, layout = "tree", seed = 5) +
  ggdag::theme_dag_blank(panel.background =
                           element_rect(fill="snow", color="snow")) +
  geom_dag_point(color = "honeydew2") +
  geom_dag_text(color = "darkgreen") +
  labs(title = "DAG #1 for Practice 5H4") +
  theme(title = element_text(color = "midnightblue"))

dag$B <- ggdag::dagify(D ~ A + M, M ~ A, M ~ S)
p$B <- ggdag(dag$B, layout = "nicely", seed = 5) +
  ggdag::theme_dag_blank(panel.background =
                           element_rect(fill="snow", color="snow")) +
  geom_dag_point(color = "honeydew2") +
  geom_dag_text(color = "darkgreen") +
  labs(title = "DAG #2 for Practice 5H4") +
  theme(title = element_text(color = "midnightblue"))
# p$B
wrap_plots(p)
```

### Model {-}

To evaluate the influence ot $South$ we will add it to the model as well
as 2 more variables called $AS$ and $MS$. $AS = A$ when $S=1$ and $A = 0$ 
otherwise. The same will be done for $MS$.

The interpretations of these 3 new variables, $S$, $A$ and $M$ will be that
the 

* coefficient of $S$ shows the influence of $South$ on the intercept
* coefficient of $AS$ shows the influence of $South$ on $A$
* coefficient of $MS$ shows the influence of $South$ on $M$


$$
\begin{align*}
D_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_A A_i + \beta_M M_i +  
  \beta_{AS} AS_i + \beta_{MS} MS_i + \beta_S S_i\\
\alpha &\sim \mathcal{N}(0, 0.2) \\
\beta_A, \beta_M, \beta_{AS}, \beta_{MS}, \beta_S &\sim \mathcal{N}(0, 0.5) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$
so we add the new variables to the data

```{r}
d <- d %>%
  mutate(AS = South * A, MS = South * M)
# d
```



and make the fit with `brm`


```{r}
a_file <- here::here("fits", "b05H04.rds")
b05H04 <- readRDS(file = a_file)
# b05H04 <- brm(data = d,
#               family = gaussian,
#               formula = D ~ 1 + A + M + AS + MS + South,
#               prior = c(prior(normal(0, 0.2), class = Intercept),
#                         prior(normal(0, 0.5), class = b),
#                         prior(exponential(1), class = sigma)),
#               cores = detectCores(), seed = 5)
# saveRDS(b05H04, file = a_file)
```

We could  use `summary()` to see the result.  We will create a simple table

```{r}
# brm posterior summary formatted with gt
brms::posterior_summary(b05H04) %>%
  as.data.frame() %>%
  filter(row.names(.) != "lp__") %>%
  eflRethinking::gt_posterior(labs = list(title = "Posterior Summary BRMS",
                           subtitle = "Practice 5H4"),
               qtl = list(Q2.5 = "2.5%", Q97.5 = "97.5%"),
               .tab_options = list(heading.background.color = "lightgreen"))
```

### Conclusion {-}

* The south increases the intercept by 0.23, that is, the rate of divorce
become increase by a factor of 0.23 for every standard deviation of $Divorce$.
This is significant considering that for the population in general the
rate is negative -0.12.
* The effect of $MedianAgeMarriage$ goes from -0.53 in general to 
-0.92 = -0.53 + -0.39 when taking into account the $South$ variable.
* The effect of $Marriage$ goes from -0.08 in general to 
0.31 = -0.08 + 0.39 when taking into account the $South$ variable.

In conclusion, the $South$ variable seems to have significant effect on the 
model so that the following DAG is our suggestion

```{r}
p$A
```

and the $South$ effect is obvious we we plot the results

```{r}
predict <- list(n = 10, a_rng = c(-2, 2), m_rng = c(-2, 2))
predict$newdata <- data.frame(
  A = c(seq_range(x=predict$a_rng, n=predict$n), 
        seq_range(x=predict$a_rng, n=predict$n)),
  M = c(seq_range(x=predict$m_rng, n=predict$n), 
        seq_range(x=predict$m_rng, n=predict$n)),
  South = rep(0:1, each=predict$n)) %>%
  mutate(
    AS = South * A,
    MS = South * M)
predict$data <- predict(b05H04, newdata=predict$newdata) %>%
  as.data.frame() %>%
  bind_cols(predict$newdata) %>%
  mutate(South = if_else(South != 0, "Not South", "South"),
         South = factor(South))
# str(predict$data)
```

```{r}
p <- list()
p$M <- ggplot(predict$data, aes(x = M, y = Estimate, color = South)) +
  # geom_smooth(stat = "identity", fill="palegreen", color="palegreen4") +
  geom_line() +
  scale_x_continuous(
    breaks = scales::breaks_extended(n = 7),
    labels = function(x) {
      y <- x * sd(d$Marriage) + mean(d$Marriage)
      scales::label_number(accuracy = 1)(y)
      },
    limits = predict$m_rng) +
  scale_y_continuous(
    breaks = scales::breaks_extended(n = 5),
    labels = function(x) {
      y <- x * sd(d$Divorce) + mean(d$Divorce)
      scales::label_number(accuracy = 1)(y)
      }) +
  ggdist::theme_ggdist() +
  labs(title ="Divorce vs Marriage Rate",
       x="Marriage rate", y="Divorce rate",
       color = NULL)
# p$M
p$A <- ggplot(predict$data, aes(x = A, y = Estimate, color = South)) +
  # geom_smooth(stat = "identity", fill="plum", color="plum4") +
  geom_line() +
  scale_x_continuous(
    breaks = scales::breaks_extended(n = 7),
    labels = function(x) {
      y <- x * sd(d$MedianAgeMarriage) + mean(d$MedianAgeMarriage)
      scales::label_number(accuracy = 1)(y)
      },
    limits = predict$m_rng) +
  scale_y_continuous(
    breaks = scales::breaks_extended(n = 5),
    labels = function(x) {
      y <- x * sd(d$Divorce) + mean(d$Divorce)
      scales::label_number(accuracy = 1)(y)
      }) +
  ggdist::theme_ggdist() +
  labs(title ="Divorce vs MedianAgeMarriage",
       x="MedianAgeMarriage", y="Divorce rate",
       color = NULL)
# p$M / p$A
wrap_plots(p) +
  plot_annotation(title = "Practice 5H4") &
  theme(legend.position = "bottom")
```

