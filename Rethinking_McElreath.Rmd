--- 
title: "Statistical Rethinking - Solutions"
author: "frank"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [books.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Packages and setup to use this book."
---

# Introduction {-}

Placeholder


## Session info {-}
## Prerequisites {-}
### Install `Rtools` {-}
### Install `rstan` {-}
### Install `rethinking` {-}
## References {-}

<!--chapter:end:index.Rmd-->

# The Golem of Prague {#golem}

```{r}
message("There is no practice for this chapter.")
```

<!--chapter:end:01_Golem.Rmd-->


# Small Worlds and Large Worlds {#worlds}

Placeholder


## 2E1 {-#prac2E1}
## 2E2 {-#prac2E2}
## 2E3 {-#prac2E3}
## 2E4 {-#prac2E4}
## 2M1 {-#prac2M1}
### 2M1 (1) {-}
### 2M1 (2) {-}
### 2M1 (3) {-}
### plot {-}
## 2M2 {-#prac2M2}
### 2M2 (1) {-}
### 2M2 (2) {-}
### 2M2 (3) {-}
### 2M2 plot {-}
## 2M3 {-#prac2M3}
## 2M4 {-#prac2M4}
## 2M5 {-#prac2M5}
## 2M6 {-#prac2M6}
## 2M7 {-#prac2M7}
## 2H1 {-#prac2H1}
## 2H2 {-#prac2H2}
## 2H3 {-#prac2H3}
## 2H4 {-#prac2H4}

<!--chapter:end:02_Worlds.Rmd-->


# Sampling the Imaginary {#sampling}

Placeholder


## 3E1 {-#prac3E1}
## 3E2 {-#prac3E2}
## 3E3 {-#prac3E3}
## 3E4 {-#prac3E4}
## 3E5 {-#prac3E5}
## 3E6 {-#prac3E6}
## 3E7 {-#prac3E7}
## 3M1 {-#prac3M1}
## 3M2 {-#prac3M2}
## 3M3 {-#prac3M3}
## 3M4 {-#prac3M4}
## 3M5 {-#prac3M5}
## 3M6 {-#prac3M6}
## 3H1 {-#prac3H1}
## 3H2 {-#prac3H2}
## 3H3 {-#prac3H3}
## 3H4 {-#prac3H4}
## 3H5 {-#prac3H5}

<!--chapter:end:03_Sampling.Rmd-->


# Linear Models {#linear}

Placeholder


## Notes {-}
### `eflINLA` {-}
## 4E1 {-#prac4E1}
## 4E2 {-#prac4E2}
## 4E3 {-#prac4E3}
## 4E4 {-#prac4E4}
## 4E5 {-#prac4E5}
## 4M1 {-#prac4M1}
## 4M2 {-#prac4M2}
## 4M3 {-#prac4M3}
## 4M4 {-#prac4M4}
## 4M5 {-#prac4M5}
## 4M6 {-#prac4M6}
## 4M7 {-#prac4M7}
### Model {-#prac4M7-Model}
### Fitting strategies {-}
### Priors {-#prac4M7-Priors}
### Using `quap` {-}
#### Fits {-}
#### Covariances {-}
#### Posteriors {-}
#### Predictions {-}
### Using `brm` {-}
#### Fits {-}
#### Posteriors {-}
#### Predictions {-}
### Using `inla` {-}
#### Fits {-}
#### Marginals {-}
#### Fitted {-}
#### Predictions {-}
### Compare `quap`, `brm`, `inla` {-}
## 4M8 {-#prac4M8}
### Model {-}
### Functions {-}
### a) Knots = 15, $w_j \sim \mathcal{N}(0, 10)$
### b) Knots = 25, $w_j \sim \mathcal{N}(0, 10)$ {-}
### c) Knots = 25, $w_j \sim \mathcal{N}(0, 20)$ {-}
### Conclusion {-}
#### Plots {-}
#### Summaries {-}
### Using `inla`
## 4H1 {-#prac4H1}
### Data and model {-}
### Using `rethinking::link()` {-}
### Using `tidyverse` {-}
## 4H2 {-#prac4H2}
### Model {-}
### 4H2 a) with `quap` {-}
### 4H2 b) with `quap` {-}
#### Get the fitted values with `quap` {-}
### 4H2 a) with `brm` {-}
### 4H2 b) with `brm` {-}
### 4H2 c) {-}
## 4H3 {-#prac4H3}
### 4H3 a) using `quap` {-}
### 4H3 b) using `quap` {-}
### 4H3 a) using `brm` {-}
### 4H3 b) using `brm` {-}
## 4H4 {-#prac4H4}
### 4H4 using `quap` {-}
### 4H4 using `brm` {-}
## 4H5 {-#prac4H5}
### The data
### The model
### Functions used {-}
### The splines {-}
### With `quap` {-}
### With `brm` {-}
#### Fitted {-}
## 4H6 {-#prac4H6}
### Data {-}
### Model {-}
### Functions {-}
### Prior distribution simulation {-}
## 4H7 {-#prac4H7}
## 4H8 {-#prac4H8}

<!--chapter:end:04-Linear.Rmd-->


# Multivariate Linear Models {#multivariate}

Placeholder


## Notes {-}
### Aknowledments {-}
### Packages {-}
## 5E1 {-}
## 5E2 {-}
## 5E3 {-}
## 5E4 {-}
## 5M1 {-}
## 5M2 {-}
## 5M3 {-}
## 5M4 {-}
### Data {-}
### Model {-}
### `brm` {-}
### `inla` {-}
### brm vs inla {-}
### Conclusion {-}
## 5M5 {-}
## 5H1 {-}
### dagitty {-}
## 5H2 {-}
### Data and model {-}
### `brm` {-}
### `inla` {-}
### compare brm vs inla {-}
## 5H3 {-}
### Data, model, DAG {-}
## 5H4 {-}
### Data {-}
### Correlations
### DAG {-}
### Model {-}
### Conclusion {-}

<!--chapter:end:05_Multivariate.Rmd-->


# Structural Causal Models {#SCM}

Placeholder


## 6E1 {-}
### Multicollinearity {-}
### Post-treatment bias {-}
### Collider bias {-}
## 6E2 {-}
## 6E3 {-}
## 6E4 {-}
## 6M1 {-}
## 6M2 {-}
## 6H1 {-}
## 6H2 {-}
### 6H2 a) By territory area size {-}
### 6H2 b) By group size {-}

<!--chapter:end:06_SCM.Rmd-->


# Ulysses' Compass {#information}

Placeholder


## 7E1 {-}
## 7E2 {-}
## 7E3 {-}
## 7E4 {-}
## 7M1 {-}
### AIC {-}
### WAIC {-}
## 7M2 {-}
### Model selection {-}
### Model comparison {-}
## 7M3 {-}
## 7M4 {-}
## 7M5 {-}
## 7M6 {-}
## 7H1 {-}
## 7H2 {-}
### Use WAIC and PSIS to evaluate the outliers {-}
### Robust regression with t-student distribution {-}
## 7H3 {-}
### islands' entropies {-}
### Kullback-Leiber divergence {-}
## 7H4 {-}
## 7H5 {-}
### 7H5 1) {-}
### 7H5 2) {-}
### 7H5 3) {-}
### 7H5 4) {-}
### 7H5 5) {-}
### 7H5 Model comparison {-}

<!--chapter:end:07_Information.Rmd-->


# Conditional Manatees {#interactions}

Placeholder


## 8E1 {-}
### (1) {-}
### (2) {-}
### (3) {-}
## 8E2 {-}
### (1) {-}
### (2) {-}
### (3) {-}
### (4) {-}
## 8E3 {-}
### (1) {-}
### (2) {-}
### (3) {-}
### (4) {-}
## 8M1 {-}
## 8M2 {-}
## 8M3 {-}
## 8M4 {-}
## 8H1 {-}
### Model and fit {-}
## 8H2 {-}
## 8H3 {-}
### (a) {-}
### (b) {-}
## 8H4 {-}
### (a) {-}
#### The priors {-}
#### The models {-}
#### The fit {-}
#### Validate the priors {-}
#### Compare the models {-}
#### Plotting the model
### (b) {-}
#### The fit {-}
#### Compare the models {-}
#### Plotting the model
### (c) {-}
#### Compare the models {-}
#### Plotting
## 8H5 {-}
## 8H6 {-}
## 8H7 {-}

<!--chapter:end:08_Interactions.Rmd-->


# Markov Chain Monte Carlo {#MCMC}

Placeholder


## 9E1 {-}
## 9E2 {-}
## 9E3 {-}
## 9E4 {-}
## 9E5 {-}
## 9E6 {-}
### Model and fit {-}
### Trace plot {-}
### Running means {-}
### Autocorrelations {-}
## 9E7 {-}
## 9M1 {-}
### Running means {-}
### Autocorrelations {-}
## 9M2 {-}
### Autocorrelations {-}
## 9M3 {-}
## 9H1 {-}
### Trace plot {-}
### Trank plot {-}
### Running means {-}
### Autocorrelations {-}
## 9H2 {-}
## 9H3 {-}
## 9H4 {-}
## 9H5 {-}
## 9H6 {-}
## 9H7 {-}

<!--chapter:end:09_MCMC.Rmd-->

```{r include=FALSE}
library(rethinking)
```



# Big Entropy and the Generalized Linear Model {#GLM}

```{r echo=FALSE}
message("No practice in this chapter")
```


<!--chapter:end:10_GLM.Rmd-->


# Counting and Classification {#Counting}

Placeholder


## 11E1 {-#prac11E1}
## 11E2 {-#prac11E2}
## 11E3 {-#prac11E3}
## 11E4 {-#prac11E4}
## 11M1 {-#prac11M1}
## 11M2 {-#prac11M2}
## 11M3 {-#prac11M3}
## 11M4 {-#prac11M4}
## 11M5 {-#prac11M5}
## 11M6 {-#prac11M6}
## 11M7 {-#prac11M7}
### Model with $\mathcal{N}(0, 1.5)$ {-}
### Model with $\mathcal{N}(0, 10)$ {-}
## 11M8 {-#prac11M8}
## 11H1 {-#prac11H1}
### Null model, general intercept {-}
### Model with treatment intercepts {-}
### Model with treatment and actor intercepts {-}
### Model comparison {-}
## 11H2 {-#prac11H2}
### (a) {-}
#### `quap` fit
#### `brm` fit
#### `inla` fit

<!--chapter:end:11_Counting.Rmd-->

```{r include=FALSE}
library(rethinking)
library(brms)
library(tidyr)
library(dplyr)
library(ggplot2)
library(paletteer)
library(bayesplot)
```


# Monsters and Mixtures {#Mixed}


## 12E1 {-}

```{r echo=FALSE}
message("TODO")
```

<!--chapter:end:12_Mixed.Rmd-->

```{r include=FALSE}
library(rethinking)
library(brms)
library(tidyr)
library(dplyr)
library(ggplot2)
library(paletteer)
library(bayesplot)
```


# Multilevel Models {#MLM}


## 13E1 {-#prac13E1}

```{r echo=FALSE}
message("TODO")
```

<!--chapter:end:13_Multilevel.Rmd-->

```{r include=FALSE}
library(rethinking)
library(brms)
library(tidyr)
library(dplyr)
library(tidybayes)
```


# Adventures in Covariance {#Covariance}


## 14E1 {-#prac14E1}

```{r echo=FALSE}
message("TODO")
```


<!--chapter:end:14_Covariances.Rmd-->

# References {-}

<div id="refs"></div>


# (APPENDIX) Appendix {-}


<!--chapter:end:50_appendix.Rmd-->


# `logit()` distribution {-#app-logit}

Placeholder


## `logistic()` / `inv_logit()` function
## `logit()` function
## `dordlogit()` function
## `pordlogit()` function
## `rordlogit()` function

<!--chapter:end:51_logit.Rmd-->


# Math notes {-#app-maths}

Placeholder


## Mode {-#app-maths-mode}
## $\Gamma$ distribution {-#app-gammaexp}

<!--chapter:end:52_maths.Rmd-->

```{r include=FALSE}
library(rethinking)
library(INLA)
library(brms)
library(eflINLA)
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(modelr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(paletteer, quietly = TRUE)
library(patchwork, quietly = TRUE)
```



# `INLA` {-#app-inla}

This appendix gives example on how to use `inla` objects. The data is simulated
then the results are verified using the parameters of the siumlation.


## Note on priors {-}

One different aspect of `INLA::inla` from `rethinking::quap` and `brms::brm` is
its treatment of **priors**.  Contrary to `quap` and `brm` the priors in `inla`
are scattered among 3 different parameters which can be confusing at first.

The priors for `inla` are organized as follows

* **Likelihood**: The priors are defined using the parameter `hyper` inside 
`control.family`. See section 5.2 of @gomez2020.
  - The default prior on precision is the Gamma(shape = 1, rate = 0.00005). 
  Internally, it is defined on the log(precision) and is therefore 
  LogGamma(shape = 1, rate = 0.00005) See p. 19 and p. 23 of @gomez2020 and section 3.2, p.44, o
  wang2018.
* **Fixed effect**: The prior are defined in `control.fixed`. See section 2.3.1, 
in particular table 2.1, of @gomez2020.
* **Latent effect**: The difference between latent and hyperparameters in INLA is that
latent Gaussain variables describe linear predictor.  Hyperparameters are all
parameters (very few or none) not arising linearly in the predictors.
The priors are in defined using the parameter `hyper` inside the
`f()` function. See section 5.2 of @gomez2020.

The generalized linear models are handled using parameters values that can be
listed with `?inla.models`

```{r}
names(inla.models())
names(inla.models())["prior"]
?inla.models
```

and the information on priors can be found in `inla.models()$prior`

```{r}
names(inla.models()$prior)
```

The prior parametrization will therefore be detailed in details with examples 
below.

## Linear regression

### Data

The simulation uses the following

* x1, x2, x3 Are correlated
* x3 is lognormal distributed with its coefficient force to be positive
* cat1 is a category with binomial distribution

```{r}
sim <- list(n = 500L,
            intercept = c("mu" = 5, "sigma" = 1),
            sigma_rate = 1,
            cat1 = c("size" = 1, "p" = 0.5),
            mus = c("x1" = 1, "x2" = 2, "x3" = 0),
            sigmas = c("x1" = 0.5, "x2" = 1, "x3" = 1),
            rhos = c("x1-x2" = 0.25, "x1-x3" = 0.5, "x2-x3" = 0.75),
            beta = c("x4" = 10))
# the dtaa is correlated but in the current model this information
# is not used.
sim <- within(sim, {
  # Rho is the correlation matrix
  Rho <- diag(nrow = length(rhos))
  Rho[lower.tri(Rho)] <- rhos
  Rho[upper.tri(Rho)] <- t(Rho[lower.tri(Rho)])
  # Sigma is the covariance matrix
  Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
  # create the matrix of correlated data
  mat <- MASS::mvrnorm(n = n, mu = mus, Sigma = Sigma)
})
# create the simulated data
sim <- within(sim, {
  data <- as.data.frame(mat)
  data <- data %>%
    mutate(x0 = rnorm(n = n, mean = intercept["mu"], sd = intercept["sigma"]),
           x3 = exp(x3),
           x4 = beta["x4"] * rbinom(n = n, size = cat1["size"], p = cat1["p"]),
           mu = x0 + x1 + x2 + x3 + x4,
           sigma = rexp(n, rate = sigma_rate),
           y = mu + rnorm(n = n, mean = 0L, sd = sigma)
           )
  })
```

### Model

The model is defined as follows

$$
\begin{align*}
y &\sim \mathcal{N}(\mu, \sigma) \\
\mu &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 exp(x_3) + \beta_4 x_4 \\
\beta_0 &\sim \mathcal{N}(mean = 5,sd = 1.5) \\
\beta_1 &\sim \mathcal{N}(mean = 1,sd = 0.5) \\
\beta_2 &\sim \mathcal{N}(mean = 2,sd = 1) \\
\beta_3 &\sim \mathcal{LogNormal}(meanlog = 0, sdlog = 1) \\
\sigma &\sim \mathcal{Exponential}(rate = 1)
\end{align*}
$$
### Formula

The formula is expressed as usual, as per the model above

```{r}
args <- list()  # create the list of parameters
args$formula <- y ~ x1 + x2 + x3
```


### Priors

#### Family (likelihood)

The default prior on the variance is defined internally in terms of logged
precision $log(\tau$. It follows the log gamma distribution 
$log(\tau) \sim LogGamma(1, 10^-5)$ (@wang2018, p.44).

This corresponds to our model so we keep it as is.

```{r}
args$family = "gaussian"
stopifnot(args$family %in% names(inla.models()$likelihood))
```



#### Fixed effects

See section 2.3.1 of @gomez2020 for details and a nice example at the end of
section 3.2, p.47, of @wang2018.

The model has 2 fixed effects in $x_1$ and $x_2$ which can be assigned
the **fixed-effect priors** from inla whose defaults are as follows


```{r}
inla.set.control.fixed.default()[c("mean", "mean.intercept", "prec", "prec.intercept")]
```

and assigning the fixed effect priors

```{r}
args$control.fixed = list(mean.intercept = 5, 
                          prec.intercept = 1 / (1.5^2),
                          mean = list(x1 = 1, x2 = 2, 
                                      default = 0),
                          prec = list(x1 = 1 / (0.5^2), x2 = 1 / (1^2), 
                                      default = 0.001))
```



#### Random effects


* the model is fixed (no random effect) and
* $x_3$ is a **constrained fixed effect** limited to the positive range.
See section 3.2.2 of @gomez2020 for details.

Therefore the model has a **constrained fixed effect** *weight_c* which is 
indicated by `clinear` in the `f()` function of the formula. This makes
*weight_c* a *latent effect* as opposed to a *likelihood effect* such
as $\sigma$ in the current model.

The available priors, defined in `f()` of *latent effect* are as follows

```{r}
names(inla.models()$prior)
```

and we will use `logtnormal` which is equivalent to our prior 
$\beta &\sim \mathcal{LogNormal}(0, 1)$

```{r}
args$control.family = list(
    hyper = list(prec = list(prior = "loggamma", param = c(1, 10e-5))))
```


### Other parameters

Finally we specify































## 4M7 {-#app-inla-04M07}

We use practice 4M7 from chapter 4 to illustrate `inla` in more details.


### Data and model {-}


```{r}
data(Howell1)
data04M07 <- Howell1 %>%
  filter(age  >= 18) %>%
  mutate(weight_c = as.vector(scale(weight, center = TRUE, scale = FALSE)))
rm(Howell1)
skimr::skim(data04M07)
```


$$
\begin{align*}
height_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta(x_i - \bar{x}) \\
\alpha &\sim \mathcal{N}(178, 20) \\
\beta &\sim \mathcal{LogNormal}(0, 1) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$
### Arguments {-}

The arguments will be kept in a list.  This is useful for repeated use and
ensure consistency when creating different inla model objects. For example
for fitted and predicted values computations as done below.

```{r}
i04M07args <- list()
```


#### `formula` {-}

See section 3.2.1 and 3.2.2 of @gomez2020, example on p. 40 at the end of 
section 3.2.2 on p. 41 and 42.

See section 5.2 in @gomez2020 on how to use the prior.

* the model is fixed (no random effect) and
* $x_3$ is a **constrained fixed effect** limited to the positive range.
See section 3.2.2 of @gomez2020 for details.

Therefore the model has a **constrained fixed effect** *weight_c* which is 
indicated by `clinear` in the `f()` function of the formula. This makes
*weight_c* a *latent effect* as opposed to a *likelihood effect* such
as $\sigma$ in the current model.

The available priors, defined in `f()` of *latent effect* are as follows

```{r}
names(inla.models()$prior)
```

and we will use `logtnormal` which is equivalent to our prior 
$\beta &\sim \mathcal{LogNormal}(0, 1)$ and is documented as follows

```{r}
# inla.doc(inla.models()$prior$logtnormal$pdf)  # this command does not work!
# inla.doc(inla.models()$prior$normal$pdf)  # this command works
```

which gives the formula of the linear predictor

```{r}
# linear predictor
i04M07args$formula <- height ~ f(weight_c, model = "clinear", range = c(0, Inf),
                         prior = "logtnormal", param = c(0, 1))
# the available latent models are in names(inla.models()$latent)
stopifnot("clinear" %in% names(inla.models()$latent))
```

#### `family` {-}

`INLA` can work with many families.  They are listed in `inla.models`.
For this case the family of the likelihood is

```{r}
# likelihood family
i04M07args$family <- "gaussian"
stopifnot(i04M07args$family %in% names(inla.models()$likelihood))
```


#### `control.fixed` {-}

See example on p. 47 at the end of section 3.2 of @wang2018.

As indicated in section 2.3.1, p.19, of @gomez2020 the prior for the intercept
is *gaussian* with the following latent hyperparameters which need to 
be modified for our model.

```{r}
inla.set.control.fixed.default()$mean.intercept
inla.set.control.fixed.default()$prec.intercept
```
Therefore the intercept prior for the  is $\alpha \sim \mathcal{N}(178, 20)$ where $sd = 20$ and 
precision is $\tau = \frac{1}{sd^2} = \frac{1}{20^2} = 0.0025$

```{r}
i04M07args$control.fixed <- list(mean.intercept = 178, prec.intercept = 1 / (20^2))
```

#### `control.family` {-}

See example on p. 47 at the end of section 3.2 of @wang2018.
See also section 5.2 in @gomez2020.

In our model we have the prior for the sigma of the likelihood set as

$$
\sigma \sim \mathcal{Exp}(rate_\sigma = 1)
$$
Since $\sigma^2 = \frac{1}{\tau}$ then in terms or $\tau$ the rate becomes
$rate_\tau = \frac{1}{\sigma^2}$ and thereofre


$$
\begin{align*}
\sigma \sim \mathcal{Exp}(rate_\sigma = 1) \\
\therefore \\
\tau \sim \mathcal{Exp}(rate_\tau = \frac{1}{\sigma^2})\\
\end{align*}
$$



As indicated in section 2.3.1, p. 19, of @gomez2020 the default of the likelihood
precision is gamma distribution with parameter $shape = 1$ and $rate = 0.00005$.
Since $shape = 1$ this is equivalent to the beta distribution then we use to
reflect our model's prior. i.e. 

$$
\tau \sim \mathcal{Gamma}(shape = 1, rate = \tau)
$$

The internal representation of the precision is $log{\tau}$ (see table 5.3 in section 
5.3.1 of @wang2018).  Therefore $\tau \sim \mathcal{Gamma}(1, 1 / log(1^2)) \approx \mathcal{Gamma}(1, 0.00005)$. 
We use 0.00005 because 0 cannot be used as a denominator.
It is the default setting.

```{r}
names(inla.models()$prior)
# change to TRUE to see document
if (FALSE) inla.doc("gamma", section = "prior")
```


Therefore we use

```{r}
i04M07args$control.family <- list(
    hyper = list(prec = list(prior = "loggamma", param = c(1, 0.00005))))
```


#### `control.compute` {-}

See section 3.4 in @wang2018 for model selection and checking.
For `cpo`, see section 3.4.3 in @wang2018 for details

This arguments, in the form of a list, is as follows

* `config = TRUE`: Compute the posteriors
* `dic = TRUE`: Compute the Divergence information criteria
* `waic = TRUE`: Compute the waic
* `cpo = TRUE`: Compute the conditional predictive ordinance.

which gives

```{r}
i04M07args$control.compute <- list(config = TRUE, dic = TRUE, waic = TRUE)
```

#### `quantiles` {-}

the quantiles for credibility intervals.  `inla` uses credibility intervals rather
than confidence intervals, usually similar when data has a centralized distribution.

```{r}
i04M07args$quantiles <- c(0.025, 0.5, 0.975)
```


### Create `inla` {-}

Since we are using a predefined list of arguments, it is convenient to use
`do.call`.  This ensure that the list of arguments can be reused later for
fitted and predict.

```{r}
# the data
i04M07args$data <- data04M07
# we use do.call to be able to use the list of parameters
i04M07 <- do.call(inla, args = i04M07args)
```



Note that all the arguments of the `inla` object can be accessed through
the `.args` item as follows

```{r}
names(i04M07$.args)
```

### Summaries {-}

which gives the summary for fixed parameters

```{r}
i04M07$summary.fixed
```

and hyperparameters which are shown on the *internal scale* which is the *log*
and this case.

```{r}
i04M07$summary.hyperpar
```


### Fitted {-#app-inla-04M07-fitted}


See [brinla](http://julianfaraway.github.io/brinla/examples/chicago.html)
on how to find the fit.

First we create the set of new data where the dependent variable has value `NA`

```{r}
# the new data
newdata <- list(n = 20L)
newdata$new <- data.frame(
  weight_c = seq_range(x = data04M07$weight_c, n = newdata$n),
  height = NA_real_
)
# the complete new dataframe with old and new data
newdata$data <- data04M07 %>%
  select(height, weight_c) %>%
  bind_rows(newdata$new)
```


We need to add `control.predictor=list(compute=TRUE)` to compute
the posterior mean of the linear predictors and use the data with the
sequence with `NA`

```{r}
# use the same arguments as the original and add the new data and
# the control.predictor
i04M07args$data <- newdata$data
i04M07args$control.predictor <- list(compute = TRUE)
i04M07 <- do.call(inla, args = i04M07args)
```

get the fitted data in a data.frame with only the new values

```{r}
# get the fitted data 
p <- list()
p$fitted <- i04M07$summary.fitted.values %>%
  select(mean, sd, `0.025quant`, `0.975quant`) %>%
  slice_tail(n = newdata$n) %>%
  bind_cols(weight_c = newdata$new$weight_c)
```

and plot the results

```{r}
# plot the fitted values against the raw data
ggplot(p$fitted, 
       aes(x = weight_c, y = mean, ymin = `0.025quant`, ymax = `0.975quant`)) +
  geom_lineribbon(size = 1, color = "rosybrown", fill = "rosybrown1") +
  geom_point(data = data04M07, aes(x = weight_c, y = height, color = age),
             inherit.aes = FALSE) +
  scale_x_continuous(breaks = scales::breaks_extended(n = 7),
                     labels = function(x) x + round(mean(data04M07$weight))) +
  scale_color_paletteer_c("scico::lapaz") +
  ggdist::theme_ggdist() +
  theme(legend.position = c(0.20, 0.80),
        title = element_text(color = "midnightblue")) +
  labs(title = "inla fit", x = "weight", y = "height")
```


### Predictions {-#app-inla-04M07-predict}

When it comes to making predictions, we encounter many different ways to do
it.  The recommended way from inla org is [inla](https://www.r-inla.org/faq#h.821k2r53fvx3)
with `i04M07$summary.random`. Thsi however returns a number of predictions
not equal to the number of predictions requested.  It is impossible to see
which predictions matches with which prediction.

Also see an example of another way in section 7.2.3, p.184 of @wang2018.

The method below was found on the internet.  Unfortunately I do not remember
the address.  My apologies. The method seems to work for any model which 
is why it is the chosen method.

The process is in 3 steps, the first 2 seps are the same as for the fitted
values (see just above)

1. Get the predicted `inla` object with `NA` values to predict
2. Sample the posterior with `inla.posterior.sample`
3. Simulate the posterior predictions by adding variability to the 
linear predictors

#### Predicted `inla` object {-}

We prepare the data set by adding data to predict

First we create the set of new data where the dependent variable has value `NA`

```{r}
# the new data
newdata <- list(n = 20L)
newdata$new <- data.frame(
  weight_c = seq_range(x = data04M07$weight_c, n = newdata$n),
  height = NA_real_
)
# the complete new dataframe with old and new data
newdata$data <- data04M07 %>%
  select(height, weight_c) %>%
  bind_rows(newdata$new)
```


We need to add `control.predictor=list(compute=TRUE)` to compute
the posterior mean of the linear predictors with the new data.

```{r}
# use the same arguments as the original and add the new data and
# the control.predictor
i04M07args$data <- newdata$data
i04M07args$control.predictor <- list(compute = TRUE)
i04M07 <- do.call(inla, args = i04M07args)
```


For every lines we can obtain the summary. For example for the new 20
predictions

```{r}
i04M07$summary.linear.predictor %>%
  filter(is.na(newdata$data$height))
```
and the marginal which was used to compute the line summary is available,
for example for $predictor.353$ which is the first new

```{r}
p <- list()
p$df <- i04M07$marginals.linear.predictor[["Predictor.353"]] %>%
  as.data.frame()
ggplot(p$df, aes(x = x, y = y)) +
  geom_line(color = "rosybrown", size = 1) +
  theme_minimal() +
  labs(x = "weight_c", y = "height")
```

**Important**: The variation from the **likelihood precision** is missing.  The
above is only the linear predictor itself.  When the link function is the
identify, the linear predictor is the fitted value. Therefore to get the predictions
we need to sample the posterior as follows.


#### Sample the posterior {-}

From the `inla` object, we extract a list of samples and, for each samples, 
the list of linear predictors

```{r}
samples <- list(n = 1000L)
samples$data <- inla.posterior.sample(n = samples$n, result = app_i04M07)
with(samples, stopifnot(length(data) == n))
```

Each sample in the list is structured as follows

```{r}
str(samples$data[[1]])
```

We need, for each sample

* The linear predictors which are numbered from their row no in the data
provided to `inla`
* The sample's precision to add variability to the linear predictor.  Remember,
from above, that the linear predictor is a fit and we need to add the likelihood
variability to it and this, for each sample.

For each sample, a dataframe of simulated data is found in the `latent` item.
The linear predictors are identified as `"Predictor:<row no>"`.

Since the new predictors are the last items in the latent dataframe we can 
extract them. Using the first sample as an example, we obtain

```{r}
# which gives the linear predictors for the first sample
cat("The linear predictors", "\n")
samples$data[[1]]$latent[353:372, ]
# and the sigma value for the first sample
cat("The sigma", "\n")
samples$data[[1]]$hyperpar[[1]]
```

therefore we use this information and repeat it for every sample to simulate
the linear predictors *with variability*.  The`predict_inla` function automates 
this process.

#### `predict_inla` {-#app-inla-04M07-predict_inla}

`predict_inla` executes the process of sampling the posterior prediction from
an inla model object.  It is actually simple.  Please see the documentation
in the `eflINLA` package for the function `predict_inla`.  For the `R` code,
just select `predict_inla` and press `F2` to see the code.

```{r}
app_i04M07args$control.predictor <- list(compute = TRUE)
app_i04M07args$data <- newdata$data
preds <- list(n = 1000L)
preds$inla <- do.call(INLA::inla, app_i04M07args)
preds$data <- eflINLA::predict_inla(object = preds$inla,
                                    pos = which(is.na(newdata$data$height)),
                                    n = preds$n)

# get the intervals
preds$intervals <- preds$data %>%
  purrr::map_df(.f = function(x) {ggdist::mean_qi(x, .width = c(.50))}) %>%
  mutate(weight_c = newdata$new$weight_c)
  

# put data in long format
preds$lng <- preds$data %>%
  pivot_longer(cols = everything(), names_to = "predictor", values_to = "height") %>%
  mutate(weight_c = rep(newdata$new$weight_c, times = preds$n))
# str(preds$lng)
```

plot the data

```{r}
ggplot(data04M07, aes(x = weight_c, y = height)) +
  stat_lineribbon(preds$lng, mapping = aes(x = weight_c, y = height),
                  color = "slateblue",inherit.aes = FALSE, .width = c(0.50, 0.75, 0.95),
                  show.legend = FALSE) +
  geom_point(color = "indianred") +
  scale_x_continuous(breaks = scales::breaks_extended(n = 7),
                     labels = function(x) round(x + mean(data04M07$weight))) +
  scale_fill_paletteer_d("ggsci::indigo_material") +
  ggdist::theme_ggdist() +
  theme(title = element_text(color = "midnightblue")) +
  labs(title = "Predictions with INLA", 
       subtitle = sprintf("sample size = %d, nb of predictors = %d", 
                          preds$n, length(preds$data)),
       x = "weight", y = "height")
```


##### Predictions vs marginal {-}

and we sample the prediction marginals to be able to compare with the prediction
samples.  The 2 of them should be similar.


```{r}
lpmargs <- list(margs = app_i04M07$marginals.linear.predictor[which(is.na(newdata$data$height))])
# get the sample for each predictor
lpmargs$data <- lapply(X = lpmargs$margs, FUN = function(x) {
  sample(x = x[, "x"],  size = samples$n, prob = x[, "y"], replace = TRUE)
  })
lpmargs$data <- do.call(cbind, lpmargs$data) %>%
  as.data.frame()
# str(lpmargs$data)

# both data.frame should have the same dim
stopifnot(all.equal(dim(preds$data), dim(lpmargs$data)))

# add the intervals
lpmargs$intervals <- lpmargs$data %>%
  purrr::map_df(.f = function(x) {ggdist::mean_qi(x, .width = c(.50))}) %>%
  mutate(weight_c = newdata$new$weight_c)
# lpmargs_summ
```

and we plot the results. The samples and marginals are close.  The marginals do
not take into account the variations.  If we increase `nsamples`, e.g. to 
5000, they become virtually the same.

```{r}
ggplot() +
  geom_point(data = preds$intervals, aes(x = weight_c, y = y, color = "samples"),
             size = 2) +
  geom_line(data = lpmargs$intervals, aes(x = weight_c, y = y, color = "marginals"),
            size = 1) +
  scale_color_manual(values= c("samples" = "darkmagenta", "marginals" = "peru")) +
  scale_x_continuous(breaks = scales::breaks_width(width = 5),
                     labels = function(x) x + round(mean(data04M07$weight))) +
  theme_minimal() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.20, 0.80)) +
  labs(title = "Comparing predictions' marginals and samples",
       subtitle = sprintf("sample size = %d, nb of predictors = %d", 
                          nrow(preds$data), nrow(preds$intervals)),
       x = "weight", y = "height", color = NULL)
```


<!--chapter:end:61_inla.Rmd-->


# `eflINLA` {-#eflINLA}

Placeholder


## Introduction {-#eflINLA-intro}
## Marginal draws {-#eflINLA-marg}
### Sample posterior: `tidy_marg_draws_inla`
### Linear predictors: `linpred_marg_draws_inla` {-#eflINLA-marg-linpred}
### Fitted predictors: `epred_marg_draws_inla` {-#eflINLA-marg-epred}
### Posterior predictions: `predict_inla` {-#eflINLA-marg-predict}

<!--chapter:end:62_eflINLA.Rmd-->

