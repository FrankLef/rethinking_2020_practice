```{r include=FALSE}
library(rethinking)
library(brms)
library(INLA)
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
```

# Linear Models {#linear}

## Notes {-}

### Mode {-}

Technically, the mode is defined as the most likely value.  Usually
the following function does precisely that.


```{r}
# find the mode
mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```


However `ggdist::Mode()` which is used in all `ggdist::mode_xxxx()` functions
gives a different number which is, in the case of a continuous variable, the
value with the highest probabilioty density.

```{r}
data(Howell1)
d <- Howell1 %>%
  filter(age  >= 18)
rm(Howell1)
the_modes <- c("mode" = mode(d$height), "ggdist" = ggdist::Mode(d$height)) 
ggplot(d, aes(x = height)) +
  geom_density() +
  geom_vline(xintercept = the_modes, color = c("red", "blue")) +
  geom_text(data = data.frame(x = the_modes, y = c(0.01, 0.02)),
            aes(x = x, y = y, label = x), color = c("red", "blue"), 
            inherit.aes = FALSE) +
  theme_light() +
  theme(panel.grid = element_blank()) +
  labs(title = "Compare mode() and ggdist::Mode()",
       subtitle = sprintf("ggdist::Mode = %.6f\nmode = %f", 
                          the_modes["ggdist"], the_modes["mode"]))
```

As shown just above, when plotting the density, `ggdist::Mode()` always gives the
right position with maximum density.  The custom `mode()` function, as recommended
for example at [mode](https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode)
is pretty much always off.  Often by a lot.

The `ggdist::Mode()` is the function used in this project.

## Custom functions {-}

### With `ggdist` {-}

```{r}
# Pivot a dataframe from ggdist::point_interval to longer format
# This is a useful function when plotting with ggrepel, for example
gather_interval <- function(df, value_var, func=ggdist::median_qi, 
                            width = c(2/3, 0.89),
                            names_to="stat", values_to="value") {
  val_col <- deparse1(substitute(value_var))
  df %>%
    func({{value_var}}, .width = width) %>%
    select(-.interval) %>%
    pivot_longer(cols = all_of(c(val_col, ".lower", ".upper")), 
                 names_to = names_to, 
                 values_to = values_to) %>%
    filter(!(stat == val_col & .width != .width[1]))  ## remove repeated "value"
}
```

test `gather_interval`

```{r}
nsamples <- 10L
df <- data.frame(x = rep(c("blue", "red"), times = nsamples),
                 y = c(rnorm(n = nsamples), 
                       rnorm(n = nsamples, mean = 10, sd = 5)))
# test with grouping variable
df <- df %>%
  group_by(x) %>%
  gather_interval(value_var = y, func = median_qi, width = c(2/3, 0.89))
# df
# every line must be uniquely identified
stopifnot(any(!duplicated(paste(df$x, df$stat, df$.width))))
```



### With `inla` {-}

In order to compare to `brms` we need to convert the marginals as follows
* Convert internal scale (log) to natural scale
* Convert precision (`prec`) to standard deviation (`sd`)

When doing summaries later, we will need to
use the inversed transformed `internal.marginal.hyperpar` for the precision and
`marginals.hyperpar` for the other hyperparameters. The `marginals.hyperpar` for
precision is not used for summaries and plotting.

```{r}
# Process the hyperparameters' marginals
#  - use inverse transformed internal.marginals.hyperpar for precision
#  - marginals.hyperpar for other hyperparameters
transform_hyper_marginal <- function(obj, sd_name = "sd") {
  stopifnot(class(obj) == "inla")
  stopifnot(length(obj$marginals.hyperpar) != 0L)
  stopifnot(length(obj$internal.marginals.hyperpar) != 0L)
  stopifnot(is.character(sd_name), nchar(sd_name) != 0L)
  
  # don't modify this without a good reason and test it
  # the regex pattern used to find the precision word
  prec_regex = "precision"
  
  marg_lst <- mapply(
    FUN = function(marg, nm, imarg) {
    if(grepl(pattern = prec_regex, x = nm, ignore.case = TRUE)) {
        # use inverse transformed internal.marginals.hyperpar for precision
        m <- INLA::inla.tmarginal(fun = function(x) 1 / sqrt(exp(x)), 
                                  marginal = imarg)
      } else {
        # use marginals.hyperpar for other hyperparameters
        m <- marg
      }
    return(m)
    }, 
  obj$marginals.hyperpar,
  names(obj$marginals.hyperpar),
  obj$internal.marginals.hyperpar,
  SIMPLIFY = FALSE)
  
  # edit the names to replace "precision" by sd_name
  names(marg_lst) <- sub(pattern = prec_regex, replacement = sd_name,
                         x = names(marg_lst), ignore.case = TRUE)
  marg_lst
}
```



```{r}
# Create the summary for hyperparameters
write_hyper_summary <- function(margs, qtl = c(0.025, 0.5, 0.975)) {
  stopifnot(is.list(margs), length(margs) != 0L)
  stopifnot(is.numeric(qtl), length(qtl) != 0L)
  
  # suffix used by INLA, usually not recommended to start the
  # name of a variable with a number as INLA does.
  # Done here just to be consistent with INLA.
  qtl_suffix <- "quant"
  
  marg_lst <- mapply(
    FUN = function(mrg, nm) {
      the_mean <- INLA::inla.emarginal(fun = function(x) x, marginal = mrg)
      the_sd <- sqrt(max(0, 
                         INLA::inla.emarginal(fun = function(x) x^2 - the_mean^2, 
                                              marginal = mrg)
                         )
                     )
    the_qtl <- t(INLA::inla.qmarginal(qtl, marginal = mrg))
    colnames(the_qtl) <- paste0(qtl, qtl_suffix)
    the_mode <- INLA::inla.mmarginal(mrg)
    data.frame(mean = the_mean,
               sd = the_sd,
               the_qtl,
               mode = the_mode,
               row.names = nm,
               check.names = FALSE)
    },
    margs,
    names(margs),
    SIMPLIFY = FALSE
    )
  do.call(rbind, marg_lst)
  }
```

The variable names used by INLA are not easy to work with, this function converts
these variable names to the naming convention used by `brms`

```{r}
rename_inlavars <- function(x) {
  stopifnot(is.character(x), length(x) != 0)
  
  df <- data.frame(
    pattern = c("[(]Intercept[)]", "^Beta for ", "sd for .+ observations.*"),
    replace = c("b_Intercept", "b_", "sigma"))
  
  for (i in seq_len(nrow(df))) {
    x <- sub(pattern = df$pattern[i], replacement = df$replace[i],  x = x)
    }
  x
}
```

test `rename_inlavars`

```{r}
vars <- c("(Intercept)", "sd for the Gausssian observations   ",
          "Beta for weight_c")
out <- rename_inlavars(vars)
# str(out)
stopifnot(identical(out, c("b_Intercept", "sigma", "b_weight_c")))
```



## 4E1 {-#prac4E1}

See section 4.4.1 in @elreath2020

$y_i \sim \mathcal{N}(\mu, \sigma)$

## 4E2 {-#prac4E2}


2 parameters, $\mu$ and $\sigma$ which are in the posterior distribution
$y_i \sim \mathcal{N}(\mu, \sigma)$

## 4E3 {-#prac4E3}

See Overthinking in section 4.3.1

$$
\begin{align*}
P(\mu, \sigma \mid y) &=

\frac{
    P(y, \mu, \sigma)
}{
    P(y)
} \\

&= \frac{
    P(y, \mu, \sigma)
}{
    \int_{\sigma} \int_{\mu} P(y \mid \mu, \sigma) \cdot P(\mu, \sigma) d\mu d\sigma \\
} \\

&= 

\frac{
    \prod_i P(y_i, \mu, \sigma)
}{
    \int_{\sigma} \int_{\mu} \prod_i P(y_i \mid \mu, \sigma) \cdot P(\mu, \sigma) d\mu d\sigma \\
} \\

&=

\frac{
    \prod_i P(y_i \mid \mu, \sigma) \cdot P(\mu) \cdot P(\sigma)
}{
    \int_{\sigma} \int_{\mu} \prod_i P(y_i \mid \mu, \sigma) \cdot P(\mu) \cdot P(\sigma) d\mu d\sigma \\
} \\

&=

\frac{
    \prod_i \mathcal{N}(y_i \mid \mu, \sigma) \cdot \mathcal{N}(\mu \mid mean = 0, sd = 10) \cdot \mathcal{Exponential}(\sigma \mid rate = 1)
}{
    \int_{\sigma} \int_{\mu}{
        \prod_{i=1}^n \mathcal{N}(y_i \mid \mu, \sigma) \cdot \mathcal{N}(\mu \mid mean = 0, sd = 10) \cdot \mathcal{Exponential}(\sigma \mid rate = 1)
    }
    d\mu d\sigma
}

\end{align*}
$$


## 4E4 {-#prac4E4}

$\mu_i = \alpha + \beta x_i$

## 4E5 {-#prac4E5}

2 parameters, $\mu$ and $\sigma$

## 4M1 {-#prac4M1}

See R code 4.13 in section 4.3.2 using this model

```{r}
# simulate the heights using the prior, not the posterior
set.seed(4)  # set the seed as random sample can vary and give error later
nsamples <- 1e4
sample_mu <- rnorm(n = nsamples, mean = 0, sd = 10)
sample_sigma <- rexp(n = nsamples, rate = 1)
df <- data.frame(height = rnorm(n = nsamples, mean = sample_mu, sd = sample_sigma))
```

```{r}
a_width <- c(2/3, 0.95, 1)

# dataframe of intervals used by ggrepel::geom_text_repel
df1 <- df %>%
  # see gather_interval() at the beginning
  gather_interval(value_var = height, func = median_qi, width = a_width)
# df1

ggplot(data = df, mapping = aes(x = height)) +
  ggdist::stat_halfeye(aes(fill = stat(cut_cdf_qi(cdf, .width = a_width))),
                       color = "darkorange") +
  scale_x_continuous(breaks = df1$value, labels = round(df1$value, 1)) +
  scale_fill_paletteer_d("ggsci::indigo_material", 
                         direction = 1,
                         name = "levels %",
                         labels = round(100 * a_width, 0),
                         na.translate = FALSE) +
  ggrepel::geom_text_repel(data = df1,
                           aes(x = value, label = round(value, 1), y = 0),
                           inherit.aes = FALSE,
                           color = "violetred") +
  theme_void() +
  theme(legend.position = c(0.8, 0.8),
        title = element_text(color = "midnightblue")) +
  labs(title = "Prior prediction of height", 
       subtitle = sprintf("4M1, sample size = %d", nrow(df)))
```

## 4M2 {-#prac4M2}

### with `quap` {-}

See section 4.4.2 on how to use `quap`, R code 4.43 with linear equation with
the `quap` formula, i.e. `flist = alist(...)`.


```{r eval=FALSE}
alist(
    height ~ dnorm(mean = mu, sd = sigma),
    mu = a + b * x,
    a ~ dnorm(mean = 0, sd = 10),
    b ~ dnorm(mean = 0, sd = 1),
    sigma ~ dexp(rate = 1)
    )
```


## 4M3 {-#prac4M3}

Make sure you remember to index the $y$ so that it is $y_i$ as well as $\mu$.
See section 4.4.2.  The published answer does not put an index on $\mu$ but it
has one on $\y$ which involves a constant $\mu$ which is normally the intercept!
This question is confusing, here we assume the same meaning as in section 4.4.2.,
that is $\mu$ varies for each $x_i$

$$
\begin{align*}
y_i &\sim \mathcal{N}(mean = \mu_i, sd = \sigma)\\
\mu_i &= \alpha + \beta x_i \\
\alpha &\sim \mathcal{N}(mean = 0, sd = 10) \\
\beta &\sim \mathcal{Uniform}(mean = 0, sd = 1) \\
\sigma &\sim \mathcal{Exponential}(\lambda = 1)
\end{align*}
$$

## 4M4 {-#prac4M4}

Don't forget the index so that $height$ is $height_i$.

This will be giving the average height per year.  The question is not clear
that it wants it by student also.

$$
\begin{align*}
height_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta \cdot year_i \\
\alpha &\sim \mathcal{N}(100, 10) \\
\beta &\sim \mathcal{Uniform}(0, 10) \\
\sigma &\sim \mathcal{Exponential}(1)
\end{align*}
$$

## 4M5 {-#prac4M5}

This tells us that $\beta$ should always be positive be with large values unlikely.
We therefore use the log-normal dist as a prior for $\beta$. See section 4.4.2.

$$
\begin{align*}
height_i &\sim Normal(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta \cdot year_i \\
\alpha &\sim \mathcal{Normal}(100, 10) \\
\beta &\sim \mathcal{LogNormal}(0, 1) \\
\sigma &\sim \mathcal{Exponential}(1)
\end{align*}
$$

## 4M6 {-#prac4M6}

Instead of using the average range as a prior for $\sigma$ we would use
$\sigma = \sqrt{64} = 8$.

> In the official solution solution McElreath says it should be 
$\sigma \sim Uniform(0, 64)$, no sqrt of the *variance* to obtain the
*standard deviation* is done.

$$
height_i \sim Normal(\mu_i, \sigma) \\
mu_i = \alpha + \beta \cdot year_i \\
\alpha \sim Normal(120, 10) \\
\beta \sim Normal(0, 10) \\
\sigma \sim Uniform(0, 8)
$$

## 4M7 {-#prac4M7}

See section 4.4.2 for model m4.3.  We add the centered weight to the data
and call it $weight_c$.

```{r}
data(Howell1)
data04M07 <- Howell1 %>%
  filter(age  >= 18) %>%
  mutate(weight_c = as.vector(scale(weight, center = TRUE, scale = FALSE)))
rm(Howell1)
skimr::skim(data04M07)
```

### Model {-#prac4M7-Model}

$$
\begin{align*}
height_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta(x_i - \bar{x}) \\
\alpha &\sim \mathcal{N}(178, 20) \\
\beta &\sim \mathcal{LogNormal}(0, 1) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$

### Priors {-#prac4M7-Priors}


The following priors are evaluated by comparing the prior distribution
and the prior predictive distribution (also called likelihood).

```{r}
priors <- list(
  alpha_mean = 178,
  alpha_sd = 20,
  beta_mean = 0,
  beta_sd = 1,
  sigma_rate = 1,
  # nb of samples from the prior
  nsamples = 30L,
  # nb of observations for each of the prior sample
  ntrials = 500L
)
```

The prior predictive distribution is simulated as follows. It is heavily
inspired by @kurtz2020b. See also [vasishth](https://vasishth.github.io/bayescogsci/book/sec-priorpred.html)
for a similar way to do it.

**Important:** We could use the prior prediction from the ``brms` fit
objects.  However then we depend on the `rstan` sampler which is subject
to possible convergence problems.  This method is independent of the fitting
methodology.  Not to mention it makes one really understand what is going on
which is the avowed objective of this book. This method is also used in 4H1
below.


```{r}
set.seed(4)
df <- data.frame(
  weight_c = modelr::seq_range(x=data04M07$weight_c , n = priors$nsamples),
  alpha = rnorm(n = priors$nsamples, mean = priors$alpha_mean, sd = priors$alpha_sd),
  beta = rlnorm(n = priors$nsamples, meanlog = priors$beta_mean, sdlog = priors$beta_sd),
  sigma = rexp(n = priors$nsamples, rate = priors$sigma_rate)) %>%
  mutate(mu = alpha + beta * weight_c)
# str(df)

prior_sim <- df %>%
  expand(nesting(weight_c, mu, sigma)) %>%
  mutate(height = purrr::map2(.x = mu, .y = sigma, 
                              .f = function(mu, sigma) {
                                data.frame(
                                  trial = seq_len(priors$ntrials),
                                  height_pred = rnorm(n = priors$ntrials, 
                                                  mean = mu, sd = sigma))
                              })) %>%
  unnest(cols = c(height))
# str(prior_sim)
```
and plot the priors vs the observations

```{r}
ggplot(prior_sim) +
  geom_density(aes(x = height_pred, y = ..scaled.., color = "prior predicted"),
               size = 1, alpha = 0.8) +
  geom_density(data = data04M07, aes(x = height, y = ..scaled.., color = "observed"),
               size = 1, alpha = 0.8) +
  geom_vline(xintercept = c(ggdist::Mode(prior_sim$height_pred),
                            ggdist::Mode(data04M07$height)), 
             color = c("mediumvioletred", "mediumseagreen"),
             linetype = "dashed") +
  scale_color_manual(values = c("prior predicted" = "mediumvioletred", 
                                "observed" = "mediumseagreen")) +
  scale_x_continuous(breaks = scales::breaks_pretty(n = 7)) +
  ggdist::theme_ggdist() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.8, 0.8)) +
  labs(title = paste0(
    "Comparing prior predictive distribution vs observed distribution",
    "\n", "vertical lines = mode"),
    subtitle = "4M7",
    x = "height", y = NULL, color = NULL)
```


The prior is reasonable as it reflects an opinion on the overall general
population.


### Using `quap` {-}

#### Centered scale {-}

The fit using the centered, model m4.3 in textbook.


```{r}
a_file <- here::here("fits", "m04M07ctr.rds")
m04M07ctr <- readRDS(file = a_file)
# df <- data04M07
# m04M07ctr <- quap(
#   flist = alist(
#     height ~ dnorm(mu, sigma),
#     mu <- a + b * weight_c,
#     a ~ dnorm(178, 20),
#     b ~ dlnorm(0, 1),
#     sigma ~ dunif(0, 50)
#   ),
#   data = df,
#   start = list(a = mean(df$weight), b = 0.5)
# )
# saveRDS(object = m04M07ctr, file = a_file)
precis(m04M07ctr)[, 1:2]
```


#### Natural scale {-}

The fit using the predictor on the natural scale


```{r}
a_file <- here::here("fits", "m04M07nat.rds")
m04M07nat <- readRDS(file = a_file)
# df <- data04M07
# m04M07nat <- quap(
#   flist = alist(
#     height ~ dnorm(mu, sigma),
#     mu <- a + b * weight,
#     a ~ dnorm(178, 20),
#     b ~ dlnorm(0, 1),
#     sigma ~ dunif(0, 50)
#   ),
#   data = df,
#   start = list(a = mean(df$weight), b = 0.5)
# )
# saveRDS(object = m04M07nat, file = a_file)
precis(m04M07nat)[, 1:2]
```


#### Covariances {-}


The parameter `corr = TRUE` does not seem to work
in `precis` so we use the var-cov matrix and convert it to correlations.

```{r}
round(cov2cor(vcov(m04M07ctr)), 4)
```


```{r}
round(cov2cor(vcov(m04M07nat)), 4)
```

Comments:

* The effect ($b$) and sigma are the same but the $a$ (Intercepts) coefficients
are different.
* The correlations are strong on the natural scale and
non-existent on the centered scale.  This is an effect that is well documented with
the correlation coefficient when *distant data points from the origin* are observed
* The 2 models, on centered and natural scales give the same prediction.

#### Posteriors {-}

```{r}
m04M07ctr_post <- extract.samples(
  object = m04M07ctr,
  n = 1000)
m04M07nat_post <- extract.samples(
  object = m04M07nat,
  n = 1000)

m04M07all_post <- m04M07ctr_post %>%
  bind_rows(m04M07nat_post) %>%
  mutate(model = factor(
    c(rep("center", length.out = n() / 2),
      rep("natural", length.out = n() / 2)
      ))) %>%
  pivot_longer(cols = c("a", "b", "sigma"), names_to = "vars")
```

the function and intervals used to plot the posteriors

```{r}
# Function to create the plot of posterior with halfeye geom
plot_post <- function(df, df1, x_var = "value", y_var = "model", 
                     fill_var = "model", x1_var = "value", y1_var = "model",
                     func, width = c(0.89), round = 0, text_size = 3,
                     the_labs = NULL,
                     clrs = list(color = "yellowgreen", 
                                 fill = "ggsci::default_nejm",
                                 title = "midnightblue")) {
  ggplot(df, 
         aes(x = .data[[x_var]], y = .data[[y_var]], fill = .data[[fill_var]])) +
  stat_halfeye(point_interval = func, .width = width,
               color = clrs$color) +
  scale_fill_paletteer_d(clrs$fill) +
  ggrepel::geom_text_repel(data = df1, 
                           aes(label = c(round(.data[[x1_var]], round)), 
                               y = .data[[y1_var]]),
                           color = "black", size = text_size) +
  ggthemes::theme_tufte() +
  theme(legend.position = "none",
        title = element_text(color = clrs$title),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
    the_labs
}

# Function to create the dataframe used for geom_text_repel
# need to remove the mean ("value") which repeats itself
df_post <- function(df, grp1_var, grp2_var, value_var, func = ggdist::median_qi, 
                    width = c(2/3)) {
  df %>%
    group_by({{grp1_var}}, {{grp2_var}}) %>%
    func({{value_var}}, .width = width) %>%
    select(-.interval) %>%
    pivot_longer(cols = c("value", ".lower", ".upper"), names_to = "stat") %>%
    filter(!(stat == "value" & .width != .width[1]))  ## remove repetitions of value
}
```



```{r}
a_func <- ggdist::median_qi
a_width <- c(2/3)

post_intervl <- m04M07all_post %>%
  select(model, vars, value) %>%
  group_by(model, vars) %>%
  gather_interval(value_var = value, func = a_func, width = a_width)

df <- m04M07all_post %>%
  filter(vars == "a")
df1 <- post_intervl %>%
  filter(vars == "a")
pA <- plot_post(df, df1, x_var = "value", y_var = "model", fill_var = "model", 
                x1_var = "value", y1_var = "model",
                func = a_func, width = a_width, round = 0, 
                the_labs = labs(title = "Intercept", x = NULL, y = NULL)) +
  theme(legend.position = "right")
# pA


df <- m04M07all_post %>%
  filter(vars == "b")
df1 <- post_intervl %>%
  filter(vars == "b")
pB <- plot_post(df, df1, x_var = "value", y_var = "model", fill_var = "model", 
                x1_var = "value", y1_var = "model",
                func = a_func, width = a_width, round = 2, 
                the_labs = labs(title = "b slope", x = NULL, y = NULL))
# pB


df <- m04M07all_post %>%
  filter(vars == "sigma")
df1 <- post_intervl %>%
  filter(vars == "sigma")
pSigma <- plot_post(df, df1, x_var = "value", y_var = "model", fill_var = "model", 
                    x1_var = "value", y1_var = "model",
                    func = a_func, width = a_width, round = 1, 
                    the_labs = labs(title = "Sigma", x = NULL, y = NULL))
# pSigma


p <- pA / (pB | pSigma) +
  plot_annotation(title = "Posterior comparisons by model",
                  theme = theme(title = element_text(color = "darkblue")))
p
```

Conclusion: The main difference is with respect to the intercept which is 
different for uncentered and centered weight.  The difference is not only
in the location but also in the scale.  The uncentered weight  is much
more inaccurate as its variance is large compared to the centered weight.

#### Predictions {-}

First we extract the simulated data for the mu values (the fit) and for the 
predicted heights with the models.

The functions and variables used

```{r}
# get the interval from the quap object
get_pred <- function(df, x_pred, func = ggdist::mean_qi, width = c(0.89)) {
  # must be exactly of length 1, do not use multiple intervals
  stopifnot(length(width) == 1)
  
  df %>%
    as.data.frame() %>%
    purrr::map_dfr(.f = func, .width = width) %>%
    mutate(x = x_pred) %>%
    relocate(x)
}

# plot the predictions and the fit against the raw data
plot_all <- function(df, df_fitted, df_predict, x_var, y_var, color_var,
                     colrs = list(
                       pred_fill = "lightgoldenrod1",
                       fit_fill = "palegreen1",
                       fit_color = "palegreen4",
                       raw_pal = "scico::lapaz"),
                     the_labs) {
  ggplot(as.data.frame(fit@data), aes(x = .data[[x_var]], 
                                      y = .data[[y_var]], 
                                      color = .data[[color_var]])) +
  geom_ribbon(data = df_predict,
              aes(x = x, ymin = ymin, ymax = ymax),
              inherit.aes = FALSE, fill = colrs$pred_fill) +
  geom_lineribbon(data = df_fitted,
                          aes(x = x, y = y, ymin = ymin, ymax = ymax),
                          inherit.aes = FALSE, fill = colrs$fit_fill,
                          color = colrs$fit_color) +
  geom_point() +
  scale_color_paletteer_c(colrs$raw_pal) +
  theme_ggdist() +
  theme(legend.position = "none",
        title = element_text(size = 10)) +
  the_labs
}

npred <- 30  # nb of predictions
nsamples <- 1000  # nb of samples
a_width = 2/3  # default width used
a_func <- ggdist::mean_qi  # function used for intervals
```

The model on the centered scale

```{r}
fit <- m04M07ctr  # the fit object

# the sequence of weights to predict height
weight_pred <- range(fit@data$weight_c)
weight_pred <- seq(from = weight_pred[1], to = weight_pred[2], length.out = npred)
weight_mean <- mean(fit@data$weight)  # to use with inverse transform in plot

# the posterior predictions with intervals
pred_center <- rethinking::sim(fit, data = list(weight_c = weight_pred), n = nsamples)
pred_center <- get_pred(pred_center, x_pred = weight_pred + weight_mean, 
                        func = a_func, width = a_width)
# pred_center

# the posterior fit
fitted_center <- rethinking::link(fit, data = list(weight_c = weight_pred), n = nsamples)
fitted_center <- get_pred(fitted_center, x_pred = weight_pred + weight_mean, 
                          func = a_func, width = a_width)
# fitted_center

p_center <- plot_all(df = as.data.frame(fit@data), 
                     df_fitted = fitted_center, df_predict = pred_center,
                     x_var = "weight", y_var = "height", color_var = "age",
                     the_labs = labs(title = "Model on centered scale",
                                     subtitle = sprintf("%.0f%% interval", 100 * a_width)))
# p_center
```

The model on the natural scale

```{r}
fit <- m04M07nat

# the sequence of weights to predict height
weight_pred <- range(fit@data$weight)
weight_pred <- seq(from = weight_pred[1], to = weight_pred[2], length.out = npred)

# the posterior predictions
pred_natural <- rethinking::sim(fit, data = list(weight = weight_pred), n = nsamples)
pred_natural <- get_pred(pred_natural, x_pred = weight_pred, 
                        func = a_func, width = a_width)
# glimpse(pred_natural)

# the posterior fit
fitted_natural <- rethinking::link(fit, data = list(weight = weight_pred), n = nsamples)
fitted_natural <- get_pred(fitted_natural, x_pred = weight_pred, 
                        func = a_func, width = a_width)
# glimpse(fitted_natural)

p_natural <- plot_all(df = as.data.frame(fit@data), 
                     df_fitted = fitted_center, df_predict = pred_center,
                     x_var = "weight", y_var = "height", color_var = "age",
                     colrs = list(
                       pred_fill = "palegreen1",
                       fit_fill = "lightgoldenrod1",
                       fit_color = "lightgoldenrod4",
                       raw_pal = "scico::lapaz"),
                     the_labs = labs(title = "Model on natural scale",
                                     subtitle = sprintf("%.0f%% interval", 100 * a_width)))
# p_natural
```

with the 2 plots

```{r}
p_center + p_natural +
  plot_annotation(title = "Both model give the same predictions.")
```


and comparing the 2 sets of predictions

```{r}
df <- data.frame(
  centered = pred_center$y,
  natural = fitted_center$y
)
ggplot(df, aes(x = centered, y = natural)) +
  geom_point() +
  theme_ggdist() +
  labs(title = "Predictions from model with natural scal vs model with centered scale")
```



### Using `brm` {-}

#### Centered scale {-}


```{r}
a_file <- here::here("fits", "b04M07ctr.rds")
b04M07ctr <- readRDS(file = a_file)
# b04M07ctr <- brms::brm(
#   data = data04M07,
#   family = gaussian,
#   formula = height ~ 1 + weight_c,
#   prior = c(
#     prior(normal(178, 20), class = Intercept),
#     prior(lognormal(0, 1), class = b, lb = 0, ub = 3),
#     prior(exponential(1), class = sigma)),
#   iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),  seed = 4)
# saveRDS(b04M07ctr, file = a_file)
```

```{r}
summary(b04M07ctr)
```

#### Natural scale {-}

```{r}
a_file <- here::here("fits", "b04M07nat.rds")
b04M07nat <- readRDS(file = a_file)
# b04M07nat <- brms::brm(
#   data = data04M07,
#   family = gaussian,
#   formula = height ~ 1 + weight,
#   prior = c(
#     prior(normal(178, 20), class = Intercept),
#     prior(lognormal(0, 1), class = b, lb = 0, ub = 3),
#     prior(exponential(1), class = sigma)),
#   iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),  seed = 4)
# saveRDS(b04M07nat, file = a_file)
```


```{r}
summary(b04M07nat)
```

and to calculate the correlations and visualize we sample the posterior


```{r}
b04M07ctr_post <- posterior_samples(b04M07ctr)
b04M07nat_post <- posterior_samples(b04M07nat)
```

which gives the following correlations which are the same as with `quap` above

```{r}
b04M07ctr_post %>%
  select(-lp__) %>%
  cor() %>%
  round(digits=2)
```


```{r}
b04M07nat_post %>%
  select(-lp__) %>%
  cor() %>%
  round(digits=2)
```
#### Predictions {-}

and we plot the posterior predictions which is easier with `brms` because
we can use the `ggdist` and `tidybayes` packages.

```{r}
# the data and variables used in this chunk
df_raw <- data04M07
n_pred <- 30  # nb of predictions
n_samples <- 1000  # nb of samples
a_width <- c(0.50, 0.75, 0.95)
weight_mean <- mean(df_raw$weight)  # used to inverse the transform

# plot prediction using the model on the centered scale
p_center <- df_raw %>%
  expand(weight_c) %>%
  modelr::seq_range(n = n_pred) %>%
  data.frame(weight_c = .) %>%
  tidybayes::add_predicted_draws(b04M07ctr, ndraws = n_samples) %>%
  ggplot(aes(x = weight_c)) +
  stat_lineribbon(aes(y = .prediction), .width = a_width) +
  geom_point(data = df_raw, aes(y = height, color = age)) +
  scale_x_continuous(breaks = scales::breaks_extended(n = 7),
                     labels = function(x) round(x + weight_mean, 1)) +
  scale_fill_paletteer_d("ggsci::deep_purple_material") +
  scale_color_paletteer_c("scico::lapaz") +
  theme_ggdist() +
  labs(title = "Predictions with model on centered scale",
       x = "centered weight + mean(weight)", y = "height")
# p_center


# plot prediction using the model on the natural scale
p_natural <- df_raw %>%
  expand(weight) %>%
  modelr::seq_range(n = n_pred) %>%
  data.frame(weight = .) %>%
  tidybayes::add_predicted_draws(b04M07nat, ndraws = n_samples) %>%
  ggplot(aes(x = weight)) +
  stat_lineribbon(aes(y = .prediction), .width = a_width) +
  geom_point(data = df_raw, aes(y = height, color = age)) +
  scale_x_continuous(breaks = scales::breaks_extended(n = 7)) +
  scale_fill_paletteer_d("ggsci::deep_orange_material") +
  scale_color_paletteer_c("scico::lapaz") +
  theme_ggdist() +
  labs(title = "Predictions with model on natural scale",
       x = "weight", y = "height")
# p_natural


p_center / p_natural +
  plot_annotation(title = "Comparing 2 models", subtitle = "04M07")
```


and again with the same results as when using `quap`.


### Using `inla` {-}

The fit with `inla` will only be done for the data on the natural scale.
It can easily be duplicated for the centered data.

#### Arguments {-}

Please see [details](#app-inla-04M07) for explanations on the
arguments used.

We will use a list to hold the arguments as they will be used repeatedly.

```{r}
i04M07args_ctr <- list()
```


##### `formula` {-}

See section 3.2.1 and 3.2.2 of @gomez2020, example on p. 40 at the end of 
section 3.2.2 on p. 41 and 42.

* the model is fixed (no random effect) and
* the $\beta$ coefficient is constrained, i.e. must be positive

Therefore the model has a constrained fixed effect *weight_c* which is 
indicated by `clinear` in the `f()` function of the formula. This makes
*weight_c* a *latent effect* as opposed to a *likelihood effect* such
as $\sigma$ in the current model.

The available priors of *latent effect* are as follows

```{r}
names(inla.models()$prior)
inla.models()$prior$logtnormal
# inla.doc("logtgaussian")
```



Therefore we will use

```{r}
i04M07args_ctr$frmla <- formula(height ~ f(weight_c, 
                                          model = "clinear", 
                                          range = c(0, Inf)))
# the available latent models are in names(inla.models()$latent)
stopifnot("clinear" %in% names(inla.models()$latent))
```

##### `family` {-}

`INLA` can work with many families.  They are listed in `inla.models`

```{r}
i04M07args_ctr$family <- "gaussian"
stopifnot(i04M07args_ctr$family %in% names(inla.models()$likelihood))
```


##### `control.fixed` {-}

See example on p. 47 at the end of section 3.2 of @wang2018.

The intercept prior is $\alpha \sim \mathcal{N}(178, 20)$ where $sd = 20$ and 
therefore precision is

```{r}
1 / (20^2)
```

and similarly the effect prior is $\beta \sim \mathcal{LogNormal}(0, 1)$ which
is a constrained effect described in the *Model* just above. Converting the mean
and sd we have the precision

```{r}
exp(0)  # exp(1) used because original distribution is LogNormal
1 / (exp(1)^2)  # exp(1) used because original distribution is LogNormal
```

which gives

```{r}
i04M07args_ctr$fixed <- list(
  mean.intercept = 178, 
  prec.intercept = 1 / (20^2),
  mean = list(weight = exp(0)),
  prec = list(weight = 1 / (exp(1)^2))
  )
```

##### `control.family` {-}

See example on p. 47 at the end of section 3.2 of @wang2018.
See also section 3.5.1 in @gomez2020.

By default, the prior of the family precision $\tau$ is assumed to be a
diffuse gamma. Since $\tau$ is expressed as $\log{}\tau$ internally, setting
the hyperparameter family to gaussian, give a log normal distribution.

Therefore we use

```{r}
i04M07args_ctr$hyper <- list(prec = list(prior = "gaussian", param = c(0, 1)))
```



##### `control.compute` {-}

See section 3.4 in @wang2018 for moel selection and checking.
For `cpo`, see section 3.4.3 in @wang2018 for details

This arguments, in the form of a list, is as follows

* `config = TRUE`: Compute the posteriors
* `dic = TRUE`: Compute the Divergence information criteria
* `waic = TRUE`: Compute the waic
* `cpo = TRUE`: Compute the conditional predictive ordinance.

which gives

```{r}
i04M07args_ctr$compute <- list(config=TRUE, dic = TRUE, waic = TRUE)
```

##### Miscellaneous {-}

the quantiles used, they are used later whendoing summaries

```{r}
i04M07args_ctr$quantiles <- c(0.025, 0.5, 0.975)
```


#### Create `inla` {-}

```{r}
a_file <- here::here("fits", "i04M07nat.rds")
i04M07nat <- readRDS(file = a_file)
# i04M07nat <- inla(
#   formula = i04M07args_ctr$frmla,
#   family = i04M07args_ctr$family,
#   data = data04M07,
#   quantiles = i04M07args_ctr$quantiles,
#   control.fixed = i04M07args_ctr$fixed,
#   control.family = list(
#     hyper = i04M07args_ctr$hyper
#     ),
#   control.compute = i04M07args_ctr$compute)
# saveRDS(i04M07nat, file = a_file)
```

which gives the summary for fixed parameters

```{r}
i04M07nat$summary.fixed
```

and hyperparameters which are shown on the *internal scale* which is the *log*
and this case.

```{r}
i04M07nat$summary.hyperpar
```


#### Marginals {-}

In order to compare to `brms` we need to convert the marginals as follows
* Convert internal scale (log) to natural scale
* Convert precision (`prec`) to standard deviation (`sd`)

When doing summaries later, we will need to
use the inversed transformed `internal.marginal.hyperpar` for the precision and
`marginals.hyperpar` for the other hyperparameters. The `marginals.hyperpar` for
precision is not used for summaries and plotting.

```{r}
i04M07args_ctr$marg <- transform_hyper_marginal(i04M07nat)
stopifnot(length(i04M07args_ctr$marg) == 2L)
```

then create the summary

```{r}

i04M07args_ctr$summ <- write_hyper_summary(margs = i04M07args_ctr$marg, 
                                          qtl = i04M07args_ctr$quantiles)
stopifnot(identical(dim(i04M07args_ctr$summ), c(2L, 6L)))

i04M07args_ctr$summ
```


so that a summary with all the parameters can be done

```{r}
inla_summ <- i04M07nat$summary.fixed %>%
  bind_rows(i04M07args_ctr$summ) %>%
  tibble::rownames_to_column(var = "var")
inla_summ
i04M07args_ctr$summ <- inla_summ
```

and we compare `inla` vs `brm`


```{r}
inla_summ <- i04M07args_ctr$summ %>%
  mutate(var = rename_inlavars(var)) %>%
  select("var", "mean", "sd")
inla_summ

compare_summ <- posterior_summary(b04M07ctr) %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "var") %>%
  filter(var != "lp__") %>%
  select(var, Estimate, Est.Error) %>%
  full_join(y = inla_summ[, c("var", "mean", "sd")], by = "var")
compare_summ
```


#### Posteriors {-}

See section 2.7 in @gomez2020 which explains well the posterior marginals.

We could plot the posterior as a line using `marginals.fixed` and
`marginals.hyperpar` (converted to sd) but we sample it here to 
be able to compare to `bmrs`.

So, lets get posterior samples from `inla`

```{r}
# sample the inla's posterior distributions
nsamples <- 2000
fmarg <- i04M07nat$marginals.fixed
hmarg <- i04M07args_ctr$marg

# sample the fixed parameters
fixed_samples <- purrr::map_dfc(
  .x = fmarg,
  .f = function(d) {
    sample(x = d[, "x"],
           size = nsamples,
           prob = d[, "y"],
           replace = TRUE)
  }
)
# str(fixed_samples)

# sample the hyperparameters
hyper_samples <- purrr::map_df(
  .x = hmarg,
  .f = function(d) {
    sample(x = d[, "x"],
           size = nsamples,
           prob = d[, "y"],
           replace = TRUE)
  }
)
# str(hyper_samples)

# put all posteriors together in long format
inla_post <- bind_cols(fixed_samples, hyper_samples) %>%
  setNames(rename_inlavars(names(.))) %>%
  pivot_longer(cols = everything(), names_to = ".variable",
               values_to = ".value") %>%
  mutate(fit = "inla")
stopifnot(all(dim(inla_post) == c(nsamples * 3, 3)))
# glimpse(inla_post)
```

and, as recommended section 2.7, p.37, of @gomez2020 we will compare
the posteriors to the `brm` posteriors which are obtained as follows

```{r}
get_variables(b04M07nat)
```
Get the posterior from `brm`

```{r}
brm_post <- gather_draws(b04M07ctr, b_Intercept, b_weight_c, sigma) %>%
  select(.variable, .value) %>%
  mutate(fit = "brm")
# str(brm_post)
```

and plot the posterior densities and modes

```{r}
# all data together
pdf <- rbind(inla_post, brm_post) %>%
  mutate(.variable = factor(.variable, 
                            levels = c("b_Intercept", "b_weight_c", "sigma"),
                            ordered = TRUE))
# the mode by variable and fit
pdf_mode <- pdf %>%
  group_by(.variable, fit) %>%
  summarize(mode = ggdist::Mode(.value))
# pdf_mode

ggplot(data = pdf, aes(x = .value, color = fit)) +
  geom_density(aes(y = ..scaled..)) +
  geom_vline(data = pdf_mode, aes(xintercept = mode, color = fit),
             linetype = "dashed") +
  scale_color_manual(values = c("brm" = "mediumvioletred", 
                                "inla" = "mediumseagreen")) +
  theme_ggdist() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = "bottom",
        strip.background = element_rect(fill = "moccasin")) +
  labs(title = "brm vs inla posterior densities with mode (vertical line)",
       subtitle = "4M7",
       x = NULL, y = NULL, color = NULL) + 
  facet_wrap(. ~ .variable, scales = "free")
```



#### Fitted {-}

See [brinla](http://julianfaraway.github.io/brinla/examples/chicago.html)
on how to find the fit.

The sequence of x values to fit.

```{r}
xseq <- modelr::seq_range(x = data04M07$weight_c, n = 20)
# with inla we have to redo the object with new predictor values and
# NA for outcome
data_fit <- data.frame(height = NA_real_, weight_c = xseq)
data_fit <- data04M07 %>%
  select(height, weight_c) %>%
  bind_rows(data_fit)
```


We need to add `control.predictor=list(compute=TRUE)` to compute
the posterior mean of the linear predictors

```{r}
# must use control.predictor = list(compute = TRUE)
inla_fit <- inla(
  formula = i04M07args_ctr$frmla,
  family = i04M07args_ctr$family,
  data = data_fit,
  quantiles = i04M07args_ctr$quantiles,
  control.fixed = i04M07args_ctr$fixed,
  control.family = list(
    hyper = i04M07args_ctr$hyper
    ),
  control.predictor = list(compute = TRUE))

inla_fit <- inla_fit$summary.linear.predictor %>%
  select(mean, sd, `0.025quant`, `0.975quant`) %>%
  slice_tail(n = length(xseq)) %>%
  mutate(weight_c = xseq)
str(inla_fit)
```


and we get the fit from `brms` to compare

```{r}
brm_fit <- fitted(b04M07ctr, newdata = data.frame(weight_c = xseq), 
                  probs = i04M07args_ctr$quantiles) %>%
  as.data.frame() %>%
  mutate(weight_c = xseq)
```


and we plot the results

```{r}
p1 <- ggplot(data = inla_fit, aes(x = weight_c, y = mean, ymin = `0.025quant`,
                                  ymax = `0.975quant`)) +
  geom_lineribbon(size = 1, color = "rosybrown", fill = "rosybrown1") +
  ggdist::theme_ggdist() +
  labs(title = "inla fit", y = "mean")
# p1

p2 <- ggplot(data = brm_fit, aes(x = weight_c, y = Estimate, ymin = Q2.5, 
                                 ymax = Q97.5)) +
  geom_lineribbon(size = 1, color = "deepskyblue", fill = "lightskyblue1") +
  ggdist::theme_ggdist() +
  labs(title = "brm fit", y = "mean")
# p2

p3 <- ggplot(data.frame(brm = brm_fit$Estimate, inla = inla_fit$mean), 
             aes(x = brm, y = inla)) +
  geom_point() +
  ggdist::theme_ggdist() +
  labs(title = "inla vs brm")
# p3
p1 + p2 + p3 +
  plot_annotation(title = "Comparing inla and brm fits",
                  theme = theme(title = element_text(color = "midnightblue")))
```


#### Predictions {-}


Create the data to predict.  This is the same as for fitted just above.

```{r}
xseq <- modelr::seq_range(x = data04M07$weight_c, n = 20)
# with inla we have to redo the object with new predictor values and
# NA for outcome
data_predict <- data.frame(height = NA_real_, weight_c = xseq)
data_predict <- data04M07 %>%
  select(height, weight_c) %>%
  bind_rows(data_predict)
```


We need to add `control.predictor=list(compute=TRUE)` to compute
the posterior mean of the linear predictors. This is the same as for fitted just
above.

```{r}
# must use control.predictor = list(compute = TRUE)
inla_predict <- inla(
  formula = i04M07args_ctr$frmla,
  family = i04M07args_ctr$family,
  data = data_predict,
  quantiles = i04M07args_ctr$quantiles,
  control.fixed = i04M07args_ctr$fixed,
  control.family = list(
    hyper = i04M07args_ctr$hyper
    ),
  control.predictor = list(compute = TRUE))

inla_predict <- inla_predict$summary.linear.predictor %>%
  select(mean, sd, `0.025quant`, `0.975quant`) %>%
  slice_tail(n = length(xseq)) %>%
  mutate(weight_c = xseq)
str(inla_predict)
```


and we get the predictions from `brms` to compare

```{r}
brm_predict <- predict(b04M07ctr, newdata = data.frame(weight_c = xseq), 
                  probs = i04M07args_ctr$quantiles) %>%
  as.data.frame() %>%
  mutate(weight_c = xseq)
```



```{r}
# the marginal of sigma for the new predictions
# converted to sd aon the natural scales
marg_sigma <- i04M07args_ctr$marg
marg_sigma <- marg_sigma[[grep("sd", x = names(marg_sigma))]] %>%
  as.data.frame()


# nb of draws for the fit used to simulate the predictions
# must be nb of rows to generate the predictions below.
# The objective of this step is to add the variability associated
# with the mu (fit, link)
nfits <- nrow(inla_predict)
set.seed(4)
# sample of mu values to use in sim
# mu is normally distributed
sample_mu <- rnorm(n = nfits, 
                   mean = inla_predict$mean, 
                   sd = inla_predict$sd)
# sample of sigma values to use in sim
sample_sigma <- sample(x = marg_sigma$x, 
                       size = nfits, 
                       prob = marg_sigma$x,
                       replace = TRUE)


# get the predictions by simulation
nsamples <- 1000L
df <- inla_predict %>%
  select(weight_c, mean, sd) %>%
  mutate(sigma = sample_sigma) %>%
  mutate(mu_sim = rnorm(n = nrow(.), mean = mean, sd = sd)) %>%
  expand(nesting(weight_c, mu_sim, sigma)) %>%
  mutate(height_sim = purrr::map2(.x = mu_sim, .y = sigma, 
                             .f = ~ rnorm(n = nsamples, mean = .x, sd = .y))) %>%
  unnest(cols = c(weight_c, height_sim))

# plot the predictions
p <- ggplot(df, aes(x = weight_c)) +
  stat_lineribbon(aes(y = height_sim), .width = c(0.50, 0.75, 0.95),
                  color = "steelblue4") +
  geom_point(data = data04M07, mapping = aes(x = weight_c, y = height),
             inherit.aes = FALSE, color = "indianred") +
  scale_fill_paletteer_d("ggsci::teal_material") +
  ggdist::theme_ggdist() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.15, 0.8)) +
  labs(title = "Predictions with INLA", subtitle = "4M7",
       x = "weight", y = "height")
p
```


## 4M8 {-#prac4M8}

The methodology used here comes from section 4.5 of @kurtz2020b to whom
I am forever grateful for the wonderful books he gives us.


We remove the rows with `NA` in the `doy` variable.

```{r}
data("cherry_blossoms")
data04M08 <- cherry_blossoms %>%
  drop_na(doy)
rm(cherry_blossoms)
stopifnot(identical(dim(data04M08), c(827L, 5L)))
skimr::skim(data04M08)
```

### Model {-}

$$
\begin{align*}
doy_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \sum_{k=1}^Kw_kB_{k, i} \\
\alpha &\sim \mathcal{N}(100, 10) \\
w_j &\sim \mathcal{N}(0, 10) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$
### a) Knots = 15, $w_j \sim \mathcal{N}(0, 10)$


```{r}
nknots <- 15
knots <- quantile(data04M08$year, probs = seq(from = 0, to = 1, length.out = nknots))
knots
```


```{r}
colr <- unclass(paletteer::paletteer_d("futurevisions::cancri"))
ggplot(data04M08, aes(x = year, y = doy, color = temp)) +
  geom_vline(xintercept = knots, color = "slateblue", alpha = 1/2) +
  geom_point(shape = 20, size = 2, alpha = 2/3) +
  scale_x_continuous(breaks = knots, labels = knots) +
  scale_color_gradientn(colors = colr) +
  theme_classic() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.05, 0.8),
        axis.text.x = element_text(size = rel(0.9))
        ) +
  labs(title = sprintf("Cherry Blossom in Japan with %d knots", nknots),
       subtitle = "4M8 a)")
```
Create the bias function with degree 3 (cubic spline) and an intercept

```{r}
library(splines)
# must specify intercept = TRUE
B <- splines::bs(x = data04M08$year, knots = knots[-c(1, nknots)], 
                 degree = 3, intercept = TRUE)
# str(B)
# this data.frame will be reused below with the posteriors
df_bias <- B %>%
  as.data.frame() %>%
  setNames(sprintf("B%02d", seq_len(ncol(.)))) %>%
  mutate(year = data04M08$year) %>%
  pivot_longer(cols = -year, names_to = "bias_func", values_to = "bias")
# str(df_bias)
```


then the data structure used to fit

```{r}
dfB <- data04M08 %>%
  mutate(B = B)
# the last column is a matrix column, with same nb of rows as the other
# columns but with a column including 17 subcolumns (!)
# glimpse(dfB)
```



```{r}
a_file <- here::here("fits", "b04M08a.rds")
b04M08a <- readRDS(file = a_file)
# b04M08a <- brm(data = dfB,
#       family = gaussian,
#       doy ~ 1 + B,
#       prior = c(prior(normal(100, 10), class = Intercept),
#                 prior(normal(0, 10), class = b),
#                 prior(exponential(1), class = sigma)),
#       cores = detectCores(), seed = 4)
# saveRDS(b04M08a, file = a_file)
```
```{r}
summary(b04M08a)
```




```{r}
df <- fitted(b04M08a) %>%
  as.data.frame() %>%
  bind_cols(data04M08)
str(df)
clrs <- unclass(paletteer::paletteer_d("futurevisions::cancri"))
pA <- ggplot(df, aes(x = year, y = doy)) +
  geom_vline(xintercept = knots[-c(1, length(knots))], color = "slateblue", alpha = 1/2) +
  geom_point(aes(color = temp)) +
  geom_lineribbon(aes(x = year, y = Estimate, ymin = Q2.5, ymax = Q97.5),
                  color = "blueviolet", fill = "cornflowerblue", alpha = 1/2) +
  scale_x_continuous(breaks = knots, labels = knots) +
  scale_color_gradientn(colors = clrs) +
  ggthemes::theme_tufte() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = "none",
        axis.text.x = element_text(size = rel(0.9))) +
  labs(title = sprintf("Cherry Blossom in Japan with %d knots", nknots),
       subtitle = "4M8 a)")
pA
```

### b) Knots = 25, $w_j \sim \mathcal{N}(0, 10)$ {-}


```{r}
nknots <- 25
knots <- quantile(data04M08$year, probs = seq(from = 0, to = 1, length.out = nknots))
knots
```


Create the bias functions with degree 3 (cubic spline) and an intercept

```{r}
library(splines)
# must specify intercept = TRUE
B <- splines::bs(x = data04M08$year, knots = knots[-c(1, nknots)], 
                 degree = 3, intercept = TRUE)
# str(B)
# this data.frame will be reused below with the posteriors
df_bias <- B %>%
  as.data.frame() %>%
  setNames(sprintf("B%02d", seq_len(ncol(.)))) %>%
  mutate(year = data04M08$year) %>%
  pivot_longer(cols = -year, names_to = "bias_func", values_to = "bias")
# str(df_bias)
```


then the data structure used to fit

```{r}
dfB <- data04M08 %>%
  mutate(B = B)
# the last column is a matrix column, with same nb of rows as the other
# columns but with a column including 17 subcolumns (!)
# glimpse(dfB)
```



```{r}
a_file <- here::here("fits", "b04M08b.rds")
b04M08b <- readRDS(file = a_file)
# b04M08b <- brm(data = dfB,
#       family = gaussian,
#       doy ~ 1 + B,
#       prior = c(prior(normal(100, 10), class = Intercept),
#                 prior(normal(0, 10), class = b),
#                 prior(exponential(1), class = sigma)),
#       cores = detectCores(), seed = 4)
# saveRDS(b04M08b, file = a_file)
```




```{r}
df <- fitted(b04M08b) %>%
  as.data.frame() %>%
  bind_cols(data04M08)
str(df)
clrs <- unclass(paletteer::paletteer_d("futurevisions::cancri"))
pB <- ggplot(df, aes(x = year, y = doy)) +
  geom_vline(xintercept = knots[-c(1, length(knots))], color = "slateblue", alpha = 1/2) +
  geom_point(aes(color = temp)) +
  geom_lineribbon(aes(x = year, y = Estimate, ymin = Q2.5, ymax = Q97.5),
                  color = "blueviolet", fill = "cornflowerblue", alpha = 1/2) +
  scale_color_gradientn(colors = clrs) +
  ggthemes::theme_tufte() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = "none",
        axis.text.x = element_text(size = rel(0.9))) +
  labs(title = sprintf("Cherry Blossom in Japan with %d knots", nknots),
       subtitle = "4M8 b)")
pB
```

### c) Knots = 25, $w_j \sim \mathcal{N}(0, 20)$ {-}


```{r}
nknots <- 25
knots <- quantile(data04M08$year, probs = seq(from = 0, to = 1, length.out = nknots))
knots
```


Create the bias functions with degree 3 (cubic spline) and an intercept

```{r}
library(splines)
# must specify intercept = TRUE
B <- splines::bs(x = data04M08$year, knots = knots[-c(1, nknots)], 
                 degree = 3, intercept = TRUE)
# str(B)
# this data.frame will be reused below with the posteriors
df_bias <- B %>%
  as.data.frame() %>%
  setNames(sprintf("B%02d", seq_len(ncol(.)))) %>%
  mutate(year = data04M08$year) %>%
  pivot_longer(cols = -year, names_to = "bias_func", values_to = "bias")
# str(df_bias)
```


then the data structure used to fit

```{r}
dfB <- data04M08 %>%
  mutate(B = B)
# the last column is a matrix column, with same nb of rows as the other
# columns but with a column including 17 subcolumns (!)
# glimpse(dfB)
```



```{r}
a_file <- here::here("fits", "b04M08c.rds")
b04M08c <- readRDS(file = a_file)
# b04M08c <- brm(data = dfB,
#       family = gaussian,
#       doy ~ 1 + B,
#       prior = c(prior(normal(100, 10), class = Intercept),
#                 prior(normal(0, 20), class = b),
#                 prior(exponential(1), class = sigma)),
#       cores = detectCores(), seed = 4)
# saveRDS(b04M08c, file = a_file)
```




```{r}
df <- fitted(b04M08c) %>%
  as.data.frame() %>%
  bind_cols(data04M08)
str(df)
clrs <- unclass(paletteer::paletteer_d("futurevisions::cancri"))
pC <- ggplot(df, aes(x = year, y = doy)) +
  geom_vline(xintercept = knots[-c(1, length(knots))], color = "slateblue", alpha = 1/2) +
  geom_point(aes(color = temp)) +
  geom_lineribbon(aes(x = year, y = Estimate, ymin = Q2.5, ymax = Q97.5),
                  color = "blueviolet", fill = "cornflowerblue", alpha = 1/2) +
  scale_color_gradientn(colors = clrs) +
  ggthemes::theme_tufte() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = "none",
        axis.text.x = element_text(size = rel(0.9))) +
  labs(title = sprintf("Cherry Blossom in Japan with %d knots and increased weight sd to 20", nknots),
       subtitle = "4M8 c)")
# pC
```

### Conclusion {-}

#### Plots {-}

The increase of nb of knots increases the fits (i.e. nb of turns)

```{r}
pA / pB
```

and the increase in variability of the weight increase the range of the coefficient.
However this is not visually obvious when looking at the scatter plot.
See just below the coefficient comparisons which is more informative.

```{r}
pB / pC
```


#### Summaries {-}

```{r}
summA <- data.frame(model = "A", fixef(b04M08a)) %>%
  tibble::rownames_to_column(var = "variable")
summB <- data.frame(model = "B", fixef(b04M08b)) %>%
  tibble::rownames_to_column(var = "variable")
summC <- data.frame(model = "C", fixef(b04M08c)) %>%
  tibble::rownames_to_column(var = "variable")
df <- bind_rows(summA, summB, summC) %>%
  mutate(variable = factor(variable,
                           levels = c("Intercept", sprintf("B%d", 1:27)),
                           ordered = TRUE))

ggplot(df[df$variable != "Intercept", ], aes(x = variable, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = model)) +
  geom_pointinterval(position = position_dodge(width = 1/2)) +
  scale_color_paletteer_d("futurevisions::cancri") +
  ggthemes::theme_hc() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = "bottom",
        legend.direction = "horizontal",
        axis.text.x = element_text(size = rel(0.85))) +
  labs(title = "Comparing the models' coefficients (excludding intercept)",
       subtitle = "4M8",
       x = NULL, y = NULL)
  # coord_flip()
```

We can see that the model are similar expect that

* Model A has significantly lower B16 and B17 coefficients. They are
nonetheless similar to the B26 and B27 coefficients of models B and C
* Model B and C have the same coefficient but model C which is the model
with the increased prior variance for the weights has a wider confidence range
for all coefficients

## 4H1 {-#prac4H1}

We first create the dataframes in many of the following practices.

```{r}
data("Howell1")
data04H01 <- Howell1 %>%
  filter(age >= 18) %>%
  mutate(weight_c = scale(weight, center = TRUE, scale = FALSE))
rm("Howell1")
stopifnot(identical(dim(data04H01), c(352L, 5L)))
```

### Model {-}

and the model that will be used

$$
\begin{align*}
height_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta \cdot weight_i \\
\alpha &\sim \mathcal{N}(178, 20) \\
\beta &\sim \mathcal{LogNormal}(1, 0.5) \\
\sigma &\sim \mathcal{Exponential}(1)
\end{align*}
$$

and get the fit with `quap`


```{r}
a_file <- here::here("fits", "m04H01.rds")
m04H01 <- readRDS(file = a_file)
# m04H01 <- quap(
#   flist = alist(
#     height ~ dnorm(mu, sigma),
#     mu <- a + b * weight,
#     a ~ dnorm(178, 20),
#     b ~ dlnorm(1, 0.5),
#     sigma ~ dexp(1)
#   ),
#   data = data04H01,
#   start = list(a = mean(d$height), b = 0.5)
# )
saveRDS(object = m04H01, file = a_file)
```

```{r}
vcov(m04H01)
# matrixcalc::is.positive.definite(vcov(m04H01), tol = 1e-4)
```


get the posterior

```{r}
m04H01_post <- extract.samples(m04H01)
m04H01_pred <- rnorm(n = nrow(m04H01_post), 
                   mean = m04H01_post$a + m04H01_post$b * 46.95,
                   sd = m04H01_post$sigma)
ggdist::mean_hdi(m04H01_pred, .width = 0.89)
```

### Using `rethinking::link()` {-}

and find the predictions using the detailed method as described
in overthinking box of section 4.4.3.4. The `rethinking::link()` function
does that.

```{r}
the_weights <- c(46.95, 43.72, 64.78, 32.59, 54.63)
nsamples <- 1000

set.seed(4)
preds <- sapply(X = the_weights, FUN = function(x) {
  rnorm(n = nsamples, 
        mean = m04H01_post$a + m04H01_post$b * x,
        sd = m04H01_post$sigma)
  })
preds <- apply(X = preds, MARGIN = 2, FUN = ggdist::mean_hdi, .width = 0.89)
preds <- do.call(rbind, preds)
preds <- preds %>%
  mutate(individual = seq_along(the_weights)) %>%
  rename(height = y) %>%
  relocate(individual)
preds
```

### Using `tidyverse` {-}

The tidyverse way will be used from hereon.

```{r}
obs <- data.frame("individual" = seq_along(the_weights), "weight" = the_weights)
set.seed(4)
preds <- obs %>%
  expand(nesting(individual, weight)) %>%
  mutate(height = purrr::map(.x = weight, 
                            .f = ~ rnorm(
                              n = nsamples, 
                              mean = m04H01_post$a + m04H01_post$b * .x, 
                              sd = m04H01_post$sigma)
                            )
         ) %>%
  unnest(cols = c(height)) %>%
  group_by(individual) %>%
  ggdist::mean_hdi(height, .width = 0.89)
preds
```


## 4H2 {-#prac4H2}

Load the data

```{r}
data("Howell1")
data04H02 <- Howell1 %>%
  filter(age < 18) %>%
  mutate(weight_c = scale(weight, center = TRUE, scale = FALSE))
rm("Howell1")
# there should be 192 rows
stopifnot(identical(dim(data04H02), c(192L, 5L)))
skimr::skim(data04H02)
```


### Model {-}

$$
\begin{align*}
height_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta \cdot weight_i \\
\alpha &\sim \mathcal{N}(80, 40) \\
\beta &\sim \mathcal{LogNormal}(1, 0.5) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$

### 4H2 a) with `quap` {-}

* Note on priors
  - Using sigma ~ dexp(rate = 1) which seems to work well with this model
  - a ~ dnorm(80, 40) is based on the average height of the kids
  - b ~ dlnorm(1, 0.5) since we assume the growth rate is positive

* Start values
  - using start data helps very much for this model converge consistently
  - for $a$ simply use the average height
  - for $b$ we use 1 as it should be strictly positive and assuming kids 
  grow faster than adults.


```{r}
a_file <- here::here("fits", "m04H02.rds")
m04H02 <- readRDS(file = a_file)
# m04H02 <- quap(
#     flist = alist(
#         height ~ dnorm(mu, sigma),
#         mu <- a + b * weight,
#         a ~ dnorm(80, 40),
#         b ~ dlnorm(1, 0.5),
#         sigma ~ dexp(1)
#         ),
#     data = data04H02,
#     start = list(a = mean(data04H01$height), b = 1)
# )
# saveRDS(m04H02, file = a_file)
m04H02_summ <- rethinking::precis(m04H02, prob = 0.89)
m04H02_summ
```

for 10 more units of weights the child should be taller between 26 and 28 cm.

### 4H2 b) with `quap` {-}

#### Get the fitted values with `quap` {-}

See section 4.4.3.4 for more details.

```{r}
weight_seq <- modelr::seq_range(data04H02$weight, n = 30)
weight_seq <- data.frame(weight = weight_seq)
```

```{r}
precis(m04H02)
# m04H02_post <- extract.samples(m04H02)
```

See the overthinking box in section 4.4.3.4 to explain `rethinking::link()`

```{r}
m04H02_fitted <- rethinking::link(fit = m04H02, 
                                 data = weight_seq, 
                                 n = 1000)
# str(m04H02_fitted)
m04H02_fitted_inrvl <- apply(X = m04H02_fitted, MARGIN = 2, FUN = function(x) {
  c("mean" = mean(x), rethinking::HPDI(x))
}) %>%
  t() %>%
  bind_cols(weight = weight_seq) %>%
  as.data.frame()
# m04H02_fitted_inrvl
```
and the prediction intervals are obtained as described in section 4.4.3.5
using `rethinking::sim()`

```{r}
m04H02_predict <- rethinking::sim(fit = m04H02,
                                 data = data.frame(weight = weight_seq),
                                 n = 1000)
m04H02_predict_inrvl <- apply(X = m04H02_predict, MARGIN = 2, FUN = function(x) {
  c("mean" = mean(x), rethinking::HPDI(x))
}) %>% 
  t() %>%
  bind_cols(weight = weight_seq) %>%
  as.data.frame()
# m04H02_predict_inrvl
```




```{r}
ggplot(data = data04H01, aes(x = weight)) +
  geom_ribbon(data = m04H02_predict_inrvl,
              aes(ymin = `|0.89`, ymax = `0.89|`),
              fill = "lightcyan") +
  geom_smooth(data = m04H02_fitted_inrvl,
              aes(y = mean, ymin = `|0.89`, ymax = `0.89|`),
              stat = "identity",
              fill = "lightcyan3", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = height, color = age), shape = 20, size = 2, alpha = 2/3) +
  scale_x_continuous( breaks = scales::breaks_extended(n = 7)) +
  scale_color_paletteer_c("pals::kovesi.linear_kryw_5_100_c67") +
  theme_minimal() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.1, 0.8)) +
  labs(title = "quap fit - Practice 4H2", x = "weight", y = "height")
```


### 4H2 a) with `brm` {-}

Same comments and conclusion as for `quap` above

```{r}
a_file <- here::here("fits", "b04H02.rds")  # save to rds file
b04H02 <- readRDS(file = a_file)
# b04H02 <- brms::brm(data = data04H02,
#                          formula = height ~ 1 + weight,
#                          family = gaussian(),
#                          prior = c(
#                            prior(normal(100, 50), class = Intercept),
#                            prior(lognormal(0, 2), class = b, lb = 0),
#                            prior(cauchy(0, 1), class = sigma)),
#                          iter = 2000, warmup = 1000, chains = 4,
#                          cores = detectCores(), seed = 4)
# saveRDS(b04H02, file = a_file)
b04H02_fixf <- brms::fixef(b04H02)
b04H02_fixf
```

### 4H2 b) with `brm` {-}


```{r}
weight_seq <- modelr::seq_range(data04H02$weight, n = 30)
weight_seq <- data.frame(weight = weight_seq)
```


```{r}
b04H02_fitted <-
  fitted(b04H02, newdata = weight_seq, probs = c(0.055, 0.945)) %>%
  data.frame() %>%
  bind_cols(weight_seq)
# glimpse(b04H02_fitted)

b04H02_predict <-
  predict(b04H02, newdata = weight_seq, probs = c(0.055, 0.945)) %>%
  data.frame() %>%
  bind_cols(weight_seq)
# glimpse(b04H02_predict)
```


and we illustrate the results


```{r}
ggplot(data = data04H02, aes(x = weight)) +
  geom_ribbon(data = b04H02_predict,
              aes(ymin = Q5.5, ymax = Q94.5),
              fill = "lightcyan") +
  geom_smooth(data = b04H02_fitted,
              aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),
              stat = "identity",
              fill = "lightcyan3", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = height, color = age), shape = 20, size = 2, alpha = 2/3) +
  scale_x_continuous( breaks = scales::breaks_extended(n = 7)) +
  scale_color_paletteer_c("pals::kovesi.linear_kryw_5_100_c67") +
  theme_minimal() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.1, 0.8)) +
  labs(title = "BRMS fit - Practice 4H2", x = "weight", y = "height")
```

### 4H2 a) with `inla` {-}


```{r}
a_file <- here::here("fits", "i04H02nat.rds")
# i04H02 <- readRDS(file = a_file)
i04H02 <- inla(
  formula = height ~ weight,
  family = "gaussian",
  data = data04H02,
  control.fixed=list(mean.intercept = 80, prec.intercept = 1 / 40^2, prec=0.001),
  control.compute = list(config=TRUE, dic = TRUE, waic = TRUE))
saveRDS(i04H02, file = a_file)
```


```{r}
# the fixed coefficients
i04H02$summary.fixed[c("(Intercept)", "weight"), c("mean", "sd")] %>%
  bind_cols(as.data.frame(fixef(b04M07nat)[, c("Estimate", "Est.Error")])) %>%
  round(digits = 2)
```



### 4H2 c) {-}

The data points seem to have a nonlinear relation with weight, visually, it seems, maybe,
that a quadratice might be a better fit.


## 4H3 {-#prac4H3}

This is covered by section 4.5.1 polynomial regression but instead of polynomial
we use a log equation.

The data is

```{r}
data("Howell1")
data04H03 <- Howell1 %>%
  mutate(weight_c = scale(weight, center = TRUE, scale = FALSE))
rm("Howell1")
stopifnot(identical(dim(data04H03), c(544L, 5L)))
skimr::skim(data04H03)
```


The model used is

$$
\begin{align*}
height_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \log{(\beta)} \cdot weight_i \\
\alpha &\sim \mathcal{N}(178, 20) \\
\beta &\sim \mathcal{N}(0, 10) \\
\sigma &\sim \mathcal{Uniform}(0, 50)
\end{align*}
$$

### 4H3 a) using `quap` {-}

> Important: We have to use `dunif` for sigma to make the `quap` converge. Otherwise
the cov matrix is not positive definite.

```{r}
a_file <- here::here("fits", "m04H03.rds")  # save to rds file
m04H03 <- readRDS(file = a_file)
# m04H03 <- rethinking::quap(
#     flist = alist(
#         height ~ dnorm(mu, sigma),
#         mu <- a + b * log(weight),
#         a ~ dnorm(178, 20),
#         b ~ dnorm(0, 10),
#         sigma ~ dunif(0, 50)
#         ),
#     data = data04H03
# )
# saveRDS(m04H03, file = a_file)
precis(m04H03, prob = 0.89)
```

Since we use $\log{weight}$ than any change of $\log{weight}$ represents
a percentage change of $weight$, therefore $b$ represents that, for every
percentage increase of the weight, the height is reduced by -2.65 of a percentage.


### 4H3 b) using `quap` {-}

We use the same process as in 4H2 just above

```{r}
weight_seq <- data.frame(
  weight = modelr::seq_range(data04H03$weight, n = 30)
  )
```

same process as in 4H2 above

```{r}
m04H03_fitted <- rethinking::link(fit = m04H03, 
                                 data = weight_seq, 
                                 n = 1000)
# str(m04H03_fitted)
m04H03_fitted_inrvl <- apply(X = m04H03_fitted, MARGIN = 2, FUN = function(x) {
  c("mean" = mean(x), rethinking::HPDI(x, prob = 0.97))
}) %>%
  t() %>%
  bind_cols(weight = weight_seq) %>%
  as.data.frame()
# m04H03_fitted_inrvl
```
and the prediction intervals are obtained as described in section 4.4.3.5
using `rethinking::sim()` (sames as in 4H2 above)

```{r}
m04H03_predict <- rethinking::sim(fit = m04H03,
                                 data = data.frame(weight = weight_seq),
                                 n = 1000)
m04H03_predict_inrvl <- apply(X = m04H03_predict, MARGIN = 2, FUN = function(x) {
  c("mean" = mean(x), rethinking::HPDI(x, prob = 0.97))
}) %>% 
  t() %>%
  bind_cols(weight = weight_seq) %>%
  as.data.frame()
# m04H03_predict_inrvl
```




```{r}
ggplot(data = data04H03, aes(x = weight)) +
  geom_ribbon(data = m04H03_predict_inrvl,
              aes(ymin = `|0.97`, ymax = `0.97|`),
              fill = "aquamarine1") +
  geom_smooth(data = m04H03_fitted_inrvl,
              aes(y = mean, ymin = `|0.97`, ymax = `0.97|`),
              stat = "identity",
              fill = "aquamarine4", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = height, color = age), shape = 20, size = 2, alpha = 2/3) +
  scale_x_continuous( breaks = scales::breaks_extended(n = 7)) +
  scale_color_paletteer_c("pals::kovesi.linear_kry_5_98_c75") +
  theme_minimal() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.1, 0.8)) +
  labs(title = "quap fit - Practice 4H3", x = "weight", y = "height")
```


### 4H3 a) using `brm` {-}


Same comments and conclusion as for `quap` above. The results are very similar.

```{r}
a_file <- here::here("fits", "b04H03.rds")  # save to rds file
b04H03 <- readRDS(file = a_file)
# b04H03 <- brms::brm(data = data04H03,
#                    formula = height ~ 1 + log(weight),
#                    family = gaussian,
#                    prior =
#                      c(prior(normal(178, 20), class = Intercept),
#                        prior(normal(0, 10), class = b),
#                        prior(cauchy(0, 1), class = sigma)),
#                    iter = 2000, warmup = 1000, chains = 4,
#                    cores = detectCores(), seed = 4)
# saveRDS(b04H03, file = a_file)
b04H03_fixf <- brms::fixef(b04H03)
b04H03_fixf
```

prior(cauchy(0, 1), class = sigma)

### 4H3 b) using `brm` {-}


```{r}
weight_seq <- data.frame(
  weight = modelr::seq_range(data04H03$weight, n = 30)
  )
```


```{r}
b04H03_fitted <-
  fitted(b04H03, newdata = weight_seq, probs = c(0.015, 0.985)) %>%
  data.frame() %>%
  bind_cols(weight_seq)
# glimpse(b04H03_fitted)

b04H03_predict <-
  predict(b04H03, newdata = weight_seq, probs = c(0.015, 0.985)) %>%
  data.frame() %>%
  bind_cols(weight_seq)
# glimpse(b04H03_predict)
```


and we illustrate the results


```{r}
ggplot(data = data04H03, aes(x = weight)) +
  geom_ribbon(data = b04H03_predict,
              aes(ymin = Q1.5, ymax = Q98.5),
              fill = "aquamarine1") +
  geom_smooth(data = b04H03_fitted,
              aes(y = Estimate, ymin = Q1.5, ymax = Q98.5),
              stat = "identity",
              fill = "aquamarine4", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = height, color = age), shape = 20, size = 2, alpha = 2/3) +
  scale_x_continuous( breaks = scales::breaks_extended(n = 7)) +
  scale_color_paletteer_c("pals::kovesi.linear_kry_5_98_c75") +
  theme_minimal() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.1, 0.8)) +
  labs(title = "BRMS fit - Practice 4H3", x = "weight", y = "height")
```

## 4H4 {-#prac4H4}

See section 4.5.1  for reference to this practice. R code 4.65 (p. 111)

The data is as in section 4.5.1.  This practice is using techniques that
are illustrated at the beginning of section 5.1 in the next chapter.

```{r}
data("Howell1")
data04H04 <- Howell1 %>%
  mutate(weight_c = scale(weight, center = TRUE, scale = FALSE),
         weight_c2 = weight_c ^ 2)
rm("Howell1")
stopifnot(identical(dim(data04H04), c(544L, 6L)))
skimr::skim(data04H04)
```


the model has been slightly modified by using the centered weight instead of
the standard weight.  It seems to work better with `quap`.

The values for $a$ are from the summary using `skimr` just above.


$$
\begin{align*}
h_i &\sim \mathcal{N}(\mu_i, \sigma)\\
\mu_i &= \alpha + \beta_1 \cdot weight\_c_i + \beta_2 \cdot weight\_c^2_i \\
\alpha &\sim \mathcal{N}(138, 50) \\
\beta_1 &\sim \mathcal{LogNormal}(0,1) \\
\beta_2 &\sim \mathcal{N}(0,1) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$


### 4H4 using `quap` {-}


```{r}
a_file <- here::here("fits", "m04H04.rds")
m04H04 <- readRDS(file = a_file)
# m04H04 <- quap(
#   flist = alist(
#     height ~ dnorm(mu, sigma),
#     mu <- a + b1 * weight_c + b2 * weight_c2,
#     a ~ dnorm(138, 50),
#     b1 ~ dlnorm(0, 1),
#     b2 ~ dnorm(0, 1),
#     sigma ~ exp(1)
#   ),
#   data = data04H04
# )
# saveRDS(m04H04, file = a_file)
rethinking::precis(m04H04)
```
The function `rethinking::extract.prior()` is used to extract the sample distributions
from the fit.  Then the process of finding the mus is the same as the overthinking box
of section 4.4.3.4 (p.107).  Or in other words, using the `link` function with
the prior used in the `post` argument
.

```{r}
m04H04_prior <- extract.prior(m04H04, n = 1000) %>%
  as.data.frame() %>%
  mutate(sigma = rexp(n = nrow(.), rate = 1))
# str(m04H04_prior)
```

and find the predictions using the detailed method as described
in overthinking box of section 4.4.3.4. The `rethinking::link()` function
does that. For an example, see at the beginning of section 5.1 in the next chapter, 
R code 5.4.

For this exercise, we will do it the long way.

```{r}
weight_seq <- data.frame(
  weight_c = modelr::seq_range(data04H04$weight_c, n = 30)
  )
```


```{r}
nsamples <- 500
set.seed(4)
prior_preds <- sapply(X = weight_seq$weight_c, FUN = function(x) {
  rnorm(n = nsamples, 
        mean = m04H04_prior$a + m04H04_prior$b1 * x + m04H04_prior$b2 * x^2,
        sd = m04H04_prior$sigma)
  })
prior_preds <- apply(X = prior_preds, MARGIN = 2, FUN = ggdist::mean_hdi, .width = 0.89)
prior_preds <- do.call(rbind, prior_preds)
prior_preds <- prior_preds %>%
  mutate(weight_c = weight_seq$weight_c) %>%
  relocate(weight_c)
# str(prior_preds)
```



```{r}
ggplot(data = data04H04, aes(x = weight_c)) +
  geom_line(data = prior_preds, aes(y = y), size = 1, color = "purple") +
  geom_point(aes(y = height, color = age), shape = 20, size = 2, alpha = 2/3) +
  scale_x_continuous( breaks = scales::breaks_extended(n = 7)) +
  scale_color_paletteer_c("pals::kovesi.rainbow_bgyrm_35_85_c71") +
  theme_minimal() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.85, 0.30)) +
  labs(title = "quap fit with PRIOR - Practice 4H4", x = "weight", y = "height")
```

which shows that the prior is not so bad, it's shape aligns with the data, only
the intercept actually need to be modified.

> Conclusion: It's a very good idea to simulate the priors.  It tells us
if they make sense.  It helps greatly in having a converging fit.


### 4H4 using `brm` {-}

**Note the use of `sample_prior = TRUE`** to be able to obtain the prior
samples.  We use the prior $b1 \sim \mathcal{N}(0,1)$ instead of $\mathcal{LogNormal}$
which gives a much better prior in this case.

```{r}
a_file <- here::here("fits", "b04H04.rds")
b04H04 <- readRDS(file = a_file)
# b04H04 <- brms::brm(data = data04H04,
#                    formula = height ~ 1 + weight_c + weight_c2,
#                    family = gaussian,
#                    prior =
#                      c(prior(normal(138, 50), class = Intercept),
#                        prior(normal(0, 1), class = b, coef = "weight_c"),
#                        prior(normal(0, 1), class = b, coef = "weight_c2"),
#                        prior(cauchy(0, 1), class = sigma)),
#                    iter = 2000, warmup = 1000, chains = 4,
#                    sample_prior = TRUE,
#                    cores = detectCores(), seed = 4)
# saveRDS(b04H04, file = a_file)
brms::fixef(b04H04)
```

create the sequence of weights to use

```{r}
weight_seq <- data.frame(
  weight_c = modelr::seq_range(data04H04$weight_c, n = 30)
  ) %>%
  mutate(weight_c2 = weight_c^2)
# glimpse(weight_seq)
```

sample the prior and create the fit

```{r}
b04H04_prior <- brms::prior_samples(b04H04)
b04H04_prior_fit <- b04H04_prior %>%
  slice_sample(n = 100) %>%
  tibble::rownames_to_column("draw") %>%
  expand(nesting(draw, Intercept, b_weight_c, b_weight_c2),
                 x = weight_seq$weight_c, x2 = weight_seq$weight_c2) %>%
  mutate(height_fit = Intercept + b_weight_c * x + b_weight_c2 * x2)
glimpse(b04H04_prior_fit)
b04H04_prior_fit_interval <- b04H04_prior_fit %>%
  group_by(x) %>%
  do(ggdist::mean_hdi(.$height_fit, .width = 0.89)) %>%
  rename("mu" = x, "height" = y)
# glimpse(b04H04_prior_fit_interval)
```


get the posterior fit($\mu$) and prediction  ($\hat{y}$)

```{r}
b04H04_fitted <-
  fitted(b04H04, newdata = weight_seq, probs = c(0.055, 0.945)) %>%
  data.frame() %>%
  bind_cols(weight_seq)
# glimpse(b04H04_fitted)

b04H04_predict <-
  predict(b04H04, newdata = weight_seq, probs = c(0.055, 0.945)) %>%
  data.frame() %>%
  bind_cols(weight_seq)
# glimpse(b04H04_predict)
```


visualize the prior and posterior

```{r}
ggplot(data04H04, aes(x = weight_c)) +
  geom_ribbon(data = b04H04_predict,
              aes(ymin = Q5.5, ymax = Q94.5),
              fill = "slategray1") +
  geom_smooth(data = b04H04_fitted,
              aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),
              stat = "identity",
              fill = "slategray4", color = "black", alpha = 2/3, size = 1) +
  geom_line(data = prior_preds, aes(x = weight_c, y = y), inherit.aes = FALSE, 
            size = 1, linetype = "dashed", color = "purple") +
  geom_point(aes(y = height, color = age), shape = 20, size = 2, alpha = 2/3) +
    theme_minimal() +
  scale_x_continuous(breaks = scales::breaks_extended(n = 7),
                     labels = function(x) round(x + mean(data04H04$weight), 0)) +
  scale_color_paletteer_c("pals::kovesi.rainbow_bgyrm_35_85_c71") +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.80, 0.30)) +
  labs(title = "BRMS fit with PRIOR - Practice 4H4", x = "weight", y = "height")
```

## 4H5 {-#prac4H5}

```{r echo=FALSE}
message("TODO")
```

## 4H6 {-#prac4H6}

```{r echo=FALSE}
message("TODO")
```


## 4H7 {-#prac4H7}

```{r echo=FALSE}
message("Practice 4H7 is missing in the second edition!")
```


## 4H8 {-#prac4H8}

```{r echo=FALSE}
message("TODO")
```
