```{r include=FALSE}
library(rethinking)
library(INLA)
library(brms)
library(eflINLA)
library(eflRethinking)
library(eflStats)
library(eflINLA)
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(simstudy)
library(modelr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(paletteer, quietly = TRUE)
library(patchwork, quietly = TRUE)
```



# `INLA` {-#app-inla}

This appendix gives example on how to use `inla` objects. The data is simulated
then the results are verified using the parameters of the siumlation.


## Note on priors {-}

One different aspect of `INLA::inla` from `rethinking::quap` and `brms::brm` is
its treatment of **priors**.  Contrary to `quap` and `brm` the priors in `inla`
are scattered among 3 different parameters which can be confusing at first.

The priors for `inla` are organized as follows

* **Likelihood**: The priors are defined using the parameter `hyper` inside 
`control.family`. See section 5.2 of @gomez2020.
  - The default prior on precision is the Gamma(shape = 1, rate = 0.00005). 
  Internally, it is defined on the log(precision) and is therefore 
  LogGamma(shape = 1, rate = 0.00005) See p. 19 and p. 23 of @gomez2020 and section 3.2, p.44, o
  wang2018.
* **Fixed effect**: The prior are defined in `control.fixed`. See section 2.3.1, 
in particular table 2.1, of @gomez2020.
* **Latent effect**: The difference between latent and hyperparameters in INLA is that
latent Gaussian variables describe linear predictor.  Hyperparameters are all
parameters (very few or none) not arising linearly in the predictors.
The priors are in defined using the parameter `hyper` inside the
`f()` function. See section 5.2 of @gomez2020.

The generalized linear models are handled using parameters values that can be
listed with `?inla.models`

```{r}
names(inla.models())
names(inla.models())["prior"]
```

and the information on priors can be found in `inla.models()$prior`

```{r}
names(inla.models()$prior)
```

The prior parametrization will therefore be detailed in details with examples 
below.

## Linear regression

### Data

See [simstudy correlated data](https://kgoldfeld.github.io/simstudy/articles/correlated.html)
for details on how to create correlated data.

However, if one wants to create a set of correlated data **with a specific**
correlation structure **and use the correlated** data in a definition this is
impossible as the data set created with genCorData is independent of a 
`defData` and cannot be used with additional definition with `defData` or 
`addColumns`, etc.

One way to do is as follows


```{r}
sim <- list()
sim$defs <- defData(varname = "x0", dist = "normal", 
                   formula = 5, variance = 1^2)
sim$defs <- defData(sim$defs, varname = "x1", dist = "nonrandom", 
                   formula = "..x1")
sim$defs <- defData(sim$defs, varname = "x2", dist = "nonrandom", 
                   formula = "..x2")
sim$defs <- defData(sim$defs, varname = "x3", dist = "nonrandom", 
                   formula = "..x3")
sim$defs <- defData(sim$defs, varname = "mu", dist = "nonrandom",
                   formula = "x0 + x1 + x2 + exp(x3)")
sim$defs <- defData(sim$defs, varname = "sigma", dist = "exponential", 
                    formula = 1)
sim$defs <- defData(sim$defs, varname = "y", dist = "normal", 
                   formula = "mu", variance = "sigma^2")

# correlation matrix
sim$Rho <- genCorMat(nvars = 3, 
                     cors = c("x1-x2" = 0.25, "x1-x3" = 0.5, "x2-x3" = 0.75))

# create x1, x2 and x3 which are correlated
# since normal distribution, we use genCorData
# (no need for mode versatile genCorGen and genCorFlex)
sim$dataCor <- genCorData(n = 1000L,
                       mu = c("x1" = 1, "x2" = 2, "x3" = 0),
                       sigma = c("x1" = 0.5, "x2" = 1, "x3" = 1),
                       corMatrix = sim$Rho,
                       cnames = c("x1", "x2", "x3"),
                       idname = "id")
# glimpse(sim$dataCor)

# This function will inject correlated data inside a simstudy definition
# to be able to use data with a specific correlated structure AND manipulate 
# the data, e.g. creating "mu".  The creation of variables inside a funciton
# avoid the conflcts that might happen otherwise.
# 
# dtDefs: data table of definitions definitions
# dtData: data table, e.g. correlated data from genCorData


# injectData <- function(dtDefs, dtData) {
#   for (nm in names(dtData)) {
#     # NOTE: .. notation in bracket is peculiar to datatables!
#     #       unlist() because must have atomic vector
#     assign(x = eval(nm), value = unlist(dtData[, ..nm], use.names = FALSE))
#   }
#   genData(nrow(dtData), dtDefs)
# }
# sim$data <- injectData(dtDefs = defs, dtData = sim$dataCor)
sim$data <- eflStats::injectData(dtDefs = sim$defs, dtData = sim$dataCor)
glimpse(sim$data)
```



```{r}
# sim <- list(n = 500L,
#             intercept = c("mu" = 5, "sigma" = 1),
#             sigma_rate = 1,
#             cat1 = c("size" = 1, "p" = 0.5),
#             mus = c("x1" = 1, "x2" = 2, "x3" = 0),
#             sigmas = c("x1" = 0.5, "x2" = 1, "x3" = 1),
#             rhos = c("x1-x2" = 0.25, "x1-x3" = 0.5, "x2-x3" = 0.75),
#             beta = c("x4" = 10))
# # the dtaa is correlated but in the current model this information
# # is not used.
# sim <- within(sim, {
#   # Rho is the correlation matrix
#   Rho <- diag(nrow = length(rhos))
#   Rho[lower.tri(Rho)] <- rhos
#   Rho[upper.tri(Rho)] <- t(Rho[lower.tri(Rho)])
#   # Sigma is the covariance matrix
#   Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
#   # create the matrix of correlated data
#   mat <- MASS::mvrnorm(n = n, mu = mus, Sigma = Sigma)
# })
# # create the simulated data
# sim <- within(sim, {
#   data <- as.data.frame(mat)
#   data <- data %>%
#     mutate(x0 = rnorm(n = n, mean = intercept["mu"], sd = intercept["sigma"]),
#            x3 = exp(x3),
#            x4 = beta["x4"] * rbinom(n = n, size = cat1["size"], p = cat1["p"]),
#            mu = x0 + x1 + x2 + x3 + x4,
#            sigma = rexp(n, rate = sigma_rate),
#            y = mu + rnorm(n = n, mean = 0L, sd = sigma)
#            )
#   })
```

### Model

The model is defined as follows

$$
\begin{align*}
y &\sim \mathcal{N}(\mu, \sigma) \\
\mu &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 exp(x_3) + \beta_4 x_4 \\
\beta_0 &\sim \mathcal{N}(mean = 5,sd = 1.5) \\
\beta_1 &\sim \mathcal{N}(mean = 1,sd = 0.5) \\
\beta_2 &\sim \mathcal{N}(mean = 2,sd = 1) \\
\beta_3 &\sim \mathcal{LogNormal}(meanlog = 0, sdlog = 1) \\
\sigma &\sim \mathcal{Exponential}(rate = 1)
\end{align*}
$$
### Simulation

We simulate the model using the `simstudy` package which can be found at
@R-simstudy. A wonderful package that is used extensively in this project to
validate the model vs the fitting method.


### Formula

The formula is expressed as usual, as per the model above. We also add the data
to the inla object.

```{r}
args <- list()  # create the list of parameters
args$data <- sim$data
args$formula <- y ~ x1 + x2 + exp(x3)
```


### Priors

#### Family (likelihood)

The default prior on the variance is defined internally in terms of logged
precision $log(\tau$. It follows the log gamma distribution 
$log(\tau) \sim LogGamma(1, 10^-5)$ (@wang2018, p.44).

This corresponds to our model so we keep it as is.

```{r}
args$family = "gaussian"
stopifnot(args$family %in% names(inla.models()$likelihood))
```



#### Fixed effects

See section 2.3.1 of @gomez2020 for details and a nice example at the end of
section 3.2, p.47, of @wang2018.

The model has 2 fixed effects in $x_1$ and $x_2$ which can be assigned
the **fixed-effect priors** from inla whose defaults are as follows


```{r}
inla.set.control.fixed.default()[c("mean", "mean.intercept", "prec", "prec.intercept")]
```

and assigning the fixed effect priors

```{r}
args$control.fixed = list(mean.intercept = 5, 
                          prec.intercept = 1 / (1.5^2),
                          mean = list(x1 = 1, x2 = 2, 
                                      default = 0),
                          prec = list(x1 = 1 / (0.5^2), x2 = 1 / (1^2), 
                                      default = 0.001))
```



#### Random effects


* the model is fixed (no random effect) and
* $x_3$ is a **constrained fixed effect** limited to the positive range.
See section 3.2.2 of @gomez2020 for details.

Therefore the model has a **constrained fixed effect** *weight_c* which is 
indicated by `clinear` in the `f()` function of the formula. This makes
*weight_c* a *latent effect* as opposed to a *likelihood effect* such
as $\sigma$ in the current model.

The available priors, defined in `f()` of *latent effect* are as follows

```{r}
names(inla.models()$prior)
```

and we will use `logtnormal` which is equivalent to our prior 
$\beta &\sim \mathcal{LogNormal}(0, 1)$

```{r}
args$control.family = list(
    hyper = list(prec = list(prior = "loggamma", param = c(1, 10e-5))))
```


### Other parameters

Finally we specify `control.compute` to obtain the DIC and WAIC. Use 
`?control.compute` for the details on this parameters. Its default values
are listed in `inla.set.control.compute.default`

```{r}
args$control.compute <- list(dic = TRUE, waic = TRUE)
```

and specify the quantiles to be obtained

```{r}
args$quantiles <- c(0.025, 0.5, 0.975)
```


### Create inla


```{r}
the_inla <- do.call(what = INLA::inla, args = args)
```


### Summaries


```{r}
eflINLA::posterior_summary_inla(the_inla)
```


```{r}
margs <- eflINLA::extract_marginals_hyper(the_inla)
# names(margs)
# length(margs)
m <- inla.emarginal(function(xx) c(xx, xx^2), margs[[1]])
# s <- 1 / (max(0, sqrt(m[2])))
marg_stats <- c("mean" = m[1], "sd" = sqrt(1 / m[2]))
# sqrt(1 / m[1]^2)
marg_stats
data_stats <- c("mean" = mean(sim$data$sigma), "sd" = sd(sim$data$sigma))
data_stats
```



## 4M7 {-#app-inla-04M07}

We use practice 4M7 from chapter 4 to illustrate `inla` in more details.


### Data and model {-}


```{r}
data(Howell1)
data04M07 <- Howell1 %>%
  filter(age  >= 18) %>%
  mutate(weight_c = as.vector(scale(weight, center = TRUE, scale = FALSE)))
rm(Howell1)
skimr::skim(data04M07)
```


$$
\begin{align*}
height_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta(x_i - \bar{x}) \\
\alpha &\sim \mathcal{N}(178, 20) \\
\beta &\sim \mathcal{LogNormal}(0, 1) \\
\sigma &\sim \mathcal{Exp}(1)
\end{align*}
$$
### Arguments {-}

The arguments will be kept in a list.  This is useful for repeated use and
ensure consistency when creating different inla model objects. For example
for fitted and predicted values computations as done below.

```{r}
i04M07args <- list()
```


#### `formula` {-}

See section 3.2.1 and 3.2.2 of @gomez2020, example on p. 40 at the end of 
section 3.2.2 on p. 41 and 42.

See section 5.2 in @gomez2020 on how to use the prior.

* the model is fixed (no random effect) and
* $x_3$ is a **constrained fixed effect** limited to the positive range.
See section 3.2.2 of @gomez2020 for details.

Therefore the model has a **constrained fixed effect** *weight_c* which is 
indicated by `clinear` in the `f()` function of the formula. This makes
*weight_c* a *latent effect* as opposed to a *likelihood effect* such
as $\sigma$ in the current model.

The available priors, defined in `f()` of *latent effect* are as follows

```{r}
names(inla.models()$prior)
```

and we will use `logtnormal` which is equivalent to our prior 
$\beta &\sim \mathcal{LogNormal}(0, 1)$ and is documented as follows

```{r}
# inla.doc(inla.models()$prior$logtnormal$pdf)  # this command does not work!
# inla.doc(inla.models()$prior$normal$pdf)  # this command works
```

which gives the formula of the linear predictor

```{r}
# linear predictor
i04M07args$formula <- height ~ f(weight_c, model = "clinear", range = c(0, Inf),
                         prior = "logtnormal", param = c(0, 1))
# the available latent models are in names(inla.models()$latent)
stopifnot("clinear" %in% names(inla.models()$latent))
```

#### `family` {-}

`INLA` can work with many families.  They are listed in `inla.models`.
For this case the family of the likelihood is

```{r}
# likelihood family
i04M07args$family <- "gaussian"
stopifnot(i04M07args$family %in% names(inla.models()$likelihood))
```


#### `control.fixed` {-}

See example on p. 47 at the end of section 3.2 of @wang2018.

As indicated in section 2.3.1, p.19, of @gomez2020 the prior for the intercept
is *gaussian* with the following latent hyperparameters which need to 
be modified for our model.

```{r}
inla.set.control.fixed.default()$mean.intercept
inla.set.control.fixed.default()$prec.intercept
```
Therefore the intercept prior for the  is $\alpha \sim \mathcal{N}(178, 20)$ where $sd = 20$ and 
precision is $\tau = \frac{1}{sd^2} = \frac{1}{20^2} = 0.0025$

```{r}
i04M07args$control.fixed <- list(mean.intercept = 178, prec.intercept = 1 / (20^2))
```

#### `control.family` {-}

See example on p. 47 at the end of section 3.2 of @wang2018.
See also section 5.2 in @gomez2020.

In our model we have the prior for the sigma of the likelihood set as

$$
\sigma \sim \mathcal{Exp}(rate_\sigma = 1)
$$
Since $\sigma^2 = \frac{1}{\tau}$ then in terms or $\tau$ the rate becomes
$rate_\tau = \frac{1}{\sigma^2}$ and thereofre


$$
\begin{align*}
\sigma \sim \mathcal{Exp}(rate_\sigma = 1) \\
\therefore \\
\tau \sim \mathcal{Exp}(rate_\tau = \frac{1}{\sigma^2})\\
\end{align*}
$$



As indicated in section 2.3.1, p. 19, of @gomez2020 the default of the likelihood
precision is gamma distribution with parameter $shape = 1$ and $rate = 0.00005$.
Since $shape = 1$ this is equivalent to the beta distribution then we use to
reflect our model's prior. i.e. 

$$
\tau \sim \mathcal{Gamma}(shape = 1, rate = \tau)
$$

The internal representation of the precision is $log{\tau}$ (see table 5.3 in section 
5.3.1 of @wang2018).  Therefore $\tau \sim \mathcal{Gamma}(1, 1 / log(1^2)) \approx \mathcal{Gamma}(1, 0.00005)$. 
We use 0.00005 because 0 cannot be used as a denominator.
It is the default setting.

```{r}
names(inla.models()$prior)
# change to TRUE to see document
if (FALSE) inla.doc("gamma", section = "prior")
```


Therefore we use

```{r}
i04M07args$control.family <- list(
    hyper = list(prec = list(prior = "loggamma", param = c(1, 0.00005))))
```


#### `control.compute` {-}

See section 3.4 in @wang2018 for model selection and checking.
For `cpo`, see section 3.4.3 in @wang2018 for details

This arguments, in the form of a list, is as follows

* `config = TRUE`: Compute the posteriors
* `dic = TRUE`: Compute the Divergence information criteria
* `waic = TRUE`: Compute the waic
* `cpo = TRUE`: Compute the conditional predictive ordinance.

which gives

```{r}
i04M07args$control.compute <- list(config = TRUE, dic = TRUE, waic = TRUE)
```

#### `quantiles` {-}

the quantiles for credibility intervals.  `inla` uses credibility intervals rather
than confidence intervals, usually similar when data has a centralized distribution.

```{r}
i04M07args$quantiles <- c(0.025, 0.5, 0.975)
```


### Create `inla` {-}

Since we are using a predefined list of arguments, it is convenient to use
`do.call`.  This ensure that the list of arguments can be reused later for
fitted and predict.

```{r}
# the data
i04M07args$data <- data04M07
# we use do.call to be able to use the list of parameters
i04M07 <- do.call(inla, args = i04M07args)
```



Note that all the arguments of the `inla` object can be accessed through
the `.args` item as follows

```{r}
names(i04M07$.args)
```

### Summaries {-}

which gives the summary for fixed parameters

```{r}
i04M07$summary.fixed
```

and hyperparameters which are shown on the *internal scale* which is the *log*
and this case.

```{r}
i04M07$summary.hyperpar
```


### Fitted {-#app-inla-04M07-fitted}


See [brinla](http://julianfaraway.github.io/brinla/examples/chicago.html)
on how to find the fit.

First we create the set of new data where the dependent variable has value `NA`

```{r}
# the new data
newdata <- list(n = 20L)
newdata$new <- data.frame(
  weight_c = seq_range(x = data04M07$weight_c, n = newdata$n),
  height = NA_real_
)
# the complete new dataframe with old and new data
newdata$data <- data04M07 %>%
  select(height, weight_c) %>%
  bind_rows(newdata$new)
```


We need to add `control.predictor=list(compute=TRUE)` to compute
the posterior mean of the linear predictors and use the data with the
sequence with `NA`

```{r}
# use the same arguments as the original and add the new data and
# the control.predictor
i04M07args$data <- newdata$data
i04M07args$control.predictor <- list(compute = TRUE)
i04M07fitted <- do.call(inla, args = i04M07args)
```

get the fitted data in a data.frame with only the new values

```{r}
# get the fitted data 
p <- list()
p$fitted <- i04M07fitted$summary.fitted.values %>%
  select(mean, sd, `0.025quant`, `0.975quant`) %>%
  slice_tail(n = newdata$n) %>%
  bind_cols(weight_c = newdata$new$weight_c)
```

and plot the results

```{r}
# plot the fitted values against the raw data
ggplot(p$fitted, 
       aes(x = weight_c, y = mean, ymin = `0.025quant`, ymax = `0.975quant`)) +
  geom_lineribbon(size = 1, color = "rosybrown", fill = "rosybrown1") +
  geom_point(data = data04M07, aes(x = weight_c, y = height, color = age),
             inherit.aes = FALSE) +
  scale_x_continuous(breaks = scales::breaks_extended(n = 7),
                     labels = function(x) x + round(mean(data04M07$weight))) +
  scale_color_paletteer_c("scico::lapaz") +
  ggdist::theme_ggdist() +
  theme(legend.position = c(0.20, 0.80),
        title = element_text(color = "midnightblue")) +
  labs(title = "inla fit", x = "weight", y = "height")
```


### Predictions {-#app-inla-04M07-predict}

When it comes to making predictions, we encounter many different ways to do
it.  The recommended way from inla org is [inla](https://www.r-inla.org/faq#h.821k2r53fvx3)
with `i04M07$summary.random`. Thsi however returns a number of predictions
not equal to the number of predictions requested.  It is impossible to see
which predictions matches with which prediction.

Also see an example of another way in section 7.2.3, p.184 of @wang2018.

The method below was found on the internet.  Unfortunately I do not remember
the address.  My apologies. The method seems to work for any model which 
is why it is the chosen method.

The process is in 3 steps, the first 2 seps are the same as for the fitted
values (see just above)

1. Get the predicted `inla` object with `NA` values to predict
2. Sample the posterior with `inla.posterior.sample`
3. Simulate the posterior predictions by adding variability to the 
linear predictors

#### Predicted `inla` object {-}

First we create the set of new data where the dependent variable has value `NA`

```{r}
# the new data
newdata <- list(n = 20L)
newdata$new <- data.frame(
  weight_c = seq_range(x = data04M07$weight_c, n = newdata$n),
  height = NA_real_
)
# the complete new dataframe with old and new data
newdata$data <- data04M07 %>%
  select(height, weight_c) %>%
  bind_rows(newdata$new)
```


We need to add `control.predictor=list(compute=TRUE)` to compute
the posterior mean of the linear predictors with the new data.

```{r}
# use the same arguments as the original and add the new data and
# the control.predictor
i04M07args$data <- newdata$data
i04M07args$control.predictor <- list(compute = TRUE)
i04M07fitted <- do.call(inla, args = i04M07args)
```


For every lines we can obtain the summary. For example for the new 20
predictions

```{r}
i04M07fitted$summary.linear.predictor %>%
  filter(is.na(newdata$data$height))
```
and the marginal which was used to compute the line summary is available,
for example for $predictor.353$ which is the first new

```{r}
p <- list()
p$df <- i04M07fitted$marginals.linear.predictor[["Predictor.353"]] %>%
  as.data.frame()
ggplot(p$df, aes(x = x, y = y)) +
  geom_line(color = "rosybrown", size = 1) +
  theme_minimal() +
  labs(x = "weight_c", y = "height")
```

**Important**: The variation from the **likelihood precision** is missing.  The
above is only the linear predictor itself.  When the link function is the
identify, the linear predictor is the fitted value. Therefore to get the predictions
we need to sample the posterior as follows.


#### Sample the posterior {-}

From the `inla` object, we extract a list of samples and, for each samples, 
the list of linear predictors

```{r}
samples <- list(n = 1000L)
samples$data <- inla.posterior.sample(n = samples$n, result = i04M07fitted)
with(samples, stopifnot(length(data) == n))
```

Each sample in the list is structured as follows

```{r}
str(samples$data[[1]])
```

We need, for each sample

* The linear predictors which are numbered from their row no in the data
provided to `inla`
* The sample's precision to add variability to the linear predictor.  Remember,
from above, that the linear predictor is a fit and we need to add the likelihood
variability to it and this, for each sample.

For each sample, a dataframe of simulated data is found in the `latent` item.
The linear predictors are identified as `"Predictor:<row no>"`.

Since the new predictors are the last items in the latent dataframe we can 
extract them. Using the first sample as an example, we obtain

```{r}
# which gives the linear predictors for the first sample
cat("The linear predictors", "\n")
samples$data[[1]]$latent[353:372, ]
# and the sigma value for the first sample
cat("The sigma", "\n")
samples$data[[1]]$hyperpar[[1]]
```

therefore we use this information and repeat it for every sample to simulate
the linear predictors *with variability*.  The`predict_inla` function automates 
this process.

#### `predict_inla` {-#app-inla-04M07-predict_inla}

`predict_inla` executes the process of sampling the posterior prediction from
an inla model object. It consists in 2 steps:

* *Create an augmented inla object* which is a list of the usual `inla` object
with a vector of their positions. The vector of positions is necessary as
the predictors are extracted using them.

Please see the documentation in the `eflINLA` package for the function 
`predict_inla`.  For the `R` code, just select `predict_inla` and press `F2` 
to see the code.

```{r}
# the new data
preds <- list(nsamples = 100L)
preds$newdata <- data.frame(
  weight_c = seq_range(x = data04M07$weight_c, n = 20),
  height = NA_real_
)
preds$inla_aug <- eflINLA::augment_inla(i04M07, newdata = preds$newdata)

preds$data <- eflINLA::predicted_draws_inla(preds$inla_aug$inla,
                                       new_pos = preds$inla_aug$new_pos,
                                       n = preds$nsamples) %>%
  as_draws_df() %>%
  select(starts_with("Predictor"))
# glimpse(preds$data)

# get the intervals
preds$intervals <- preds$data %>%
  purrr::map_df(.f = function(x) {ggdist::mean_qi(x, .width = c(.50))}) %>%
  mutate(weight_c = preds$newdata$weight_c)
# preds$intervals

# put data in long format
preds$lng <- preds$data %>%
  pivot_longer(cols = starts_with("Predictor")) %>%
  mutate(weight_c = rep(preds$newdata$weight_c, times = nrow(preds$data)))
# glimpse(preds$lng)
```

plot the data

```{r}
ggplot(data04M07, aes(x = weight_c, y = height)) +
  stat_lineribbon(preds$lng, mapping = aes(x = weight_c, y = value),
                  color = "slateblue",inherit.aes = FALSE, .width = c(0.50, 0.75, 0.95),
                  show.legend = FALSE) +
  geom_point(color = "indianred") +
  scale_x_continuous(
    breaks = scales::breaks_extended(n = 7),
    labels = function(x) {
      y <- x + mean(data04M07$weight)
      scales::label_number(accuracy = 1)(y)
  }) +
  scale_fill_paletteer_d("ggsci::indigo_material") +
  ggdist::theme_ggdist() +
  theme(title = element_text(color = "midnightblue")) +
  labs(title = "Predictions with INLA", 
       subtitle = sprintf("sample size = %d, nb of predictors = %d", 
                          preds$nsamples, length(preds$data)),
       x = "weight", y = "height")
```
