```{r include=FALSE}
library(rethinking)
library(brms)
library(INLA)
library(eflStats)
library(eflINLA)
library(eflRethinking)
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(tidybayes.rethinking, quietly = TRUE)
library(posterior, quietly = TRUE)
library(modelr, quietly = TRUE)
library(simstudy, quietly = TRUE)
library(scales)
library(dagitty, quietly = TRUE)
library(ggdag, quietly = TRUE)
library(ggraph, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
```


# Structural Causal Models {#SCM}

The graph layout are available at [layout](https://rdrr.io/cran/ggraph/man/layout_tbl_graph_igraph.html).
See also the documentation of `ggraph::Layouts`.

The recommended layout is *sugiyama* for DAG. However there are cases where the 
*kk* layout gives a more desirable, i.e. planar, layout.  See 6M1 for example.


The default dag plot used

```{r}
gg_dag <- function(dag, layout = "sugiyama", seed = 6,
                   clrs_status = NULL, clrs = NULL, sizes = NULL) {
  
  # default status colors
  clrs_status_default <- list(latent="mediumvioletred", 
                              exposure="lightcoral", 
                              outcome="cornflowerblue")
  pos <- !(names(clrs_status_default) %in% names(clrs_status))
  clrs_status <- append(x = clrs_status, values = clrs_status_default[pos])
  
  # default colors
  clrs_default <- list(text ="black",  na = "honeydew3", bg = "snow")
  pos <- !(names(clrs_default) %in% names(clrs))
  clrs <- append(x = clrs, values = clrs_default[pos])
  
  # default sizes
  sizes_default <- list(node = 14, text = 3.5)
  pos <- !(names(sizes_default) %in% names(sizes))
  sizes <- append(x = sizes, values = sizes_default[pos])
  
  
  dag %>% 
    tidy_dagitty(seed = seed, layout = layout) %>%
    ggdag_status(color = status, text = TRUE, 
                       node_size = sizes$node, text_size = sizes$text, 
                       text_col = clrs$text) +
    # geom_dag_label_repel(aes(label = name, fill = status), color = "white") +
    scale_color_manual(values = clrs_status, na.value = clrs$na) +
    scale_fill_manual(values = clrs_status, na.value = clrs$na) +
    # ggraph::geom_node_text(aes(label = name)) +
    # ggraph::geom_edge_link(aes(start_cap = label_rect(node1.name), end_cap =
    # label_rect(node2.name)))
    ggdag::theme_dag_blank(panel.background = element_rect(fill=clrs$bg, color=clrs$bg)) +
    theme(legend.position = "none",
          title = element_text(color = "midnightblue"))
}
```



## 6E1 {-#prac6E1}

### Multicollinearity {-}

See section 6.1 for all details.

When predictors are closely related to each other when, for example,
they have a common, unobserved cause.  In `ggdag` an unmeasured variable
is called a latent variable.

For example the cost of a product $P$ can be the result of using 2 manufacturing cells
$C_1$ and $C_2$, both of which can be substituted for one another depending 
on some unobserved/latent decision $D$.

```{r}
dag <- dagify(
  Out ~ Var1 + Var2,
  Var1 ~ U,
  Var2 ~ U,
  outcome = "Out",
  latent = "U")
  # tidy_dagitty(seed = 6, layout = "sugiyama")
# dag
dag

p <- list()

p$dag <- gg_dag(dag) +
  labs(title = "Multicollinearity", subtitle = "6H1")

p$dag

dag_tbl <- tidygraph::as_tbl_graph(dag)  # cannot use %>% here
p$arrow <- dag_tbl %>%
  ggraph::ggraph(layout = "sugiyama") +
  ggraph::geom_node_text(aes(label = name)) +
  ggraph::geom_edge_link(aes(start_cap = label_rect(node1.name), 
                             end_cap = label_rect(node2.name)),
                         arrow = arrow()) +
  ggdag::theme_dag_blank(panel.background = 
                           element_rect(fill="snow", color="snow")) +
  theme(legend.position = "none") +
  labs(title = "Multicollinearity", subtitle = "6H1")
p$arrow


wrap_plots(p) +
  plot_annotation("2 ways to do the DAG")
```



### Post-treatment bias {-}

See section 6.2 for all details.

When a predictor $X_1$ of the outcome $y$ included in the model is actually the outcome of another predictor $X_2$ and the 2 outcomes $x_1$ and $y$ are related.

For example the cost of a product $P_1$ includes 
another product $P_2$ and $P_1$ and $P_2$ give the final product ready for sale. 


```{r}
dag <- list()
dag$d1 <- dagify(
  Out1 ~ Var1,
  Out2 ~ Out1,
  Out2 ~ Var2,
  outcome = "Out2")
# dag1

dag$d2 <- dagify(
  Out1 ~ U,
  Out1 ~ Var1,
  Out2 ~ Var2,
  Out2 ~ U,
  latent = "U",
  outcome = "Out2")
# dag2

p <- list()


p$dag1 <- gg_dag(dag$d1) +
  labs(title = "Post-treatment bias", 
       subtitle = "Out1 and Out2 are not ind. because of Var1")

p$dag2 <- gg_dag(dag$d2) +
  labs(title = "Post-treatment bias", 
       subtitle = "Out1 and Out2 are not ind. because of U")

wrap_plots(p) +
  plot_annotation("2 possible post-treatment biases")
```


### Collider bias {-}

See section 6.3 for all details.


Collider bias causes a statistical, not necessarily causal, bias.

For example, the quality of a product $Out$ could be influenced by the
manufacturing process using $Var1$ and $Var2$ but some raw materials affect
the quality/performance of $Var1$ and $Out$. 



```{r}
dag <- dagify(
  Out ~ U,
  Out ~ Var1,
  Out ~ Var2,
  Var1 ~ Var2,
  Var1 ~ U,
  latent = "U",
  outcome = "Out")
# dag

p <- list()

p$dag <- gg_dag(dag) +
  labs(title = "Post-treatment bias", 
       subtitle = "If we condition on Var2 then Out become dependent of the value of Var2")

wrap_plots(p) +
  plot_annotation("Post-treatment bias")
```


## 6E2 {-#prac6E2}

See examples and graphs in 6H1 just above.


## 6E3 {-#prac6E3}

See section 6.4.1

```{r}
dag <- list()
dag$fork <- dagify(
  O1 ~ V,
  O2 ~ V,
  outcome = c("O1", "O2"))

dag$chain <- dagify(
  V2 ~ V1,
  O ~ V2,
  outcome = "O")

dag$collider <- dagify(
  O ~ V1,
  O ~ V2,
  outcome = "O")

dag$collider <- dagify(
  O ~ V1,
  O ~ V2,
  outcome = "O")

dag$descendant <- dagify(
  O1 ~ V1,
  O1 ~ V2,
  O2 ~ O1,
  outcome = "O2")

p <- list()

p$fork <- gg_dag(dag$fork, sizes = list(node = 8, text = 3)) + 
  labs(title = "Fork")

p$chain <- gg_dag(dag$chain, sizes = list(node = 8, text = 3)) + 
  labs(title = "Chain")

p$collider <- gg_dag(dag$collider, sizes = list(node = 8, text = 3)) + 
  labs(title = "Collider")

p$descendant <- gg_dag(dag$descendant, sizes = list(node = 8, text = 3)) + 
  labs(title = "Descendant")


wrap_plots(p) +
  plot_annotation("6E3") &
  theme(title = element_text(color = "midnightblue", size = 8))
```

## 6E4 {-#prac6E4}

The sample is biased because the selection of the outcome creates a dependency
between the 2 causal variables.  For example, the sum $z$ of 2 variables $x$ and
$y$.  $x$ and $y$ are independent but, if we condition on $z$ then $x$ and $y$
are dependent.  For example, if $z = 5$ then the choice of $x$ will tell us
what $y$ is, e.g. if $x=3$ then we must have $y=2$.

Using the example at the open of the chapter, since the outcome of being 
selected ($S$) depends on whether the scientific studies
is newsworthy ($NW$) or trustworthy ($TW$) then we have the following DAG


```{r, out.width="67%"}
dag <- dagify(
  S ~ TW,
  S ~ NW,
  outcome = "S")

gg_dag(dag) +
  labs(title = "Scientific studies selection (collider bias)", 
       subtitle = "If we condition on S then TW become dependent of the value of NW")
```

## 6M1 {-#prac6M1}

The DAG on page 186 is

```{r}
dag <- list()
dag$coords <- tibble(name = c("X", "U", "A", "B", "C", "Y", "V"),
                     x = c(1, 1, 2, 2, 3, 3, 3.5),
                     y = c(1, 2, 2.5, 1.5, 2, 1, 1.5))
# the original DAG on p. 186
dag$original <- dagify(
  Y ~ C + X,
  X ~ U,
  B ~ U + C,
  C ~ A,
  U ~ A,
  exposure = "X",
  outcome = "Y",
  latent = "U",
  coords = dag$coords)
# the modified DAG
dag$modified <- dagify(
  Y ~ C + X + V,
  X ~ U,
  B ~ U + C,
  C ~ A + V,
  U ~ A,
  exposure = "X",
  outcome = "Y",
  latent = c("U", "V"),
  coords = dag$coords)

p <- list()
p$original <- dag$original %>%
  gg_dag() +
  labs(title = "Two Roads (section 6.4.2, p. 186)", 
       subtitle = "6M1")

p$modified <- dag$modified %>%
  gg_dag() +
  labs(title = "Two Roads MODIFIED", 
       subtitle = "6M1")
wrap_plots(p)
```

The original DAG had the following paths from $X$ to $Y$ with the adjustments set

```{r}
p <- dagitty::paths(dag$original, from = "X", to = "Y")$paths
p
# the adjustments set
s <- dagitty::adjustmentSets(dag$original, exposure = "X", outcome = "Y")
cat("The adjustment set", "\n")
s
```

The modified DAG has the following paths from $X$ to $Y$ and adjustment set

```{r}
p <- dagitty::paths(dag$modified, from = "X", to = "Y")$paths
p
```
Thus we no have 4 backdoor paths. As described in section 6.4.2, p. 186, we
must now verify if a backdoor path is open and close it.  It it is not open
we must not accidentally open it with, for example, a collider.

The set of variable to close is only $A$.  This is because $C$ has become
a collider. Tat is, closing $C$ will create a path with $V$.

```{r}
# the adjustments set
s <- dagitty::adjustmentSets(dag$modified, exposure = "X", outcome = "Y")
cat("\n","The adjustment set", "\n")
s
```


## 6M2 {-#prac6M2}

The simulation is made in the form of a *chain*. Observe that the correlation
between $x$ and $z$ is strong, nearly 0.9.


```{r}
set.seed(as.integer(as.Date("2021-11-07")))
sim <- list()
sim <- within(sim, {
  defs <- defData(varname = "x", dist = "normal", formula = 0, variance = 1)
  defs <- defData(defs, varname = "z", dist = "normal", formula = "x", variance = 0.2)
  defs <- defData(defs, varname = "y", dist = "normal", formula = "z", variance = 1)
  data <- genData(n = 100L, dtDefs = defs)
})
# output must e standardized
sim$data <- sim$data %>%
  select(-id) %>%
  mutate(across(.cols = everything(), .fns = function(x) as.vector(scale(x))))
# glimpse(sim$data)
GGally::ggpairs(sim$data)
```

and the model fit is now

```{r}
a_file <- here::here("fits", "i06M02.rds")
i06M02 <- readRDS(file = a_file)
# i06M02 <- inla(
#   data = sim$data,
#   formula = y ~ x + z,
#   )
# saveRDS(i06M02, file = a_file)
```


and we plot the resulting posterior distributions

```{r}
p <- list()

p$data <- eflINLA::tidy_marg_draws_inla(i06M02) %>%
  posterior::as_draws_df() %>%
  pivot_longer(cols = !matches("[.]chain|[.]iteration|[.]draw"))
# str(p$data)

p$intr <- eflINLA::tidy_marg_draws_inla(i06M02) %>%
  as_draws_df() %>%
  eflStats::gather_intervals_rng(fun = mode_qi)
# p$intr

ggplot(p$data, aes(x = value, y = name)) +
  stat_halfeye(aes(fill = name),
               point_interval = mode_qi, 
               .width = c(0.5, 0.75), fatten_point = 2) +
  ggrepel::geom_text_repel(data = p$intr, 
                           aes(x = .value, y = .variable,
                               label = scales::label_number(accuracy = 0.1)(.value)),
                           inherit.aes = FALSE) +
  # scale_x_continuous(labels = scales::label_number(accuracy = 0.1)) +
  scale_fill_paletteer_d("fishualize::Scarus_quoyi") +
  theme_ggdist() +
  theme(legend.position = "none") +
  labs(title = "Practice 6M2", x = NULL, y = NULL)

```

The main difference with the leg example of section 6.1.1 is that the 
predictors' coefficients do not overlap and their variable, relative to their
size, is much lower.

This is caused by the fact that we now have a *chain* whereas the leg example
was a *collider*.

## 6M3 {-#prac6M3}

*Learning to analyse DAG requires practice*. This is definitively an area where
using technology to avoid mistake is almost a must.  Even when simple cases, it
should be a good idea to use *dagitty*.


```{r}
dag <- list()
dag$coords1 <- tibble(name = c("A", "X", "Y", "Z"),
                     x = c(3, 1, 3, 2),
                     y = c(2, 1, 1, 2))
dag$g1 <- dagify(
  X ~ Z,
  Y ~ A + X + Z,
  Z ~ A,
  exposure = "X",
  outcome = "Y",
  coords = dag$coords1)

dag$g2 <- dagify(
  Y ~ A + X + Z,
  Z ~ A + X,
  exposure = "X",
  outcome = "Y",
  coords = dag$coords1)

dag$coords2 <- tibble(name = c("A", "X", "Y", "Z"),
                     x = c(1, 1, 3, 2),
                     y = c(2, 1, 1, 2))
dag$g3 <- dagify(
  X ~ A,
  Y ~ X,
  Z ~ A + X + Y,
  exposure = "X",
  outcome = "Y",
  coords = dag$coords2)

dag$g4 <- dagify(
  X ~ A,
  Y ~ X + Z,
  Z ~ A + X,
  exposure = "X",
  outcome = "Y",
  coords = dag$coords2)

p <- list()
p$g1 <- dag$g1 %>%
  gg_dag() +
  labs(title = "DAG 1", 
       subtitle = "6M3")

p$g2 <- dag$g2 %>%
  gg_dag() +
  labs(title = "DAG 2", 
       subtitle = "6M3")

p$g3 <- dag$g3 %>%
  gg_dag() +
  labs(title = "DAG 3", 
       subtitle = "6M3")

p$g4 <- dag$g4 %>%
  gg_dag() +
  labs(title = "DAG 4", 
       subtitle = "6M3")

wrap_plots(p)
```


```{r}
dagitty::adjustmentSets(dag$g1, exposure = "X", outcome = "Y")
```


```{r}
dagitty::adjustmentSets(dag$g2, exposure = "X", outcome = "Y")
```


```{r}
dagitty::adjustmentSets(dag$g3, exposure = "X", outcome = "Y")
```



```{r}
dagitty::adjustmentSets(dag$g4, exposure = "X", outcome = "Y")
```



## 6H1 {-#prac6H1}

This practice is explained section 6.4.3.

It is important to note that the question asks the *total causal influence* of
the number of waffle houses on the divorce rate.  The Overthinking box of
6.4.3, p. 188, explains that the *total* causal effect is determined by using 
the *do-operator*, i.e. $P(D = d \mid do(W = w))$ vs the *direct causal effect* 
which simply conditions on $W$, i.e. $P(D = d \mid W = w)$.

```{r}
dag <- list()
dag$coords <- tibble(name = c("A", "D", "M", "S", "W"),
                     x = c(1, 3, 2, 1, 3),
                     y = c(1, 1, 2, 3, 3))
dag$g <- dagify(
  A ~ S,
  D ~ A + M + W,
  M ~ A + S,
  W ~ S,
  exposure = "W",
  outcome = "D",
  coords = dag$coords)
p <- list()
p$g <- dag$g %>%
  gg_dag() +
  labs(title = "Waffle Houses", 
       subtitle = "6H1")
p$g
```

To find the *total causal* effect we must use the *do-operator* which closes
all the backdoors. See section 3.2 and 3.3 of @pearl2016 for details.

The backdoors can be closed by controlling ${A, M}$ or simply ${S}$ as shown
just below with `adjustmentSets()`.


```{r}
dagitty::adjustmentSets(dag$g, exposure = "W", outcome = "D")
```

 That is, "we can close them all by conditioning on $S$" (in the middle of 
 p. 187, just before R code 6.30).

Now conditioning on $S$ simply means filtering the data on the values of $S$.
Since $S$ is and indicator for $S=1, South$ and $S=0, Not South$ we will filter
on $S=0$ to remove the effect of $S$.

```{r}
data("WaffleDivorce")
data06H01 <- WaffleDivorce %>%
  as_tibble() %>%
  select(D = Divorce,
         A = MedianAgeMarriage,
         M = Marriage,
         S = South,
         W = WaffleHouses) %>%
  mutate(across(.cols = -S, .fns = standardize),
         S = factor(S))
rm(WaffleDivorce)
```


and we regress after filtering on $S=0$ to avoid spurious effects as discussed above

```{r}
a_file <- here::here("fits", "i06H01.rds")
i06H01 <- readRDS(file = a_file)
# i06H01 <- INLA::inla(
#   data = data06H01,
#   formula = D ~ W + S,
#   quantiles = c(0.025, 0.975)
# )
# saveRDS(i06H01, file = a_file)

summary(i06H01)$fixed
```

Which shows that once we take the $South$ into account, te $W = Waffle$ has
very little impact. We note that it is very variable relative to its mean.

## 6H2 {-#prac6H2}

We use the DAG and data from 6H1 above.

The implied conditional independencies are

```{r}
dagitty::impliedConditionalIndependencies(dag$g)
```
The model for $A \perp\!\!\!\perp W \mid S$

```{r}
a_file <- here::here("fits", "i06H02a.rds")
i06H02a <- readRDS(file = a_file)
# i06H02a <- INLA::inla(
#   data = data06H01,
#   formula = A ~ W + S
# )
# saveRDS(i06H02a, file = a_file)
summary(i06H02a)$fixed
```


The model for $D \perp\!\!\!\perp S \mid A, M, W$

```{r}
a_file <- here::here("fits", "i06H02d.rds")
i06H02d <- readRDS(file = a_file)
# i06H02d <- INLA::inla(
#   data = data06H01,
#   formula = D ~ S + A + M + W
# )
# saveRDS(i06H02d, file = a_file)
summary(i06H02d)$fixed
```

The effect of $S$ on $D$ is less than $A$ but still not negligible. By adding
variables that would replace $S$ and separate the different causes of $A$ and $M$
we might able to have more choices to reduce the impact of $S$.


The model for $M \perp\!\!\!\perp W \mid S$

```{r}
a_file <- here::here("fits", "i06H02m.rds")
i06H02m <- readRDS(file = a_file)
# i06H02m <- INLA::inla(
#   data = data06H01,
#   formula = M ~ W + S
# )
# saveRDS(i06H02m, file = a_file)
summary(i06H02m)$fixed
```

which confirms that $M \perp\!\!\!\perp W \mid S$ is true.


## 6H3 {-#prac6H3}

See practice 7H5 in chapter 7 for corresponding exercises.

```{r}
data(foxes)
data06H03 <- foxes %>%
  select(group, A = area,
         `F` = avgfood,
         S = groupsize,
         W = weight) %>%
  mutate(across(.cols = -group, .fns = standardize))
rm(foxes)
```


```{r}
dag <- list()
dag$coords <- tibble(name = c("A", "F", "W", "S"),
                     x = c(2, 1, 2, 3),
                     y = c(3, 2, 1, 2))
dag$g <- dagify(
  `F` ~ A,
  S ~ `F`,
  W ~ `F` + S,
  exposure = "A",
  outcome = "W",
  coords = dag$coords)
p <- list()
p$g <- dag$g %>%
  gg_dag() +
  labs(title = "Foxes", 
       subtitle = "6H3")
p$g
```


$$
\begin{align*}
W_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta \cdot A_i \\
\alpha &\sim \mathcal{N}(0, 0.5) \\
\beta &\sim \mathcal{N}(0, 0.2) \\
\sigma &\sim \mathcal{HalfCauchy}(0, 1)
\end{align*}
$$

and the fit

```{r}
a_file <- here::here("fits", "b06H03.rds")
b06H03 <- readRDS(file = a_file)
# b06H03 <- brm(
#   data = data06H03,
#   formula = W ~ 1 + A,
#   family = gaussian,
#   prior = c(
#     prior(normal(0, 0.5), class = Intercept),
#     prior(normal(0, 0.2), class = b),
#     prior(cauchy(0, 1), class = sigma)
#   ),
#   iter = 2000, warmup = 1000, chains = 4, core = detectCores(),
#   seed = 6
# )
# saveRDS(b06H03, file = a_file)
summary(b06H03)
```

and create the full data for plotting

```{r}
samples <- list()
samples$fitted <- fitted(b06H03) %>%
  as.data.frame() %>%
  rename(
    fit = Estimate,
    fit_err = Est.Error,
    fit_Q2.5 = Q2.5,
    fit_Q97.5 = Q97.5)

samples$data <- data06H03 %>%
  bind_cols(samples$fitted)
```



```{r}
ggplot(data = samples$data, mapping = aes(x = A, y = W)) +
  geom_smooth(mapping = aes(y = fit, ymin = fit_Q2.5, ymax = fit_Q97.5),
              stat = "identity",
              fill = "olivedrab3", color = "olivedrab4", alpha = 0.5) +
  geom_point(aes(color = group)) +
  scale_color_paletteer_c("oompaBase::jetColors") + 
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Practice 6H3: Weight vs Area Size",
       color = "group",
       x = "area size (std)", y = "weight (std)")
```

Would increasing the area make the foxes healthier? The answer is no given that
the effect of $A$ from the fit just above is almost nil.

## 6H4 {-#prac6H4}


```{r}
a_file <- here::here("fits", "b06H04.rds")
b06H04 <- readRDS(file = a_file)
# b06H04 <- brm(
#   data = data06H03,
#   formula = W ~ 1 + `F`,
#   family = gaussian,
#   prior = c(
#     prior(normal(0, 0.5), class = Intercept),
#     prior(normal(0, 0.2), class = b),
#     prior(cauchy(0, 1), class = sigma)
#   ),
#   iter = 2000, warmup = 1000, chains = 4, core = detectCores(),
#   seed = 6
# )
# saveRDS(b06H04, file = a_file)
summary(b06H04)
```

Adding food has little impact.  We would need to adjust for group size to
remove its effect.

## 6H5 {-#prac6H5}


```{r}
a_file <- here::here("fits", "b06H05.rds")
b06H05 <- readRDS(file = a_file)
# b06H05 <- brm(
#   data = data06H03,
#   formula = W ~ 1 + S,
#   family = gaussian,
#   prior = c(
#     prior(normal(0, 0.5), class = Intercept),
#     prior(normal(0, 0.2), class = b),
#     prior(cauchy(0, 1), class = sigma)
#   ),
#   iter = 2000, warmup = 1000, chains = 4, core = detectCores(),
#   seed = 6
# )
# saveRDS(b06H05, file = a_file)
summary(b06H05)
```

The causal impact of group size is similar to food, that is very little.  That
is caused by the fact that $F$, $S$ and $W$ are a collider. That is, as soon as 
we use $F$ and/or $S$ a dependency of $W$ is created on the other variable.


## 6H6 {-#prac6H6}

```{r echo=FALSE}
message("TODO")
```



## 6H7 {-#prac6H7}


```{r echo=FALSE}
message("TODO")
```
