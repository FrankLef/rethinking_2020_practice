```{r include=FALSE}
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(rethinking)
library(brms)
library(loo)
library(ggdist, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
```


# Ulysses' Compass {#information}


## 7E1 {-}

See section 7.2.2.

* Continuous: there should be no "jump" in the index.
* Measure of uncertainty is increasing: As the number of possible choices increases, the uncertainty increases and so should the measure
* Measure should be additive: We should be able to add the uncertainty of an event to the uncertainty of another event so that the total uncertainty is higher

## 7E2 {-}

See equation 7.1 in section 7.2.2.

```{r}
p <- c(0.7, 0.3)
sum(-p * log(p))
```

## 7E3 {-}

See equation 7.1 in section 7.2.2.

```{r}
p <- c(0.2, 0.25, 0.25, 0.3)
sum(-p * log(p))
```


## 7E4 {-}

In this case, since the last probability = 0 we don't include it. See the Overthinking
box at the end of section 7.2.2 about L'Hopital's rule.

```{r}
p <- rep(1/3, times = 3)
sum(-p * log(p))
```


## 7M1 {-}

### AIC

See beginning of section 7.4.2 for AIC

$$
AIC = D_{train} + 2 \cdot p = -2 \cdot lppd + 2 \cdot p
$$
The assumptions on which AIC is based are

1. priors are flat or overwhelmed by data
2. posterior distribution is approximately multivariate Gaussian
3. The sample size $N$ is much greater than the number of parameters $k$, $N \gg k$


### WAIC

See section 7.4.2 for definition


$$
WAIC = -2 (lppd - p_{WAIC}) = -2 (lppd - \sum_{i=1}^N{var_{\theta} \log{p(y_i \mid \theta)}})
$$
where $p_{waic}$ is called the penalty term or effective number of parameters.

WAIC makes no assumption about the shape of the posterior. *It provides an
approximation of the out-of-sample deviance that converges to the cross-validation 
approximation in a large sample.* Section 7.4.2, p. 220.


## 7M2 {-}

Model selection and model comparison are defined in the introduction of
section 7.5.

### Model selection

Selecting the model with the lowest information criterion value (WAIC, LOO) 
and discard the other models. It should be avoided as you loose the information
about the relative model accuracy contained in the different criteria.

### Model comparison

Using multiple models t understand how different variables influence predictions
and implied conditional independencies which explain causality.

2 issues arise in model comparison
1. Distinction between models for predictions and models for causation
2. Pointwise nature of model comparison to understand the influential data points.

## 7M3 {-}

See rethinking box at the end of section 7.4.3 which explains that all the
information criteria assume that the data is **generated by the same process**.

In addition 2 more technical details are to be remembered to justify
data cleaning

1. If you don't remove the missing cases, the internal routines of the stat functions might treat them differently without you knowing about it
2. The information criteria is sensitive to the number of observations, therefore make sure no dropping of NA, NaN or Inf occurs unexpectedly my cleaning up the data before performing the analysis.

## 7M4 {-}

A shown in figure 7.9 of section 7.4.3  as the number of parameters increases (i.e. the priors are less concentrated) than the effective number of parameters increase and the average
deviance is reduced.

## 7M5 {-}

Because they provide a more relevant range of hypothesis thus reduced the 
importance given to marginal values of the parameters.

## 7M6 {-}

Because they reduce the range of hypothesis to be tested so narrowly that relevant values and/or parameters are not given enough relevance to appear in the final model.


## 7H1 {-}


```{r}
data(Laffer)
d <- Laffer
skimr::skim(d)
```

```{r}
a_file <- here::here("fits", "b07H01.rds")
b07H01 <- readRDS(file = a_file)
# b07H01 <- brm(
#   data = d,
#   formula = tax_revenue ~ 1 + tax_rate,
#   family = gaussian,
#   prior = c(
#     prior(normal(25, 10), class = Intercept),
#     prior(normal(0, 0.5), class = b),
#     prior(exponential(1), class = sigma)
#     ),
#   iter = 2000, warmup = 1000, chains = detectCores(),
#   core = detectCores(), seed = 7
#  )
# b07H01 <- brms::add_criterion(b07H01, criterion = c("waic", "loo"))
# saveRDS(b07H01, file = a_file)
summary(b07H01)
```

get the fitted and predicted values.

```{r}
rate_seq <- tibble(tax_rate = seq(from = 0, to = 36, by = 1))

b07H01_fitted <-
  fitted(b07H01, newdata = rate_seq) %>%
  data.frame() %>%
  bind_cols(rate_seq)
glimpse(b07H01_fitted)

b07H01_predict <-
  predict(b07H01, newdata = rate_seq) %>%
  data.frame() %>%
  bind_cols(rate_seq)  
glimpse(b07H01_predict)
```


plot the data

```{r}
ggplot(data = d, aes(x = tax_rate)) +
  geom_ribbon(data = b07H01_predict,
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "lightcyan") +
  geom_smooth(data = b07H01_fitted,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "lightcyan3", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = tax_revenue), color = "purple", shape = 20, size = 3, alpha = 2/3) +
  scale_x_continuous(breaks = scales::breaks_extended(n = 7)) +
  coord_cartesian(xlim = range(d$tax_rate), ylim = range(d$tax_revenue)) +
  theme_minimal() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = c(0.1, 0.8)) +
  labs(title = "Laffer model", x = "tax rate", y = "tax revenue")
```


## 7H2 {-}

### Use WAIC and PSIS to evaluate the outliers

See section 7.5.2, figure 7.10.


```{r}
dp <- tibble(pareto_k = b07H01$criteria$loo$diagnostics$pareto_k,
       p_waic   = b07H01$criteria$waic$pointwise[, "p_waic"],
       tax_rate      = d$tax_rate,
       out = p_waic > 0.5)
ggplot(dp, aes(x = pareto_k, y = p_waic, color = out)) +
    geom_vline(xintercept = 0.5, linetype = 2, color = "red", alpha = 1/2) +
    geom_point() +
    ggrepel::geom_text_repel(aes(label = round(tax_rate, 1))) +
    scale_color_manual(values = c("darkgreen", "violetred")) +
    scale_shape_manual(values = c(19, 19)) +
    theme_minimal() +
    theme(legend.position = "none") +
    labs(title = "Lafer model - Gaussian likelihood", subtitle = "Labels represents tax rates",
         x = "WAIC", y = "PSIS")
```


### Robust regression with t-student distribution

See section 7.5.2, R code 7.35

```{r}
a_file <- here::here("fits", "b07H02.rds")
b07H02 <- readRDS(a_file)
# b07H02 <-
#   brm(data = d,
#       family = student,
#       bf(tax_revenue ~ 1 + tax_rate, nu = 2),
#       prior = c(prior(normal(25, 10), class = Intercept),
#                 prior(normal(0, 0.5), class = b),
#                 prior(exponential(1), class = sigma)),
#       iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),
#       seed = 5)
# b07H02 <- brms::add_criterion(b07H02, criterion = c("waic", "loo"))
# saveRDS(b07H02, file = a_file)
summary(b07H02)
```



```{r}
dp <- tibble(pareto_k = b07H02$criteria$loo$diagnostics$pareto_k,
       p_waic   = b07H02$criteria$waic$pointwise[, "p_waic"],
       tax_rate      = d$tax_rate,
       out = p_waic > 0.5)
ggplot(dp, aes(x = pareto_k, y = p_waic, color = out)) +
    geom_vline(xintercept = 0.5, linetype = 2, color = "red", alpha = 1/2) +
    geom_point() +
    ggrepel::geom_text_repel(aes(label = round(tax_rate, 1))) +
    scale_color_manual(values = c("darkgreen", "violetred")) +
    scale_shape_manual(values = c(19, 19)) +
    theme_minimal() +
    theme(legend.position = "none") +
    labs(title = "Lafer model - t-Student likelihood", 
         subtitle = "Labels represents tax rates",
         x = "WAIC", y = "PSIS")
```

## 7H3 {-}

```{r}

```



## OLD {-}

Loading the data

```{r}
data("Howell1")
d <- Howell1
d$age <- scale(x = d$age)  # standardize the age
set.seed(1000)
i <- sample(1:nrow(d), size = nrow(d) / 2)
d_train <- d[i, ]  # d1 in textbook: the training set
d_test <- d[-i, ]  # d2 in textbook: the testing set
```

get the basic stat summary

```{r}
d_summary <- psych::describe(x = d[, c("height", "weight", "age")])
d_summary
```

Creating the models

### Model M1

```{r}
# NOTE: reaches max iteration sometimes, if that is the case, rerun it
set.seed(123)  # it always converge with this seed, works better than start values.
M1 <- rethinking::map(
    flist = alist(
        height ~ dnorm(mu, exp(sigma_log)),
        mu <- a + bAge1 * age,
        a ~ dnorm(100, 200),# based on the height mean
        bAge1 ~ dnorm(10, 30),  # PI result when first iteration converged
        sigma_log ~ dnorm(0, 5)
    ),
    data = d_train
    # start = list(mu = mean(d$height))  # start values seem to cause more problem than they solve!
)
rethinking::precis(M1)
```

### Model M2

```{r}
# IMPORTANT NOTE: Use I() when using ^ in R formula
# IMPORATANT NOTE: Because of problem with convergence, I have saved the model in the current directory.
                   # see below for filw location and command to reload
# NOTE: reaches max iteration sometimes, if that is the case, rerun it
# NOTE: difficulte to obtain convergence, but the values below seem to work in a stable manner, after several trials
# M2 <- rethinking::map(
#     flist = alist(
#         height ~ dnorm(mu, exp(sigma_log)),
#         mu <- a + bAge1 * age + bAge2 * I(age^2),
#         a ~ dnorm(100, 200),# based on the height mean
#         bAge1 ~ dnorm(10, 50),  # PI result when first iteration converged
#         bAge2 ~ dnorm(-50, 50),  # PI result when first iteration converged
#         sigma_log ~ dnorm(0, 5)
#     ),
#     data = d_train
#     # start = list(mu = 152 + 25 * mean(d_train$age) - 15 * mean(d_train$age),
#     #              sigma = 2.5)  # start values seem to cause more problem than they solve!
# )
# rethinking::precis(M2)

# save the reults when convergence is ok
# saveRDS(M2, file = "chap6_M2.rds")
# M2 <- readRDS(file = "chap06_M2.rds")
# rethinking::precis(M2)
```

The result obtained when it converges is as follows, need several iterations to obtain convergence

            Mean StdDev   5.5%  94.5%
a         152.40   1.02 150.76 154.03
bAge1      25.79   0.84  24.46  27.13
bAge2     -14.12   0.70 -15.24 -13.00
sigma_log   2.52   0.04   2.45   2.58



